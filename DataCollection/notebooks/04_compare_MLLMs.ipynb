{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e05d1c91-3da7-418b-a111-c59112ff15d5",
   "metadata": {},
   "source": [
    "# Run Different M-LLMs for Semantic Comparison\n",
    "\n",
    "This notebook uses different M-LLMs to generate synthetic descriptions about the synthetically generated data\n",
    "\n",
    "**Note** This notebook essentially puts the images through 3 different M-LLMs to see how well they can generate labels and descriptions. The models here were way to big to load! I am currently researching new, smaller models to run the analysys. In the meantime, I suggest using different models, or trying your luck by turning this into a script. \n",
    "\n",
    "\n",
    "\n",
    "***Workshop***: AI/ML Pipeline - Synthetic Data Generation; January 23, 2026  \n",
    "***Platform***: CyVerse Jupyter Lab PyTorch GPU\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facfc781-5499-4e8a-ae49-e8d6a15c3b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install specific modules in case they were missed \n",
    "%pip install python-dotenv\n",
    "\n",
    "#working with GPUs\n",
    "#https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one\n",
    "\n",
    "# for all\n",
    "# %pip install -U torch diffusers transformers accelerate bitsandbytes sentencepiece qwen-vl-utils\n",
    "# %pip install peft\n",
    "\n",
    "##\n",
    "# For torchvision: https://pytorch.org/get-started/locally/\n",
    "\n",
    "# #windows\n",
    "# %pip install torch torchvision --index-url https://download.pytorch.org/whl/cu126\n",
    "# %pip install flash_attn-2.8.0.post2+cu12torch2.7cxx11abiTRUE-cp312-cp312-win_amd64.whl\n",
    "\n",
    "# #mac\n",
    "# %pip install torch torchvision\n",
    "# %pip install flash-attn --no-build-isolation\n",
    "\n",
    "# #linux\n",
    "# %pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu\n",
    "# %pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14d1d774-6ce8-459f-a1a3-55bc134f87d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import json\n",
    "import gc\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "from huggingface_hub import login\n",
    "from transformers import (\n",
    "    AutoProcessor, \n",
    "    Qwen2_5_VLForConditionalGeneration, \n",
    "    MllamaForConditionalGeneration, # Often used for Llama Vision models\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForVision2Seq,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "\n",
    "from PIL import Image\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3508f4f-f59a-4c95-973c-ef82df4ed52a",
   "metadata": {},
   "source": [
    "### Load Configuration\n",
    "\n",
    "Review and adjust generation parameters if needed.\n",
    "Remember to include your HF user token to access the free models\n",
    "- https://huggingface.co/ > login or signup\n",
    "    - Billing > Access Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96d02828",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-23 14:06:46,447 - src.config - INFO - Loaded environment variables from /home/jovyan/data-store/AI-ML_PipelineWorkshop/DataCollection/config/.env\n",
      "2026-01-23 14:06:46,454 - src.config - INFO - Loaded configuration from /home/jovyan/data-store/AI-ML_PipelineWorkshop/DataCollection/config/generation_config.yaml\n",
      "2026-01-23 14:06:46,454 - src.config - INFO - Logging configured successfully\n",
      "2026-01-23 14:06:46,456 - src.config - INFO - Loaded environment variables from /home/jovyan/data-store/AI-ML_PipelineWorkshop/DataCollection/config/.env\n",
      "2026-01-23 14:06:46,462 - src.config - INFO - Loaded configuration from /home/jovyan/data-store/AI-ML_PipelineWorkshop/DataCollection/config/generation_config.yaml\n",
      "2026-01-23 14:06:46,462 - src.config - INFO - Logging configured successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bringing in local modules\n",
      "All modules imported successfully\n",
      "Working directory: /home/jovyan/data-store/AI-ML_PipelineWorkshop/DataCollection/notebooks\n"
     ]
    }
   ],
   "source": [
    "# Add parent directory to path\n",
    "parent_dir = Path.cwd().parent\n",
    "if str(parent_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(parent_dir))\n",
    "\n",
    "print(\"bringing in local modules\")\n",
    "# brining in more modeules because of image generation process\n",
    "# modules are found in the DataCollection/src folder\n",
    "# from src import config, gemini_client, data_loader, prompt_builder, output_handler\n",
    "from src import config, data_loader, prompt_builder, output_handler\n",
    "\n",
    "print(\"All modules imported successfully\")\n",
    "print(f\"Working directory: {Path.cwd()}\")\n",
    "\n",
    "\n",
    "# 4-Bit Config (CRITICAL for running 17B Model on 16GB GPU)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Load configuration\n",
    "# DataCollection/src/config.py ... def load_confi()\n",
    "# using \"generation_config.yaml\" for setup\n",
    "cfg = config.load_config()\n",
    "data_dir = cfg.get_data_path('generated')\n",
    "\n",
    "\n",
    "# Login to HuggingFace \n",
    "login(token = 'your token')\n",
    "# login(token = '')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f631cde3-ce3b-44b7-a730-5545f8dde678",
   "metadata": {},
   "source": [
    "### Create helper functions\n",
    "This will help create the arguments and inter functions when we call the models later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e8780e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_images(handler):\n",
    "    # Get the directory where images were just saved\n",
    "    # (handler.images_dir is a Path object from your output_handler.py)\n",
    "    image_files = sorted(list(handler.images_dir.glob(f\"*.{handler.image_format}\")))\n",
    "\n",
    "    if not image_files:\n",
    "        # Show the last 3 images\n",
    "        # recent_images = image_files[-3:]\n",
    "        print(\"No images found to analyze!\")\n",
    "    return image_files\n",
    "\n",
    "def save_result(image_name, model_name, result_text, handler):\n",
    "    #Helper to save analysis to JSON\n",
    "    file_path = handler.analysis_dir / f\"{image_name}_{model_name}.json\"\n",
    "    # file_path = data_dir / f\"{image_name}_{model_name}.json\"\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump({\n",
    "            \"image\": image_name,\n",
    "            \"model\": model_name,\n",
    "            \"output\": result_text\n",
    "        }, f, indent=2)\n",
    "\n",
    "def cleanup_gpu():\n",
    "    #Force garbage collection to free VRAM for the next model\n",
    "    del model, processor\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03755cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_qwen(image_files, prompt, handler):\n",
    "       \n",
    "        # use the 3B model in bfloat16. \n",
    "    # This takes ~7GB VRAM, fitting easily on your 16GB GPU without flaky quantization.\n",
    "    model_id = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
    "    \n",
    "    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.bfloat16,  # Native Half-Precision\n",
    "        device_map=\"cuda\",           # Force specific GPU load\n",
    "        attn_implementation=\"flash_attention_2\" if torch.cuda.get_device_capability()[0] >= 8 else \"eager\"\n",
    "    )\n",
    "    \n",
    "    processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "    #iterate through files and run each image through qwen using prompt   \n",
    "    for img in tqdm(image_files, desc=\"Qwen Analysis\"):\n",
    "        # Check if done\n",
    "        if (handler.analysis_dir / f\"{img.stem}_qwen_vl.json\").exists(): continue\n",
    "\n",
    "        # Prepare Input (Chat Format)\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\", \"image\": str(img)},\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                ],\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        # Preprocess\n",
    "        text_input = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        image_inputs, video_inputs = process_vision_info(messages)\n",
    "        \n",
    "        inputs = processor(\n",
    "            text=[text_input],\n",
    "            images=image_inputs,\n",
    "            videos=video_inputs,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "        # Generate embeddings\n",
    "        generated_ids = model.generate(**inputs, max_new_tokens=256)\n",
    "        \n",
    "        # Decode\n",
    "        generated_ids_trimmed = [\n",
    "            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "        ]\n",
    "        output_text = processor.batch_decode(\n",
    "            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "        )[0]\n",
    "\n",
    "        # Save\n",
    "        save_result(img, \"qwen_vl\", output_text, handler)\n",
    "    del model, processor\n",
    "    cleanup_gpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0a21f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_qwen(image_files, prompt, handler):\n",
    "#     # MODEL A: QWEN 2.5 VL (OCR Expert)\n",
    "#     print(\"\\nLoading Qwen 2.5 VL\")\n",
    "#     model_id = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "#     #Fetch model\n",
    "#     processor = AutoProcessor.from_pretrained(model_id)\n",
    "#     model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "#         model_id, quantization_config=bnb_config, device_map=\"auto\"\n",
    "#     )\n",
    "\n",
    "#     #iterate through files and run each image through qwen using prompt   \n",
    "#     for img in tqdm(image_files, desc=\"Qwen Analysis\"):\n",
    "#         # Check if done\n",
    "#         if (handler.analysis_dir / f\"{img.stem}_qwen_vl.json\").exists(): continue\n",
    "        \n",
    "#         inputs = processor(\n",
    "#             text=[{\"type\": \"image\", \"image\": str(img)}, {\"type\": \"text\", \"text\": prompt}],\n",
    "#             padding=True, return_tensors=\"pt\"\n",
    "#         ).to(\"cuda\")\n",
    "        \n",
    "#         out = model.generate(**inputs, max_new_tokens=200)\n",
    "#         text = processor.batch_decode(out, skip_special_tokens=True)[0]\n",
    "#         save_result(img, \"qwen_vl\", text, handler)\n",
    "        \n",
    "#     del model, processor\n",
    "#     cleanup_gpu()\n",
    "#     return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fa0adc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_phi4(image_files, prompt, handler):\n",
    "    #good for tagging\n",
    "    print(\"\\nLoading Phi-4 Multimodal\")\n",
    "    model_id = \"microsoft/phi-4-multimodal-instruct\"\n",
    "    processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id, device_map=\"cuda\", torch_dtype=\"auto\", trust_remote_code=True\n",
    "    )\n",
    "        \n",
    "    for img in tqdm(image_files, desc=\"Phi Analysis\"):\n",
    "        if (handler.analysis_dir / f\"{img.stem}_phi4.json\").exists(): continue\n",
    "        \n",
    "        image = Image.open(img)\n",
    "        inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(\"cuda\")\n",
    "        out = model.generate(**inputs, max_new_tokens=50)\n",
    "        text = processor.batch_decode(out, skip_special_tokens=True)[0]\n",
    "        save_result(img, \"phi4\", text, handler)\n",
    "\n",
    "    del model, processor\n",
    "    cleanup_gpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43292b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_llama4_scout(image_files, prompt, handler):\n",
    "# LLAMA 4 SCOUT (Reasoning)\n",
    "    print(\"\\n Loading Llama 4 Scout\")\n",
    "    # Note: make sure you have access to this model on Huggingface\n",
    "    model_id = \"meta-llama/Llama-4-Scout-17B-16E\" \n",
    "    processor = AutoProcessor.from_pretrained(model_id)\n",
    "    model = AutoModelForVision2Seq.from_pretrained(\n",
    "        model_id, quantization_config=bnb_config, device_map=\"auto\", trust_remote_code=True\n",
    "    )\n",
    "            \n",
    "    for img in tqdm(image_files, desc=\"Llama Analysis\"):\n",
    "        if (handler.analysis_dir / f\"{img.stem}_llama_scout.json\").exists(): continue\n",
    "        \n",
    "        # Llama Vision input format\n",
    "        image = Image.open(img)\n",
    "        inputs = processor(image, prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        out = model.generate(**inputs, max_new_tokens=150)\n",
    "        text = processor.decode(out[0], skip_special_tokens=True)\n",
    "        save_result(img, \"llama_scout\", text, handler)\n",
    "\n",
    "    del model, processor\n",
    "    cleanup_gpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8207b8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-23 14:07:04,887 - src.output_handler - INFO - Output directories created at /home/jovyan/data-store/AI-ML_PipelineWorkshop/DataCollection/data/generated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading Phi-4 Multimodal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n",
      "  warnings.warn(\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31137b6d761242b580678a64d490d4c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "speech_conformer_encoder.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encountered exception while importing backoff: No module named 'backoff'\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "This modeling file requires the following packages that were not found in your environment: backoff. Run `pip install backoff`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 25\u001b[0m\n\u001b[1;32m     21\u001b[0m image_files \u001b[38;5;241m=\u001b[39m analyze_images(handler)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#run the mllms to compare\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# run_qwen(image_files, prompt, handler)\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[43mrun_phi4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhandler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m run_llama4_scout(image_files, prompt, handler)\n",
      "Cell \u001b[0;32mIn[7], line 6\u001b[0m, in \u001b[0;36mrun_phi4\u001b[0;34m(image_files, prompt, handler)\u001b[0m\n\u001b[1;32m      4\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmicrosoft/phi-4-multimodal-instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m processor \u001b[38;5;241m=\u001b[39m AutoProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m tqdm(image_files, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPhi Analysis\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (handler\u001b[38;5;241m.\u001b[39manalysis_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg\u001b[38;5;241m.\u001b[39mstem\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_phi4.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mexists(): \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:586\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    583\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madapter_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m adapter_kwargs\n\u001b[1;32m    585\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_remote_code \u001b[38;5;129;01mand\u001b[39;00m trust_remote_code:\n\u001b[0;32m--> 586\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m \u001b[43mget_class_from_dynamic_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    589\u001b[0m     _ \u001b[38;5;241m=\u001b[39m hub_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_revision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    590\u001b[0m     \u001b[38;5;66;03m# This block handles the case where the user is loading a model with `trust_remote_code=True`\u001b[39;00m\n\u001b[1;32m    591\u001b[0m     \u001b[38;5;66;03m# but a library model exists with the same name. We don't want to override the autoclass\u001b[39;00m\n\u001b[1;32m    592\u001b[0m     \u001b[38;5;66;03m# mappings in this case, or all future loads of that model will be the remote code model.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/dynamic_module_utils.py:604\u001b[0m, in \u001b[0;36mget_class_from_dynamic_module\u001b[0;34m(class_reference, pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, repo_type, code_revision, **kwargs)\u001b[0m\n\u001b[1;32m    602\u001b[0m     code_revision \u001b[38;5;241m=\u001b[39m revision\n\u001b[1;32m    603\u001b[0m \u001b[38;5;66;03m# And lastly we get the class inside our newly created module\u001b[39;00m\n\u001b[0;32m--> 604\u001b[0m final_module \u001b[38;5;241m=\u001b[39m \u001b[43mget_cached_module_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodule_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.py\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m get_class_in_module(class_name, final_module, force_reload\u001b[38;5;241m=\u001b[39mforce_download)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/dynamic_module_utils.py:467\u001b[0m, in \u001b[0;36mget_cached_module_file\u001b[0;34m(pretrained_model_name_or_path, module_file, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, repo_type, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module_needed \u001b[38;5;129;01min\u001b[39;00m modules_needed:\n\u001b[1;32m    466\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ((submodule_path \u001b[38;5;241m/\u001b[39m module_file)\u001b[38;5;241m.\u001b[39mparent \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_needed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.py\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mexists():\n\u001b[0;32m--> 467\u001b[0m             \u001b[43mget_cached_module_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m                \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_file\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43mmodule_needed\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.py\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m                \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m                \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m                \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m                \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m                \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m                \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    479\u001b[0m             new_files\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_needed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.py\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(new_files) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m revision \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/dynamic_module_utils.py:427\u001b[0m, in \u001b[0;36mget_cached_module_file\u001b[0;34m(pretrained_model_name_or_path, module_file, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, repo_type, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;66;03m# Check we have all the requirements in our environment\u001b[39;00m\n\u001b[0;32m--> 427\u001b[0m modules_needed \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_imports\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresolved_module_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;66;03m# Now we move the module inside our cached dynamic modules.\u001b[39;00m\n\u001b[1;32m    430\u001b[0m full_submodule \u001b[38;5;241m=\u001b[39m TRANSFORMERS_DYNAMIC_MODULE_NAME \u001b[38;5;241m+\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msep \u001b[38;5;241m+\u001b[39m submodule\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/dynamic_module_utils.py:260\u001b[0m, in \u001b[0;36mcheck_imports\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_packages) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 260\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis modeling file requires the following packages that were not found in your environment: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    262\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(missing_packages)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Run `pip install \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(missing_packages)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    263\u001b[0m     )\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m get_relative_imports(filename)\n",
      "\u001b[0;31mImportError\u001b[0m: This modeling file requires the following packages that were not found in your environment: backoff. Run `pip install backoff`"
     ]
    }
   ],
   "source": [
    "#include structured output prompt from larger analysis \n",
    "prompt = \"Only reply with a json file structure. Your response should have the following json structure:\" \\\n",
    "\"labels:{ Label_1: please choose the most probable label from this list of labels for the image: 'protest', 'digital image with text in Spanish', 'digital image with text in English', 'a small group of people', 'an illustration or cartoon', 'solidarity', 'an image of a woman and text', 'an image of a man and text', 'a person or selfie', 'a sign(s) or banner(s)', 'statues, landmarks, buildings', 'informational', 'personal belongings or objects', 'image created by bot'\" \\\n",
    "\"         Label_2: please choose the second most probable label from this list of labels for the image: 'protest', 'digital image with text in Spanish', 'digital image with text in English', 'a small group of people', 'an illustration or cartoon', 'solidarity', 'an image of a woman and text', 'an image of a man and text', 'a person or selfie', 'a sign(s) or banner(s)', 'statues, landmarks, buildings', 'informational', 'personal belongings or objects', 'image created by bot'}\" \\\n",
    "\"Description: please create a description of the image\" \\\n",
    "\"Please include blank labels or descriptions if you are unable to provide them. Do not stray from the json structure.\"\n",
    "\"Please DO NOT include any leading or follow up text or comments, only provide the json file.\" \\\n",
    "\n",
    "\n",
    "# Based on the output handler module,\n",
    "# This automatically creates folders in DataCollection/data/generated: images/, metadata/, logs/\n",
    "handler = output_handler.OutputHandler(\n",
    "    output_dir=cfg.get_output_path(),  # Uses path from generation_config.yaml\n",
    "    image_format=cfg.output.get('format', 'png'),\n",
    "    export_csv=True,\n",
    "    date_organized=True\n",
    ")\n",
    "\n",
    "\n",
    "# Run Analysis\n",
    "image_files = analyze_images(handler)\n",
    "\n",
    "#run the mllms to compare\n",
    "# run_qwen(image_files, prompt, handler)\n",
    "run_phi4(image_files, prompt, handler)\n",
    "run_llama4_scout(image_files, prompt, handler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02505bd1-6250-483c-89c1-b39aabf9dcec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
