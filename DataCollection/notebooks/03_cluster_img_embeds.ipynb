{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9be87135",
   "metadata": {},
   "source": [
    "# Cluster the Image Embeddings and Visualize\n",
    "\n",
    "After generating images in notebook 2, we now cluster them based on their feature embeddings. We can then visualize and evaluate the cluster to see what types of images were generated and how well different types of images were clustered together.\n",
    "\n",
    "*Workshop*: AI/ML Pipeline - Synthetic Data Generation; January 23, 2026  \n",
    "*Platform*: CyVerse Jupyter Lab PyTorch GPU\n",
    "\n",
    "## Pipeline Overview\n",
    "\n",
    "1. Load configuration and source data\n",
    "2. Use CLIP to get embeddings of the synthetic data\n",
    "3. Cluster the embeddings and evaluate those clusters\n",
    "4. Visualize the clusters\n",
    "5. Review results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf393b9-62b5-487b-bcc2-ff5a01989067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just in case\n",
    "# %pip install umap-learn hdbscan scikit-learn matplotlib seaborn transformers torch pillow\n",
    "# %pip install plotly nbformat\n",
    "# #Install the specific version that includes the bundled engine\n",
    "# %pip install \"kaleido==0.2.1\"\n",
    "# or\n",
    "# pip install plotly[kaleido]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643bf49f-6d18-40d9-bb5c-7a567d2871c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "#visualization and density analysis\n",
    "import umap\n",
    "import hdbscan\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"notebook\"\n",
    "\n",
    "# Model\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "from huggingface_hub import login\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "# Device Setup (CLIP is light, so CPU is usually fine, but GPU is faster)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfc0e8b-c21a-4642-a96b-1a8e49713946",
   "metadata": {},
   "source": [
    "### Load Configuration\n",
    "\n",
    "Review and adjust generation parameters if needed.\n",
    "Remember to include your HF user token to access the free models\n",
    "- https://huggingface.co/ > login or signup\n",
    "    - Billing > Access Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbe60c3-d0b4-4d23-a7ab-65fff7cfac4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add parent directory to path\n",
    "parent_dir = Path.cwd().parent\n",
    "if str(parent_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(parent_dir))\n",
    "\n",
    "print(\"bringing in local modules\")\n",
    "# brining in more modeules because of image generation process\n",
    "# modules are found in the DataCollection/src folder\n",
    "# from src import config, gemini_client, data_loader, prompt_builder, output_handler\n",
    "from src import config, data_loader, output_handler\n",
    "\n",
    "print(\"All modules imported successfully\")\n",
    "print(f\"Working directory: {Path.cwd()}\")\n",
    "\n",
    "# Load configuration\n",
    "# DataCollection/src/config.py ... def load_confi()\n",
    "# using \"generation_config.yaml\" for setup\n",
    "cfg = config.load_config()\n",
    "data_dir = cfg.get_data_path('generated')\n",
    "img_dir = cfg.get_data_path('generated/images')\n",
    "output_dir = cfg.get_data_path('generated/analysis')\n",
    "print(img_dir)\n",
    "\n",
    "# Login to HuggingFace just in case it's needed for clip model\n",
    "# login(token = 'your token')\n",
    "# login(token = '')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fe6613",
   "metadata": {},
   "source": [
    "## Create Image Feature Embeddings\n",
    "Using the CLIP model\n",
    "\n",
    "CLIP, or contrastive language-image pretraining, uses a Vision Transformer (ViT) model pre-trained on about 400 million contrastive image text pairs to encode the images. Essentially, the model references learned visual concepts from the natural language pre-training which supports zero-shot transfer for learning on new data (Radford et al, 2021). The CLIP model in this work comes from HuggingFace’s OpenAI and uses these image-text pairs for feature embedding and image classification tasks further down the pipeline. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df9cae5-9e4d-4c66-93a8-31f90f4a1f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_embeddings(image_dir):\n",
    "    \n",
    "    print(\"Loading CLIP model\")\n",
    "    # Use the standard OpenAI CLIP model\n",
    "    model_id = \"openai/clip-vit-base-patch32\"\n",
    "    model = CLIPModel.from_pretrained(model_id).to(device)\n",
    "    processor = CLIPProcessor.from_pretrained(model_id)\n",
    "\n",
    "    # image_files = sorted(list(image_dir.glob(\"*.png\")))\n",
    "    image_files = sorted(list(handler.images_dir.glob(f\"*.{handler.image_format}\")))\n",
    "\n",
    "    embeddings = []\n",
    "    filenames = []\n",
    "    print(f\"Processing {len(image_files)} images\")\n",
    "    \n",
    "    # Process in batches to be memory safe\n",
    "    batch_size = 32\n",
    "    for i in tqdm(range(0, len(image_files), batch_size)):\n",
    "        batch_paths = image_files[i:i + batch_size]\n",
    "        \n",
    "        # Load images\n",
    "        images = [Image.open(p) for p in batch_paths]\n",
    "        \n",
    "        # Preprocess\n",
    "        inputs = processor(images=images, return_tensors=\"pt\", padding=True).to(device)\n",
    "        \n",
    "        # Inference (Get features)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.get_image_features(**inputs)\n",
    "        \n",
    "        # Normalize embeddings (crucial for cosine similarity/clustering)\n",
    "        outputs = outputs / outputs.norm(p=2, dim=-1, keepdim=True)\n",
    "        \n",
    "        # Store as numpy array (move to CPU first)\n",
    "        embeddings.append(outputs.cpu().numpy())\n",
    "        filenames.extend([p.name for p in batch_paths])\n",
    "\n",
    "    # Concatenate all batches\n",
    "    if embeddings:\n",
    "        embeddings = np.vstack(embeddings)\n",
    "        return filenames, embeddings\n",
    "    else:\n",
    "        return [], np.array([])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722bc44e-a641-46ed-b6e4-e34688134202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the output handler module,\n",
    "# This automatically creates folders in DataCollection/data/generated: images/, metadata/, logs/\n",
    "handler = output_handler.OutputHandler(\n",
    "    output_dir=cfg.get_output_path(),  # Uses path from generation_config.yaml\n",
    "    image_format=cfg.output.get('format', 'png'),\n",
    "    export_csv=True,\n",
    "    date_organized=True\n",
    ")\n",
    "# Run Extraction\n",
    "filenames, clip_embeddings = get_image_embeddings(img_dir)\n",
    "print(f\"Extracted Embeddings Shape: {clip_embeddings.shape}\") \n",
    "# Expected shape: (Num_Images, 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ad03cb",
   "metadata": {},
   "source": [
    "## Cluster Embeddings and Evaluate\n",
    "- UMAP, HDBSCAN, Visualize with t-SNE\n",
    "UMAP, or Uniform Manifold Approximation and Projection, is a dimensionality reduction technique that visualizes embeddings (image or text) in a high dimensional space. Essentially, it groups clusters in high-dimensional space, then optimizes embeddings in the clusters to a low-dimensional layout while still preserving the feature distances (McInnes, Healy, Melville,  2018). \n",
    "\n",
    "The HDBSCAN algorithm (HIerarchical Density Based Spatial Clustering of Applications with Noise), is a density based clustering algorithm that finds clusters of various densities instead of trying to fit uniform clusters. It works well for real-world applications that might have imbalanced data. HDBSCAN is a clustering algorithm that groups features using hierarchical partitioning to maximize the stability of the selected clusters while also accounting for noise (Campello, Moulavi, Sander, 2013)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13991166-22a9-4c49-a1ce-ca48a1959633",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_clustering_analysis(embeddings, filenames):\n",
    "    print(\"Running UMAP dimensionality reduction\")\n",
    "    # Reduce to 15 dimensions for density scanning (standard practice)\n",
    "    # n_neighbors: controls how much local structure is preserved\n",
    "    umap_reducer = umap.UMAP(\n",
    "        n_neighbors=15, \n",
    "        n_components=15, \n",
    "        metric='cosine', \n",
    "        random_state=42\n",
    "    )\n",
    "    coords_2d = umap_reducer.fit_transform(embeddings)\n",
    "    \n",
    "    print(\"Running HDBSCAN density clustering\")\n",
    "    # Cluster the reduced data\n",
    "    # min_cluster_size: smallest group to consider a \"cluster\"\n",
    "    # min_samples: how conservative the clustering is\n",
    "    clusterer = hdbscan.HDBSCAN(\n",
    "        min_cluster_size=3, # Low number because you likely have a small test batch\n",
    "        min_samples=1, \n",
    "        metric='euclidean'\n",
    "    )\n",
    "    labels = clusterer.fit_predict(coords_2d)\n",
    "    probabilities = clusterer.probabilities_\n",
    "    print(probabilities)\n",
    "\n",
    "    num_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    print(f\"Found {num_clusters} clusters (Label -1 represents 'noise' or 'outliers')\")\n",
    "    \n",
    "    # Filter out Noise (Label -1) for fair scoring\n",
    "    mask = labels != -1\n",
    "    clustered_data = coords_2d[mask]\n",
    "    clustered_labels = labels[mask]\n",
    "    n_clusters = len(set(clustered_labels))\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(f\"({n_clusters} clusters found)\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    if n_clusters > 1:\n",
    "        # Silhouette: Measure of how similar an object is to its own cluster (cohesion) \n",
    "        # compared to other clusters (separation). Range: -1 to 1.\n",
    "        sil_score = silhouette_score(clustered_data, clustered_labels)\n",
    "        \n",
    "        # Calinski-Harabasz: Ratio of the sum of between-clusters dispersion and \n",
    "        # of within-cluster dispersion. Higher is better.\n",
    "        ch_score = calinski_harabasz_score(clustered_data, clustered_labels)\n",
    "        \n",
    "        print(f\"Silhouette Score:       {sil_score:.4f}  (Target: > 0.5)\")\n",
    "        print(f\"Calinski-Harabasz:      {ch_score:.4f}  (Higher is better)\")\n",
    "    else:\n",
    "        print(\"Not enough clusters (need > 1) to calculate separation scores.\")\n",
    "\n",
    "    # create dataframe\n",
    "    df = pd.DataFrame({\n",
    "        'filename': filenames,\n",
    "        'x': coords_2d[:, 0],\n",
    "        'y': coords_2d[:, 1],\n",
    "        'cluster': labels,\n",
    "        'probability': probabilities\n",
    "    })\n",
    "    \n",
    "    # Sort so 'Cluster 0' is at top, and Noise (-1) is at bottom\n",
    "    df = df.sort_values(by=['cluster', 'probability'], ascending=[True, False])\n",
    "    print(df.head())\n",
    "\n",
    "    # Save the data to CSV so you can inspect which images belong to which cluster\n",
    "    df.to_csv(output_dir /\"cluster_probs.csv\")\n",
    "    \n",
    "    return df, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fd7fe0-683b-43c0-937f-c58df5c3eb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Clustering and Execute the Analysis\n",
    "\n",
    "if len(filenames) > 5:\n",
    "    # cluster_labels = cluster_embeddings(clip_embeddings)\n",
    "    df_results, labels = run_clustering_analysis(clip_embeddings, filenames)\n",
    "    print(\"\\nAnalysis Complete. Data is ready for visualization.\")\n",
    "else:\n",
    "    print(\"Not enough images to run clustering (Need > 5). Using placeholder labels.\")\n",
    "    cluster_labels = np.zeros(len(filenames))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2cf242",
   "metadata": {},
   "source": [
    "## Evaluate Clusters\n",
    "- Silhouette Score\n",
    "- Calinski-Harabasz Index \n",
    "\n",
    " The output of these clusters was evaluated using the Silhouette Score, which represents the density and separation of a cluster. Specifically, the silhouette score is a clustering evaluation metric that evaluates the cluster quality of an algorithm, particularly for unsupervised data. It’s used to study the separation distance between the resulting clusters where a score closer to 1 shows that a particular sample is far away from the neighboring cluster, where a value of 0 shows a sample point is close to a decision boundary between neighboring clusters, the score provides an average value for all samples (Learn, 2017).\n",
    "\n",
    " The Calinski-Harabasz Index (CH) quantifies the ratio of intracluster and intercluster variance, i.e. the ratio of within and between cluster variance. This score helps indicate how compact and well separated a cluster is from other clusters, to create a good distribution (Rachwal et al, 2023). It divides the variance of the sums of squares of the distances of individual data points to their cluster center by the sum of squares to the distance between other cluster centers (Rachwal et al, 2023). A higher CH index shows that clusters are dense and well separated, but there is no index score upper bound, making the score interpretation relative to other metrics (Rachwal et al, 2023)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3887aafb-55af-4827-8ed5-cbd36f39877c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_clusters(df):\n",
    "    print(\"Generating Interactive Graph\")\n",
    "    \n",
    "    # Convert cluster to string so Plotly treats it as a distinct Category (colors), \n",
    "    # rather than a continuous number (gradient)\n",
    "    df['cluster_str'] = df['cluster'].astype(str)\n",
    "    \n",
    "    # Create the Interactive Plot\n",
    "    fig = px.scatter(\n",
    "        df, \n",
    "        x='x', \n",
    "        y='y', \n",
    "        color='cluster_str',\n",
    "        symbol='cluster_str', # Different shapes for different clusters\n",
    "        hover_data=['filename', 'probability'], # Shows up when you mouse over dots\n",
    "        title='Interactive Synthetic Image Map (t-SNE/UMAP Projection of CLIP Embeddings)',\n",
    "        color_discrete_sequence=px.colors.qualitative.G10 # A nice distinct color palette\n",
    "    )\n",
    "    \n",
    "    # Styling for better readability\n",
    "    fig.update_traces(marker=dict(size=10, line=dict(width=1, color='DarkSlateGrey')))\n",
    "    fig.update_layout(\n",
    "        legend_title_text='Cluster ID',\n",
    "        plot_bgcolor='white',\n",
    "        xaxis_title=\"Dimension 1\",\n",
    "        yaxis_title=\"Dimension 2\",\n",
    "        height=700 # Large height for detail\n",
    "    )\n",
    "\n",
    "    # save the visualization\n",
    "    # You can open this file in any web browser later and still zoom/hover\n",
    "    html_path = output_dir / \"cluster_map_interactive.html\"\n",
    "    fig.write_html(str(html_path))\n",
    "    png_path = output_dir / \"cluster_map_static.png\"\n",
    "    fig.write_image(str(png_path), scale=2)\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e869accb-9bb1-4ea2-91fb-612d56059011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df_results.head())\n",
    "# Execute the Visualization\n",
    "visualize_clusters(df_results)\n",
    "\n",
    "# Display Data Tables\n",
    "print(\"\\nHigh-Confidence Cluster Assignments:\")\n",
    "# Show the \"best\" example of each cluster\n",
    "display(df_results[df_results['cluster'] != -1].groupby('cluster').head(2))\n",
    "\n",
    "print(\"\\nOutliers / Noise (Cluster -1):\")\n",
    "display(df_results[df_results['cluster'] == -1].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89fea5c-a472-462e-a50b-aa454f8a79af",
   "metadata": {},
   "source": [
    "### If plotly doesn't work!\n",
    "Use regular matpotlib vizualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476a32d8-57d0-49be-9c7d-f6632b072c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(embeddings, labels, filenames):\n",
    "    print(\"Running t-SNE for visualization\")\n",
    "    \n",
    "    # Adjust perplexity based on dataset size\n",
    "    n_samples = len(embeddings)\n",
    "    perplex = min(30, n_samples - 1) if n_samples > 1 else 1\n",
    "\n",
    "    tsne = TSNE(\n",
    "        n_components=2, \n",
    "        perplexity=perplex, \n",
    "        random_state=42, \n",
    "        init='pca', \n",
    "        learning_rate='auto'\n",
    "    )\n",
    "    tsne_results = tsne.fit_transform(embeddings)\n",
    "    \n",
    "    # Create DataFrame for plotting\n",
    "    df = pd.DataFrame({\n",
    "        'x': tsne_results[:, 0],\n",
    "        'y': tsne_results[:, 1],\n",
    "        'cluster': labels,\n",
    "        'filename': filenames\n",
    "    })\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Scatter plot with HDBSCAN clusters as colors\n",
    "    # We use a distinct color palette. Outliers (-1) usually colored grey/black.\n",
    "    sns.scatterplot(\n",
    "        data=df, \n",
    "        x='x', \n",
    "        y='y', \n",
    "        hue='cluster', \n",
    "        palette='tab10', \n",
    "        style='cluster',\n",
    "        s=100, \n",
    "        alpha=0.8\n",
    "    )\n",
    "    \n",
    "    plt.title('Synthetic Image Landscape (t-SNE Projection of CLIP Embeddings)', fontsize=15)\n",
    "    plt.xlabel('t-SNE Dimension 1')\n",
    "    plt.ylabel('t-SNE Dimension 2')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title=\"HDBSCAN Cluster\")\n",
    "    \n",
    "    # Optional: Label points with filenames (cluttered if too many images)\n",
    "    if len(df) < 55:\n",
    "        for i in range(df.shape[0]):\n",
    "            plt.text(\n",
    "                df.x[i]+0.2, \n",
    "                df.y[i], \n",
    "                df.filename[i], \n",
    "                fontsize=8, \n",
    "                alpha=0.7\n",
    "            )\n",
    "            \n",
    "    # Save Plot\n",
    "    save_path = output_dir / \"tsne_cluster_map.png\"\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"Visualization saved to {save_path}\")\n",
    "    \n",
    "    # return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693ec20f-7e24-4991-8dae-46c53f555d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run clustering Vizualization\n",
    "visualize_results(clip_embeddings, labels, filenames)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
