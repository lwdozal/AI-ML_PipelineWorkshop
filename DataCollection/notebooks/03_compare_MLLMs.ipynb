{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ef07da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U torch diffusers transformers accelerate bitsandbytes sentencepiece qwen-vl-utils\n",
    "\n",
    "#####\n",
    "# For torchvision: https://pytorch.org/get-started/locally/\n",
    "\n",
    "# #windows\n",
    "# %pip install torch torchvision --index-url https://download.pytorch.org/whl/cu126\n",
    "\n",
    "# #mac\n",
    "# %pip install torch torchvision\n",
    "\n",
    "# #linux\n",
    "# %pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9dfef63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import json\n",
    "import gc\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "from huggingface_hub import login\n",
    "from transformers import (\n",
    "    AutoProcessor, \n",
    "    Qwen2_5_VLForConditionalGeneration, \n",
    "    MllamaForConditionalGeneration, # Often used for Llama Vision models\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForVision2Seq,\n",
    "    BitsAndBytesConfig\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d02828",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-22 21:42:21,756 - src.config - INFO - Loaded environment variables from c:\\Users\\lwert\\OneDrive - University of Arizona\\Documents\\Fellowships\\Jetstream\\AI-ML_PipelineWorkshop\\DataCollection\\config\\.env\n",
      "2026-01-22 21:42:21,781 - src.config - INFO - Loaded configuration from c:\\Users\\lwert\\OneDrive - University of Arizona\\Documents\\Fellowships\\Jetstream\\AI-ML_PipelineWorkshop\\DataCollection\\config\\generation_config.yaml\n",
      "2026-01-22 21:42:21,782 - src.config - INFO - Logging configured successfully\n",
      "2026-01-22 21:42:21,804 - src.config - INFO - Loaded environment variables from c:\\Users\\lwert\\OneDrive - University of Arizona\\Documents\\Fellowships\\Jetstream\\AI-ML_PipelineWorkshop\\DataCollection\\config\\.env\n",
      "2026-01-22 21:42:21,823 - src.config - INFO - Loaded configuration from c:\\Users\\lwert\\OneDrive - University of Arizona\\Documents\\Fellowships\\Jetstream\\AI-ML_PipelineWorkshop\\DataCollection\\config\\generation_config.yaml\n",
      "2026-01-22 21:42:21,829 - src.config - INFO - Logging configured successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bringing in local modules\n",
      "All modules imported successfully\n",
      "Working directory: c:\\Users\\lwert\\OneDrive - University of Arizona\\Documents\\Fellowships\\Jetstream\\AI-ML_PipelineWorkshop\\DataCollection\\notebooks\n"
     ]
    }
   ],
   "source": [
    "# Add parent directory to path\n",
    "parent_dir = Path.cwd().parent\n",
    "if str(parent_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(parent_dir))\n",
    "\n",
    "print(\"bringing in local modules\")\n",
    "# brining in more modeules because of image generation process\n",
    "# modules are found in the DataCollection/src folder\n",
    "# from src import config, gemini_client, data_loader, prompt_builder, output_handler\n",
    "from src import config, data_loader, prompt_builder, output_handler\n",
    "\n",
    "print(\"All modules imported successfully\")\n",
    "print(f\"Working directory: {Path.cwd()}\")\n",
    "\n",
    "# Load configuration\n",
    "# DataCollection/src/config.py ... def load_confi()\n",
    "# using \"generation_config.yaml\" for setup\n",
    "cfg = config.load_config()\n",
    "# 4-Bit Config (CRITICAL for running 17B Model on 16GB GPU)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "cfg = config.load_config()\n",
    "data_dir = cfg.get_data_path('generated')\n",
    "\n",
    "\n",
    "# Login to HuggingFace (Required for Llama 4 Scout)\n",
    "login(token = 'your token')\n",
    "# login(token = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e8780e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_images(handler):\n",
    "    # Get the directory where images were just saved\n",
    "    # (handler.images_dir is a Path object from your output_handler.py)\n",
    "    image_files = sorted(list(handler.images_dir.glob(f\"*.{handler.image_format}\")))\n",
    "\n",
    "    if not image_files:\n",
    "        # Show the last 3 images\n",
    "        # recent_images = image_files[-3:]\n",
    "        print(\"No images found to analyze!\")\n",
    "    return image_files\n",
    "\n",
    "def save_result(image_name, model_name, result_text, handler):\n",
    "    \"\"\"Helper to save analysis to JSON\"\"\"\n",
    "    file_path = handler.analysis_dir / f\"{image_name}_{model_name}.json\"\n",
    "    # file_path = data_dir / f\"{image_name}_{model_name}.json\"\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump({\n",
    "            \"image\": image_name,\n",
    "            \"model\": model_name,\n",
    "            \"output\": result_text\n",
    "        }, f, indent=2)\n",
    "\n",
    "def cleanup_gpu():\n",
    "    \"\"\"Force garbage collection to free VRAM for the next model\"\"\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a0a21f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_qwen(image_files, prompt, handler):\n",
    "    # MODEL A: QWEN 2.5 VL (OCR Expert)\n",
    "    print(\"\\nLoading Qwen 2.5 VL\")\n",
    "    model_id = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "    #Fetch model\n",
    "    processor = AutoProcessor.from_pretrained(model_id)\n",
    "    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "        model_id, quantization_config=bnb_config, device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    #iterate through files and run each image through qwen using prompt   \n",
    "    for img in tqdm(image_files, desc=\"Qwen Analysis\"):\n",
    "        # Check if done\n",
    "        if (handler.analysis_dir / f\"{img.stem}_qwen_vl.json\").exists(): continue\n",
    "        \n",
    "        inputs = processor(\n",
    "            text=[{\"type\": \"image\", \"image\": str(img)}, {\"type\": \"text\", \"text\": prompt}],\n",
    "            padding=True, return_tensors=\"pt\"\n",
    "        ).to(\"cuda\")\n",
    "        \n",
    "        out = model.generate(**inputs, max_new_tokens=200)\n",
    "        text = processor.batch_decode(out, skip_special_tokens=True)[0]\n",
    "        save_result(img, \"qwen_vl\", text, handler)\n",
    "        \n",
    "    del model, processor\n",
    "    cleanup_gpu()\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4fa0adc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_phi4(image_files, prompt, handler):\n",
    "    #good for tagging\n",
    "    print(\"\\nLoading Phi-4 Multimodal\")\n",
    "    model_id = \"microsoft/phi-4-multimodal-instruct\"\n",
    "    processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id, device_map=\"cuda\", torch_dtype=\"auto\", trust_remote_code=True\n",
    "    )\n",
    "        \n",
    "    for img in tqdm(image_files, desc=\"Phi Analysis\"):\n",
    "        if (handler.analysis_dir / f\"{img.stem}_phi4.json\").exists(): continue\n",
    "        \n",
    "        image = Image.open(img)\n",
    "        inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(\"cuda\")\n",
    "        out = model.generate(**inputs, max_new_tokens=50)\n",
    "        text = processor.batch_decode(out, skip_special_tokens=True)[0]\n",
    "        save_result(img, \"phi4\", text, handler)\n",
    "\n",
    "    del model, processor\n",
    "    cleanup_gpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43292b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_llama4_scout(image_files, prompt, handler):\n",
    "# LLAMA 4 SCOUT (Reasoning)\n",
    "    print(\"\\n Loading Llama 4 Scout\")\n",
    "    # Note: make sure you have access to this model on Huggingface\n",
    "    model_id = \"meta-llama/Llama-4-Scout-17B-16E\" \n",
    "    processor = AutoProcessor.from_pretrained(model_id)\n",
    "    model = AutoModelForVision2Seq.from_pretrained(\n",
    "        model_id, quantization_config=bnb_config, device_map=\"auto\", trust_remote_code=True\n",
    "    )\n",
    "            \n",
    "    for img in tqdm(image_files, desc=\"Llama Analysis\"):\n",
    "        if (handler.analysis_dir / f\"{img.stem}_llama_scout.json\").exists(): continue\n",
    "        \n",
    "        # Llama Vision input format\n",
    "        image = Image.open(img)\n",
    "        inputs = processor(image, prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        out = model.generate(**inputs, max_new_tokens=150)\n",
    "        text = processor.decode(out[0], skip_special_tokens=True)\n",
    "        save_result(img, \"llama_scout\", text, handler)\n",
    "\n",
    "    del model, processor\n",
    "    cleanup_gpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8207b8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-22 21:39:03,548 - src.output_handler - INFO - Output directories created at c:\\Users\\lwert\\OneDrive - University of Arizona\\Documents\\Fellowships\\Jetstream\\AI-ML_PipelineWorkshop\\DataCollection\\data\\generated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No images found to analyze!\n",
      "\n",
      "Loading Qwen 2.5 VL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nAutoVideoProcessor requires the Torchvision library but it was not found in your environment. Check out the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m image_files = analyze_images(handler)\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m#run the mllms to compare\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[43mrun_qwen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhandler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m run_phi4(image_files, prompt, handler)\n\u001b[32m     26\u001b[39m run_llama4_scout(image_files, prompt, handler)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mrun_qwen\u001b[39m\u001b[34m(image_files, prompt, handler)\u001b[39m\n\u001b[32m      4\u001b[39m model_id = \u001b[33m\"\u001b[39m\u001b[33mQwen/Qwen2.5-VL-7B-Instruct\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m#Fetch model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m processor = \u001b[43mAutoProcessor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n\u001b[32m      8\u001b[39m     model_id, quantization_config=bnb_config, device_map=\u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      9\u001b[39m )\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m#iterate through files and run each image through qwen using prompt   \u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lwert\\OneDrive - University of Arizona\\Documents\\Fellowships\\Jetstream\\AI-ML_PipelineWorkshop\\ai_workshop2026\\Lib\\site-packages\\transformers\\models\\auto\\processing_auto.py:396\u001b[39m, in \u001b[36mAutoProcessor.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m    392\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m processor_class.from_pretrained(\n\u001b[32m    393\u001b[39m         pretrained_model_name_or_path, trust_remote_code=trust_remote_code, **kwargs\n\u001b[32m    394\u001b[39m     )\n\u001b[32m    395\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m processor_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m396\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprocessor_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[38;5;66;03m# Last try: we use the PROCESSOR_MAPPING.\u001b[39;00m\n\u001b[32m    400\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m PROCESSOR_MAPPING:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lwert\\OneDrive - University of Arizona\\Documents\\Fellowships\\Jetstream\\AI-ML_PipelineWorkshop\\ai_workshop2026\\Lib\\site-packages\\transformers\\processing_utils.py:1394\u001b[39m, in \u001b[36mProcessorMixin.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[39m\n\u001b[32m   1391\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1392\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mtoken\u001b[39m\u001b[33m\"\u001b[39m] = token\n\u001b[32m-> \u001b[39m\u001b[32m1394\u001b[39m args = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_arguments_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1395\u001b[39m processor_dict, kwargs = \u001b[38;5;28mcls\u001b[39m.get_processor_dict(pretrained_model_name_or_path, **kwargs)\n\u001b[32m   1396\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.from_args_and_dict(args, processor_dict, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lwert\\OneDrive - University of Arizona\\Documents\\Fellowships\\Jetstream\\AI-ML_PipelineWorkshop\\ai_workshop2026\\Lib\\site-packages\\transformers\\processing_utils.py:1453\u001b[39m, in \u001b[36mProcessorMixin._get_arguments_from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m   1450\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1451\u001b[39m         attribute_class = \u001b[38;5;28mcls\u001b[39m.get_possibly_dynamic_module(class_name)\n\u001b[32m-> \u001b[39m\u001b[32m1453\u001b[39m     args.append(\u001b[43mattribute_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m(pretrained_model_name_or_path, **kwargs))\n\u001b[32m   1455\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m args\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lwert\\OneDrive - University of Arizona\\Documents\\Fellowships\\Jetstream\\AI-ML_PipelineWorkshop\\ai_workshop2026\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2157\u001b[39m, in \u001b[36mDummyObject.__getattribute__\u001b[39m\u001b[34m(cls, key)\u001b[39m\n\u001b[32m   2155\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (key.startswith(\u001b[33m\"\u001b[39m\u001b[33m_\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key != \u001b[33m\"\u001b[39m\u001b[33m_from_config\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m key == \u001b[33m\"\u001b[39m\u001b[33mis_dummy\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m key == \u001b[33m\"\u001b[39m\u001b[33mmro\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m key == \u001b[33m\"\u001b[39m\u001b[33mcall\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   2156\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m2157\u001b[39m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lwert\\OneDrive - University of Arizona\\Documents\\Fellowships\\Jetstream\\AI-ML_PipelineWorkshop\\ai_workshop2026\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2143\u001b[39m, in \u001b[36mrequires_backends\u001b[39m\u001b[34m(obj, backends)\u001b[39m\n\u001b[32m   2140\u001b[39m         failed.append(msg.format(name))\n\u001b[32m   2142\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[32m-> \u001b[39m\u001b[32m2143\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join(failed))\n",
      "\u001b[31mImportError\u001b[39m: \nAutoVideoProcessor requires the Torchvision library but it was not found in your environment. Check out the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "#include structured output prompt from larger analysis \n",
    "prompt = \"Only reply with a json file structure. Your response should have the following json structure:\" \\\n",
    "\"labels:{ Label_1: please choose the most probable label from this list of labels for the image: 'protest', 'digital image with text in Spanish', 'digital image with text in English', 'a small group of people', 'an illustration or cartoon', 'solidarity', 'an image of a woman and text', 'an image of a man and text', 'a person or selfie', 'a sign(s) or banner(s)', 'statues, landmarks, buildings', 'informational', 'personal belongings or objects', 'image created by bot'\" \\\n",
    "\"         Label_2: please choose the second most probable label from this list of labels for the image: 'protest', 'digital image with text in Spanish', 'digital image with text in English', 'a small group of people', 'an illustration or cartoon', 'solidarity', 'an image of a woman and text', 'an image of a man and text', 'a person or selfie', 'a sign(s) or banner(s)', 'statues, landmarks, buildings', 'informational', 'personal belongings or objects', 'image created by bot'}\" \\\n",
    "\"Description: please create a description of the image\" \\\n",
    "\"Please include blank labels or descriptions if you are unable to provide them. Do not stray from the json structure.\"\n",
    "\"Please DO NOT include any leading or follow up text or comments, only provide the json file.\" \\\n",
    "\n",
    "\n",
    "# Based on the output handler module,\n",
    "# This automatically creates folders in DataCollection/data/generated: images/, metadata/, logs/\n",
    "handler = output_handler.OutputHandler(\n",
    "    output_dir=cfg.get_output_path(),  # Uses path from generation_config.yaml\n",
    "    image_format=cfg.output.get('format', 'png'),\n",
    "    export_csv=True,\n",
    "    date_organized=True\n",
    ")\n",
    "\n",
    "\n",
    "# Run Analysis\n",
    "image_files = analyze_images(handler)\n",
    "\n",
    "#run the mllms to compare\n",
    "run_qwen(image_files, prompt, handler)\n",
    "run_phi4(image_files, prompt, handler)\n",
    "run_llama4_scout(image_files, prompt, handler)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_workshop2026",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
