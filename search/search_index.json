{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"AI/ML Pipeline Workshop","text":""},{"location":"#synthetic-data-generation-multimodal-llm-evaluation-and-network-analysis","title":"Synthetic Data Generation, Multimodal LLM Evaluation, and Network Analysis","text":"<p>Workshop Date: January 23, 2026 Funded by: Jetstream2 Platform: CyVerse Discovery Environment</p>"},{"location":"#about-this-workshop","title":"About This Workshop","text":"<p>Welcome to the AI Workbench for Synthetic Data Generation, Generative M-LLM Comparison, and Network Building! This hands-on workshop teaches you how to build an AI/MLOps pipeline that:</p> <ol> <li>Generates synthetic images using Google Gemini API</li> <li>Uses Multimodal Large Language Models (M-LLMs) to generate labels and captions</li> <li>Performs semantic evaluation of generated content</li> <li>Builds network structures to understand thematic representations</li> </ol> <p>This workshop focuses on a social movement case study, using synthetic data generation to address data privacy concerns while maintaining research value.</p>"},{"location":"#workshop-registration","title":"Workshop Registration","text":"<p>Enroll in the Workshop to access the CyVerse Discovery Environment and all workshop resources.</p>"},{"location":"#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this workshop, you will understand:</p> <ol> <li>AI/MLOps pipeline creation using Open Source CARE principles</li> <li>Synthetic data generation techniques</li> <li>Multimodal Large Language Models (M-LLMs)</li> <li>Centrality measures for network analysis</li> <li>Human-in-the-Loop evaluation methods</li> </ol>"},{"location":"#workshop-structure","title":"Workshop Structure","text":"<p>The workshop follows three main stages:</p>"},{"location":"#1-data-collection-and-evaluation","title":"1. Data Collection and Evaluation","text":"<p>Generate synthetic images based on a fictional scenario combining: - Atropia data (fictional country news from U.S. military training) - World Bank synthetic data for an imaginary country - Public social movement images as visual references</p>"},{"location":"#2-model-development","title":"2. Model Development","text":"<ul> <li>Train and fine-tune models on data subsets</li> <li>Generate image labels and captions using M-LLMs</li> <li>Evaluate model performance using multiple metrics</li> <li>Process text data (lemmatization, emoji translation, hashtag normalization)</li> </ul>"},{"location":"#3-model-deployment","title":"3. Model Deployment","text":"<ul> <li>Build semantic similarity networks</li> <li>Perform community detection analysis</li> <li>Visualize thematic patterns and relationships</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<ol> <li>Create a CyVerse Account</li> <li>Set up your environment</li> <li>Follow the workflow tutorials</li> </ol>"},{"location":"#additional-resources","title":"Additional Resources","text":"<ul> <li>CyVerse Learning Center</li> <li>Workshop Registration</li> <li>Project Repository</li> </ul>"},{"location":"#about-the-author","title":"About the Author","text":"<p>Laura Dozal PhD Candidate, College of Information University of Arizona</p>"},{"location":"#support-and-acknowledgments","title":"Support and Acknowledgments","text":"<p>This workshop is supported by: - Jetstream2 - CyVerse - The Data Science Institute at the University of Arizona</p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the GNU General Public License v3.0.</p> <p>Note: This workshop uses synthetic data due to Instagram privacy policies. Original social movement data is not shared to protect user privacy while maintaining educational value.</p>"},{"location":"Data_Management_Plan/","title":"Data Management Plan","text":"<p>This document takes into consideration data that would be collected and made available for dplPy and dendrochronologist to use.</p>"},{"location":"Data_Management_Plan/#1-data-description","title":"1. Data Description","text":"<p>We will collect tree ring measurements from various tree species in our study area. Data will include tree ring width measurements, corresponding years, and species information. These measurements will be stored in structured CSV and RWL files, with metadata indicating sample location, collection date, and field methods.</p>"},{"location":"Data_Management_Plan/#2-data-collection","title":"2. Data Collection","text":"<p>Tree cores will be extracted from trees using increment borers. Ring width measurements will be taken using specialized equipment. Species identification will be done based on field observations and confirmed through lab analysis.</p>"},{"location":"Data_Management_Plan/#3-data-storage-and-backup","title":"3. Data Storage and Backup","text":"<p>Tree ring data and associated metadata will be stored in a dedicated directory on our secure research server. Automated daily backups will be conducted to ensure data integrity.</p>"},{"location":"Data_Management_Plan/#4-data-organization-and-documentation","title":"4. Data Organization and Documentation","text":"<p>Data will be organized by sample location, species, and collection year. Each dataset will have an accompanying metadata file detailing data structure, variable definitions, and data processing steps. </p> <p>(These metadata files are obtainable when downloading data through the NOAA tree ring archive.)</p>"},{"location":"Data_Management_Plan/#5-data-preservation-and-long-term-access","title":"5. Data Preservation and Long-Term Access","text":"<p>Upon project completion, the dataset will be deposited in a recognized tree ring database or repository, ensuring long-term preservation and discoverability (NOAA). The repository will assign a Digital Object Identifier (DOI) for reliable referencing.</p>"},{"location":"Data_Management_Plan/#6-data-sharing-and-access","title":"6. Data Sharing and Access","text":"<p>After a brief embargo period to allow for primary analysis, the dataset will be openly accessible through the repository under a Creative Commons Attribution (CC-BY) license, promoting data reuse with proper attribution.</p>"},{"location":"Data_Management_Plan/#7-data-ethics-and-legal-compliance","title":"7. Data Ethics and Legal Compliance","text":"<p>Ethics approval has been obtained for the study. All data collection follows ethical guidelines, and any potential impact on tree populations or ecosystems will be minimized.</p>"},{"location":"Data_Management_Plan/#8-roles-and-responsibilities","title":"8. Roles and Responsibilities","text":"<p>The lead dendrochronologist will oversee data management, ensuring accuracy and quality. Field technicians will collect core samples and measurements. The institution's data management team will facilitate repository submission.</p>"},{"location":"Data_Management_Plan/#9-budget-and-resources","title":"9. Budget and Resources","text":"<p>Funding has been allocated for specialized dendrochronology equipment, data storage, repository fees, and data management software.</p> <p>This Data Management Plan ensures responsible collection, storage, and sharing of tree ring data, in adherence to dendrochronological best practices and ethical standards. It promotes the transparency, reproducibility, and accessibility of tree ring research.</p>"},{"location":"Governance_Operations/","title":"dplPy-for-FOSS Governance and Operations Document","text":"<p>This is a living document. Changes are expected throughout the life of the project.</p>"},{"location":"Governance_Operations/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction<ul> <li>Project Overview and Objectives<ul> <li>dplPy</li> <li>FOSS Reference Hub</li> </ul> </li> <li>Our Team<ul> <li>Organizational Structure</li> </ul> </li> </ul> </li> <li>Operations<ul> <li>Communications</li> <li>Procedures</li> </ul> </li> <li>Community Practices<ul> <li>Open Science Commitment</li> <li>Land Acknowledgement</li> <li>Diversity Statement</li> <li>Code of Conduct</li> </ul> </li> <li>Attribution, Authorship, and Ownership</li> </ol>"},{"location":"Governance_Operations/#introduction","title":"Introduction","text":"<p>This Project Governance document defines operating procedures and rules aimed to clarify and support the involvement of dplPy in FOSS and the FOSS reference hub.</p>"},{"location":"Governance_Operations/#project-overview-and-objectives","title":"Project Overview and Objectives","text":"<p>This section highights the dplPy project on its own and its implementation in the FOSS Reference Hub.</p>"},{"location":"Governance_Operations/#dplpy","title":"dplPy","text":"<p>dplPy is the Python implementation of the OpenDendro project, an open-source framework that aims to provide dendrochronologists with the required analytical software necessary for tree-ring related research. The majority of the code for dplPy was developed by Ifeoluwa Ale, a student Computer Scienctist at the University of Arizona, and supervised by Michele Cosi, Dr. Tyson Swetnam, and Dr. Kevin Anchukaitis. OpenDendro is supervised by Dr. Kevin Anchukaitis, Andy Bunn, Dr. Tyson Swetnam and Dr. Edward Cook.</p>"},{"location":"Governance_Operations/#foss-reference-hub","title":"FOSS Reference Hub","text":"<p>The FOSS reference hub is a reference repository for the Foundational Open Science Skills (FOSS) workshop series. Together with dplPy, the aim of this repository is to serve as a reference for the capstone, which FOSS attendees are expected to complete within the duration of the FOSS workshop.</p> <p>The dplPy project was selected to enhance the FOSS Reference Hub with a reliable codebase, ensuring it mirrors a genuine repository that participants can use as a model when attempting to replicate similar setups for their individual projects.</p> <p>The goals of the FOSS Reference Hub are to: - Serve as an exemplary reference for a well structured research object. - Guide FOSS attendees through weekly exercises designed to progressively enhance their project repositories.</p>"},{"location":"Governance_Operations/#our-team","title":"Our Team","text":"<p>Meet the education team at CyVerse/Data Science Institute:</p> <ul> <li>Tyson Swetnam: Co-PI of CyVerse and Director of Open Science, Institute for Computation and Data-enabled Insight (ICDI) at the University of Arizona</li> <li>Jeff Gillan: Data Scientist for CyVerse with relative research in the field of GIS/remote sensing.</li> <li>Carlos Lizarraga: Educator in Computational and Data Science at the University of Arizona Data Science Institute. Carlos' background is in applied research scientist as a Professor from the Physics Department at the University of Sonora, where he taught Computational Physics.</li> <li>Michele Cosi: Science Analyst for CyVerse with a background in genetics and genomics of rice.</li> <li>Tina Lee: Head of User Engagement at CyVerse.</li> </ul> <p>With the collaboration of the dplPy team: </p> <ul> <li>Andy Bunn: Environmental scientist interested in climate and energy from the College of Environment at Western Washington University, founding director of the Institute for Energy Studies, and main writer of OpenDendro software dplR and xDateR. </li> <li>Edward Cook: expert in the dendrochronology field and main writer of the tool ARSTAN. </li> <li>Kevin Anchukaitis: Climate scientist, paleoclimatologist, dendrochronologist, faculty member  in Geosciences and the Laboratory of Tree-Ring Research at the University of Arizona, leading the efforts behind the OpenDendro platform.</li> <li>Ifeuwa Ale: Junior Computer Science student at the University of Arizona and main contributor to the dplPy codebase for OpenDendro.</li> </ul> <p>FOSS will also be hosting the following guest speakers:</p> <ul> <li>Jason Williams: Assistant Director, Diversity and Research Readiness at the  DNA Learning Center of Cold Spring Harbor.</li> <li>Wade Bishop: Professor in the School of Information Sciences at the University of Tennessee-Knoxville. He is the Director of Graduate Studies as well as the Research Data Management Certificate Coordinator</li> </ul>"},{"location":"Governance_Operations/#organizational-structure","title":"Organizational Structure","text":"<pre><code>FOSS\n\u251c\u2500\u2500 CyVerse/Data Science Institute\n\u2502   \u251c\u2500\u2500 Tyson Swetnam \n\u2502   \u251c\u2500\u2500 Jeff Gillan\n\u2502   \u251c\u2500\u2500 Carlos Lizarraga\n\u2502   \u251c\u2500\u2500 Tina Lee\n\u2502   \u2514\u2500\u2500 Michele Cosi\n\u251c\u2500\u2500 FOSS Reference Hub\n\u2502   \u2514\u2500\u2500 dplPy\n\u2502        \u251c\u2500\u2500 Kevin Anchukaitis\n\u2502        \u251c\u2500\u2500 Edward Cook\n\u2502        \u251c\u2500\u2500 Andy Bunn\n\u2502        \u2514\u2500\u2500 Ifeuwa Ale\n\u2514\u2500\u2500 Guest Speakers\n    \u251c\u2500\u2500 Jason Williams\n    \u2514\u2500\u2500 Wade Bishop\n</code></pre>"},{"location":"Governance_Operations/#operations","title":"Operations","text":"<p>In order to ensure well distributed work loads across FOSS and the FOSS teaching materials including the FOSS Reference Hub, we have outlined communications and procedure components in order to maximize effectiveness and efficiency.</p>"},{"location":"Governance_Operations/#communications","title":"Communications","text":"<p>Internal communications: communications within the CyVerse/Data Science Institute team is carried out through private Slack channels (CyVerse, CyVerseLearning, Data7) and Zoom meetings.</p> <p>External communications: communications to the dplPy team is carried out through a private Slack channel (openDendro), whilst communication to the guest speakers is done through email (Wade Bishop) and the CyVerse/CyverseLearning Slack Channel (Jason Williams).</p> <p>Note keeping &amp; attendees: communications to FOSS attendees is achieved through the CyVerseLearning Slack channel. This will allow attendees to share thoughts and notes on lectures and materials during and after the workshop. Note keeping during the workshop is carried out through HackMD, which links are shared during the workshop sessions. All workshops will be done through Zoom, creating a virtual classroom environment allowing remote attendees to take part to the workshop. Recordings of these workshop sessions will be made available to attendees through a private YouTube playlist. </p>"},{"location":"Governance_Operations/#procedures","title":"Procedures","text":"<p>All of the code base required for FOSS, the FOSS Reference Hub and the dplPy project is stored in their respective GitHub repositories:</p> <ul> <li>FOSS</li> <li>FOSS Reference Hub</li> <li>dplPy </li> </ul> <p>All of these repositories can only be modified by the CyVerse/Data Science Institute team (or the dplPy team regarding the dplPy repository) or through a reviewed pull request. Websites for FOSS (foss.cyverse.org) and OpenDendro (opendendro.org) are created using MkDocs-Material and hosted on GitHub Pages, built through automated GitHub Actions.</p>"},{"location":"Governance_Operations/#community-practices","title":"Community Practices","text":""},{"location":"Governance_Operations/#open-science-commitment","title":"Open Science Commitment","text":"<p>We are committed to Open Science as we operate through transparency, collaboration, and accessibility, guided by theFAIR (Findable, Accessible, Interoperable, Reusable) and CARE (Collective Benefit, Accountability, Responsibility, Equity) principles.</p> <p>Our educational materials and publications are thoughtfully curated to align with Open Science ideals. We seek to empower others to adopt and implement Open Science practices, making research more accessible and reproducible.</p>"},{"location":"Governance_Operations/#land-acknowledgement","title":"Land Acknowledgement","text":"<p>As part of the University of Arizona, we acknowledge and respect the land and territories on which we are able to perform our duties as educators with the following statement: We respectfully acknowledge the University of Arizona is on the land and territories of Indigenous peoples. Today, Arizona is home to 22 federally recognized tribes, with Tucson being home to the O\u2019odham and the Yaqui. Committed to diversity and inclusion, the University strives to build sustainable relationships with sovereign Native Nations and Indigenous communities through education offerings, partnerships, and community service.</p>"},{"location":"Governance_Operations/#diversity-statement","title":"Diversity Statement","text":"<p>We encourage everyone to participate and are committed to building a community for all.  Although we will fail attimes, we seek to treat everyone as fairly and equally as possible. Whenever a participant has made a mistake, we expect them to take responsibility for it. If someone has been harmed or offended, if you are a witness to an event of this nature, it is your responsibility to both listen carefully and respectfully to the victims, as well as to take action to bring attention to the behaviour, and to make every effort to stop the behaviour. </p> <p>It is also our responsibility to do our best to right the wrong.  Although this list cannot be exhaustive, we explicitly honor diversity in age, gender, gender identity or expression, culture, ethnicity, language, national origin, political beliefs, profession, race, religion, sexual orientation, socioeconomic status, and technical ability. We will not tolerate discrimination based on any of the protected characteristics above, including participants with disabilities.</p>"},{"location":"Governance_Operations/#code-of-conduct","title":"Code of Conduct","text":"<p>In conjunction with for using CyVerse cyberinfrastructure, this Code of Conduct applies to all Event participants and their activities while using CyVerse resources and/or attending the Event.</p> <p>CyVerse is dedicated to providing professional computational research and educational experiences for all of our users, regardless of domain focus, academic status, educational level, gender/gender identity/expression, age, sexual orientation, mental or physical ability, physical appearance, body size, race, ethnicity, religion (or lack thereof), technology choices, dietary preferences, or any other personal characteristic.</p> <p>When using CyVerse or participating at an Event, we expect you to:</p> <p>Interact with others and use CyVerse professionally and ethically by complying with our Policies. Constructively critize ideas and processes, not people. Follow the Golden Rule (treat others as you want to be treated) when interacting online or in-person with collaborators, trainers, and support staff. Comply with this Code in spirit as much as the letter, as it is neither exhaustive nor complete in identifying any and all possible unacceptable conduct. We do not tolerate harassment of other users or staff in any form (including, but not limited to, violent threats or language, derogatory language or jokes, doxing, insults, advocating for or encouraging any of these behaviors). Sexual language and imagery are not appropriate at any time (excludes Protected Health Information in compliance with HIPAA). Any user violating this Code may be expelled from the platform and the workshop at CyVerse's sole discretion without warning.</p> <p>To report a violation of this Code, directly message a trainer via Slack or email info@cyverse.org with the following information:</p> <p>Your contact information Names (real, username, pseudonyms) of any individuals involved, and or witness(es) if any. Your account of what occurred and if the incident is ongoing. If there is a publicly available record (a tweet, public chat log, etc.), please include a link or attachment. Any additional information that may be helpful in resolving the issue.</p>"},{"location":"Governance_Operations/#attribution-authorship-and-ownership","title":"Attribution, Authorship, and Ownership","text":"<p>FOSS teaching materials and FOSS Reference Hub are made available under the CC BY 4.0 License: https://creativecommons.org/licenses/by/4.0/legalcode.</p> <p>dplPy and OpenDendro are made available under the GNU General Public Licence v3.0: https://creativecommons.org/licenses/by/4.0/legalcode</p> <p>We kindly ask to respect the Licences by which this material is developed.</p>"},{"location":"cyverse_account/","title":"Creating a CyVerse Account","text":"<p>CyVerse provides powerful cyberinfrastructure for data-driven discovery. To participate in this workshop, you'll need a CyVerse account to access the Discovery Environment where we'll run our AI/ML pipeline.</p>"},{"location":"cyverse_account/#what-is-cyverse","title":"What is CyVerse?","text":"<p>CyVerse is a cyberinfrastructure platform funded by the National Science Foundation that provides:</p> <ul> <li>Computational resources for data analysis</li> <li>Cloud storage for research data</li> <li>Pre-configured analysis environments</li> <li>Access to high-performance computing through partnerships with Jetstream2</li> <li>Educational resources and training materials</li> </ul>"},{"location":"cyverse_account/#account-creation-steps","title":"Account Creation Steps","text":""},{"location":"cyverse_account/#1-navigate-to-cyverse-user-portal","title":"1. Navigate to CyVerse User Portal","text":"<p>Visit https://user.cyverse.org</p>"},{"location":"cyverse_account/#2-click-sign-up","title":"2. Click \"Sign Up\"","text":"<p>Look for the registration or sign-up button on the homepage.</p>"},{"location":"cyverse_account/#3-complete-the-registration-form","title":"3. Complete the Registration Form","text":"<p>You'll need to provide:</p> <ul> <li>Username: Choose a unique username (alphanumeric characters and underscores)</li> <li>Email address: Use a valid email you can access</li> <li>Password: Create a strong password</li> <li>First and Last Name: Your full name</li> <li>Institution: Your affiliated organization or institution</li> <li>Optional information: Research interests, department, etc.</li> </ul>"},{"location":"cyverse_account/#4-verify-your-email","title":"4. Verify Your Email","text":"<p>After submitting the form:</p> <ol> <li>Check your email inbox for a verification message from CyVerse</li> <li>Click the verification link in the email</li> <li>Your account will be activated</li> </ol>"},{"location":"cyverse_account/#5-complete-your-profile","title":"5. Complete Your Profile","text":"<p>Once verified, log in and complete your user profile:</p> <ul> <li>Add additional contact information</li> <li>Describe your research interests</li> <li>Accept the terms of service</li> </ul>"},{"location":"cyverse_account/#workshop-registration","title":"Workshop Registration","text":"<p>After creating your CyVerse account, you must register for the workshop:</p> <p>Register for the AI/ML Pipeline Workshop</p> <p>Workshop registration ensures you have:</p> <ul> <li>Access to workshop-specific resources</li> <li>Allocated compute time on Jetstream2</li> <li>Workshop materials and datasets</li> <li>Support during the workshop</li> </ul>"},{"location":"cyverse_account/#accessing-the-discovery-environment","title":"Accessing the Discovery Environment","text":"<p>Once your account is created and you're registered for the workshop:</p> <ol> <li>Navigate to https://de.cyverse.org</li> <li>Log in with your CyVerse credentials</li> <li>You'll see the Discovery Environment dashboard</li> </ol>"},{"location":"cyverse_account/#launch-jupyter-lab","title":"Launch Jupyter Lab","text":"<p>For this workshop, you'll use the Jupyter Lab PyTorch GPU environment:</p> <ol> <li>Click on \"Apps\" in the left sidebar</li> <li>Look for \"Instant Launches\" section</li> <li>Find and click \"Jupyter Lab PyTorch GPU\"</li> <li>Wait for the environment to load (this may take a few minutes)</li> </ol>"},{"location":"cyverse_account/#troubleshooting","title":"Troubleshooting","text":""},{"location":"cyverse_account/#email-verification-issues","title":"Email Verification Issues","text":"<p>If you don't receive the verification email:</p> <ul> <li>Check your spam/junk folder</li> <li>Make sure you entered the correct email address</li> <li>Request a new verification email through the user portal</li> <li>Contact CyVerse support at support@cyverse.org</li> </ul>"},{"location":"cyverse_account/#login-problems","title":"Login Problems","text":"<p>If you can't log in:</p> <ul> <li>Verify your username and password</li> <li>Try resetting your password</li> <li>Clear your browser cache and cookies</li> <li>Contact support if problems persist</li> </ul>"},{"location":"cyverse_account/#workshop-access-issues","title":"Workshop Access Issues","text":"<p>If you can't access workshop resources:</p> <ul> <li>Confirm you've registered for the workshop at the registration link</li> <li>Allow up to 24 hours for registration processing</li> <li>Contact the workshop organizer or CyVerse support</li> </ul>"},{"location":"cyverse_account/#additional-resources","title":"Additional Resources","text":"<ul> <li>CyVerse Learning Center: Tutorials and documentation</li> <li>CyVerse User Manual: Comprehensive guides</li> <li>CyVerse Support: Technical assistance</li> <li>CyVerse Status: Check system status</li> </ul>"},{"location":"cyverse_account/#getting-help","title":"Getting Help","text":"<p>If you need assistance:</p> <ol> <li>During the workshop: Ask the instructor or teaching assistants</li> <li>Email support: support@cyverse.org</li> <li>Documentation: Check the CyVerse Learning Center</li> <li>Community: Join the CyVerse community forums</li> </ol>"},{"location":"cyverse_account/#data-privacy-and-security","title":"Data Privacy and Security","text":"<p>CyVerse takes data privacy seriously:</p> <ul> <li>Your data is stored securely</li> <li>You control who has access to your files</li> <li>Read the CyVerse Data Policy for details</li> <li>Follow best practices for sensitive data handling</li> </ul> <p>Once you've created your account and registered for the workshop, proceed to the Setup Guide to configure your environment.</p>"},{"location":"data_collection/","title":"Data Collection and Synthetic Data Generation","text":"<p>The first stage of our AI/ML pipeline focuses on generating synthetic image data using Google Gemini API. This approach allows us to create a realistic dataset while respecting privacy concerns.</p>"},{"location":"data_collection/#overview","title":"Overview","text":"<p>In this module, you'll learn how to:</p> <ul> <li>Generate synthetic images based on a fictional scenario</li> <li>Create descriptive prompts for image generation</li> <li>Evaluate synthetic data quality</li> <li>Prepare datasets for downstream ML tasks</li> </ul>"},{"location":"data_collection/#why-synthetic-data","title":"Why Synthetic Data?","text":"<p>This workshop uses synthetic data for several important reasons:</p> <ol> <li>Privacy Protection: Original social movement data from Instagram cannot be shared due to privacy policies</li> <li>Ethical Research: Synthetic data allows teaching without exposing real individuals</li> <li>Controlled Scenarios: We can create specific scenarios for educational purposes</li> <li>Reproducibility: Everyone generates similar datasets following the same process</li> </ol>"},{"location":"data_collection/#data-sources","title":"Data Sources","text":"<p>Our synthetic scenario combines three data sources:</p>"},{"location":"data_collection/#1-atropia-data","title":"1. Atropia Data","text":"<p>Fictional country news and events from U.S. military training exercises. Atropia is an imaginary country used for military simulations, providing realistic geopolitical scenarios.</p>"},{"location":"data_collection/#2-world-bank-synthetic-data","title":"2. World Bank Synthetic Data","text":"<p>Economic and demographic data for an imaginary country, providing context for social conditions and potential triggers for social movements.</p>"},{"location":"data_collection/#3-public-social-movement-images","title":"3. Public Social Movement Images","text":"<p>Real public images from various social movements serve as visual references for the types of scenes, compositions, and elements to include in synthetic images.</p>"},{"location":"data_collection/#workflow","title":"Workflow","text":""},{"location":"data_collection/#step-1-set-up-data-collection-environment","title":"Step 1: Set Up Data Collection Environment","text":"<p>Navigate to the DataCollection folder:</p> <pre><code>cd /home/jovyan/data-store/AI-ML_PipelineWorkshop/DataCollection\n</code></pre> <p>Ensure your virtual environment is activated and Google Gemini API key is configured (see Setup Guide).</p>"},{"location":"data_collection/#step-2-open-the-generation-notebook","title":"Step 2: Open the Generation Notebook","text":"<p>Open <code>Generate_images.ipynb</code> in Jupyter Lab:</p> <ol> <li>In the file browser, navigate to <code>DataCollection/</code></li> <li>Double-click <code>Generate_images.ipynb</code></li> <li>Select your kernel (workshop_env)</li> </ol>"},{"location":"data_collection/#step-3-understand-the-generation-process","title":"Step 3: Understand the Generation Process","text":"<p>The notebook guides you through:</p>"},{"location":"data_collection/#a-scenario-development","title":"A. Scenario Development","text":"<ul> <li>Review the fictional Atropia scenario</li> <li>Understand the social movement context</li> <li>Identify key visual elements to generate</li> </ul>"},{"location":"data_collection/#b-prompt-engineering","title":"B. Prompt Engineering","text":"<p>Learn to craft effective prompts for image generation:</p> <ul> <li>Specificity: Detailed descriptions produce better results</li> <li>Context: Include setting, atmosphere, and mood</li> <li>Visual elements: Specify people, objects, actions, and composition</li> <li>Style considerations: Artistic style, lighting, perspective</li> </ul> <p>Example prompt structure: <pre><code>A [setting] with [people/objects] [performing action].\n[Lighting/atmosphere]. [Composition details].\nStyle: [photorealistic/artistic style].\n</code></pre></p>"},{"location":"data_collection/#c-image-generation","title":"C. Image Generation","text":"<p>Use Google Gemini API to generate images:</p> <pre><code>import google.generativeai as genai\nimport os\nfrom dotenv import load_dotenv\n\n# Load API key\nload_dotenv()\ngenai.configure(api_key=os.getenv('GOOGLE_API_KEY'))\n\n# Configure model\nmodel = genai.GenerativeModel('gemini-pro-vision')\n\n# Generate image\nprompt = \"Your detailed prompt here\"\nresponse = model.generate_content(prompt)\n\n# Save image\nwith open('output_image.png', 'wb') as f:\n    f.write(response.image)\n</code></pre>"},{"location":"data_collection/#d-batch-generation","title":"D. Batch Generation","text":"<p>Generate multiple images with variation:</p> <ul> <li>Create prompt templates</li> <li>Add variation parameters</li> <li>Generate image sets</li> <li>Organize output files</li> </ul>"},{"location":"data_collection/#step-4-quality-assessment","title":"Step 4: Quality Assessment","text":"<p>Evaluate generated images using these criteria:</p>"},{"location":"data_collection/#visual-quality","title":"Visual Quality","text":"<ul> <li>Resolution and clarity</li> <li>Realistic appearance</li> <li>Appropriate composition</li> <li>Consistent style</li> </ul>"},{"location":"data_collection/#content-relevance","title":"Content Relevance","text":"<ul> <li>Matches scenario description</li> <li>Includes specified elements</li> <li>Appropriate context and setting</li> <li>Culturally appropriate</li> </ul>"},{"location":"data_collection/#dataset-balance","title":"Dataset Balance","text":"<ul> <li>Variety of scenes and scenarios</li> <li>Different perspectives and compositions</li> <li>Range of lighting conditions</li> <li>Diverse representation</li> </ul>"},{"location":"data_collection/#step-5-data-organization","title":"Step 5: Data Organization","text":"<p>Organize your generated dataset:</p> <pre><code>DataCollection/\n\u251c\u2500\u2500 generated_images/\n\u2502   \u251c\u2500\u2500 protest_scenes/\n\u2502   \u251c\u2500\u2500 gatherings/\n\u2502   \u251c\u2500\u2500 individual_portraits/\n\u2502   \u2514\u2500\u2500 contextual_scenes/\n\u251c\u2500\u2500 metadata/\n\u2502   \u251c\u2500\u2500 image_prompts.json\n\u2502   \u251c\u2500\u2500 generation_parameters.json\n\u2502   \u2514\u2500\u2500 quality_scores.csv\n\u2514\u2500\u2500 documentation/\n    \u2514\u2500\u2500 generation_log.md\n</code></pre>"},{"location":"data_collection/#evaluation-methods","title":"Evaluation Methods","text":""},{"location":"data_collection/#spot-checking","title":"Spot Checking","text":"<p>Manually review a sample of generated images:</p> <ol> <li>Select 10-15 random images</li> <li>Evaluate against quality criteria</li> <li>Identify common issues</li> <li>Refine prompts if needed</li> </ol>"},{"location":"data_collection/#automated-metrics","title":"Automated Metrics","text":"<p>Use computational methods to assess quality:</p> <pre><code>from PIL import Image\nimport numpy as np\n\ndef assess_image_quality(image_path):\n    img = Image.open(image_path)\n\n    # Resolution check\n    width, height = img.size\n    resolution_score = min(width * height / (1920 * 1080), 1.0)\n\n    # Color distribution\n    img_array = np.array(img)\n    color_variance = np.std(img_array)\n\n    # Brightness\n    brightness = np.mean(img_array)\n\n    return {\n        'resolution_score': resolution_score,\n        'color_variance': color_variance,\n        'brightness': brightness\n    }\n</code></pre>"},{"location":"data_collection/#human-in-the-loop-evaluation","title":"Human-in-the-Loop Evaluation","text":"<p>Involve human judgment for nuanced assessment:</p> <ul> <li>Rate image relevance (1-5 scale)</li> <li>Identify inappropriate content</li> <li>Assess cultural sensitivity</li> <li>Verify scenario alignment</li> </ul>"},{"location":"data_collection/#best-practices","title":"Best Practices","text":""},{"location":"data_collection/#prompt-engineering-tips","title":"Prompt Engineering Tips","text":"<ol> <li>Be specific: \"A crowded urban plaza during sunset\" vs \"A place with people\"</li> <li>Include context: Add temporal, spatial, and emotional context</li> <li>Iterate: Refine prompts based on results</li> <li>Document: Keep track of successful prompts</li> </ol>"},{"location":"data_collection/#data-management","title":"Data Management","text":"<ol> <li>Version control: Track prompt versions and parameters</li> <li>Metadata: Record generation details for each image</li> <li>Quality logs: Document evaluation results</li> <li>Backup: Save generated data in multiple locations</li> </ol>"},{"location":"data_collection/#ethical-considerations","title":"Ethical Considerations","text":"<ol> <li>Avoid stereotypes: Ensure diverse and respectful representations</li> <li>Cultural sensitivity: Review images for cultural appropriateness</li> <li>Transparency: Clearly label data as synthetic</li> <li>Purpose limitation: Use data only for stated educational purposes</li> </ol>"},{"location":"data_collection/#outputs","title":"Outputs","text":"<p>By the end of this module, you should have:</p> <ul> <li> A dataset of 50-100 synthetic images</li> <li> Metadata file with prompts and parameters</li> <li> Quality assessment scores</li> <li> Documentation of the generation process</li> </ul>"},{"location":"data_collection/#next-steps","title":"Next Steps","text":"<p>Once you've generated and evaluated your synthetic dataset:</p> <ol> <li>Prepare data for model training</li> <li>Proceed to Model Development</li> <li>Begin generating labels and captions using M-LLMs</li> </ol>"},{"location":"data_collection/#additional-resources","title":"Additional Resources","text":"<ul> <li>Google Gemini API Documentation</li> <li>Prompt Engineering Guide</li> <li>Synthetic Data Best Practices</li> </ul>"},{"location":"data_collection/#troubleshooting","title":"Troubleshooting","text":""},{"location":"data_collection/#api-rate-limits","title":"API Rate Limits","text":"<p>If you hit rate limits: - Add delays between requests (time.sleep(2)) - Batch requests appropriately - Monitor your quota usage</p>"},{"location":"data_collection/#poor-image-quality","title":"Poor Image Quality","text":"<p>If images don't meet quality standards: - Refine prompts with more detail - Adjust generation parameters - Try different style specifications - Review example prompts in the notebook</p>"},{"location":"data_collection/#content-issues","title":"Content Issues","text":"<p>If generated content is inappropriate: - Add content filters to prompts - Specify desired tone and atmosphere - Review and reject problematic outputs - Document issues for future prompt refinement</p> <p>Questions? Check the CyVerse Learning Center or ask during the workshop.</p>"},{"location":"model_deployment/","title":"Model Deployment and Network Analysis","text":"<p>In this final module, you'll deploy your models and perform network analysis to understand semantic relationships and thematic patterns in your data. This stage transforms individual predictions into knowledge graphs.</p>"},{"location":"model_deployment/#overview","title":"Overview","text":"<p>This module covers:</p> <ul> <li>Building semantic similarity networks</li> <li>Graph construction and visualization</li> <li>Community detection algorithms</li> <li>Centrality measures</li> <li>Narrative structure analysis</li> </ul>"},{"location":"model_deployment/#what-is-network-analysis","title":"What is Network Analysis?","text":"<p>Network analysis examines relationships and connections between entities:</p> <ul> <li>Nodes: Images, labels, captions, or concepts</li> <li>Edges: Semantic similarities or co-occurrences</li> <li>Communities: Clusters of related content</li> <li>Structure: Overall patterns and themes</li> </ul>"},{"location":"model_deployment/#graph-types","title":"Graph Types","text":""},{"location":"model_deployment/#bipartite-graphs","title":"Bipartite Graphs","text":"<p>Connect two different types of nodes (e.g., images and labels):</p> <pre><code>import networkx as nx\nimport matplotlib.pyplot as plt\n\n# Create bipartite graph\nG = nx.Graph()\n\n# Add nodes with bipartite attribute\nimages = ['img1', 'img2', 'img3']\nlabels = ['protest', 'gathering', 'march']\n\nG.add_nodes_from(images, bipartite=0)  # Images\nG.add_nodes_from(labels, bipartite=1)  # Labels\n\n# Add edges (image-label associations)\nedges = [\n    ('img1', 'protest'),\n    ('img1', 'gathering'),\n    ('img2', 'gathering'),\n    ('img3', 'march')\n]\nG.add_edges_from(edges)\n\n# Visualize\npos = nx.bipartite_layout(G, images)\nnx.draw(G, pos, with_labels=True, node_color=['lightblue']*3 + ['lightgreen']*3)\nplt.show()\n</code></pre>"},{"location":"model_deployment/#multipartite-graphs","title":"Multipartite Graphs","text":"<p>Connect multiple types of nodes (images, labels, captions):</p> <pre><code># Create multipartite graph\nG = nx.Graph()\n\n# Add different node types\nimages = ['img1', 'img2']\nlabels = ['protest', 'crowd']\ncaptions = ['caption1', 'caption2']\n\nG.add_nodes_from(images, node_type='image')\nG.add_nodes_from(labels, node_type='label')\nG.add_nodes_from(captions, node_type='caption')\n\n# Add edges between different types\nG.add_edges_from([\n    ('img1', 'protest'),\n    ('img1', 'caption1'),\n    ('protest', 'caption1')\n])\n</code></pre>"},{"location":"model_deployment/#semantic-similarity-networks","title":"Semantic Similarity Networks","text":"<p>Connect nodes based on semantic similarity:</p> <pre><code>from sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\n\n# Load model\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\n# Generate embeddings\ncaptions = [\n    \"A peaceful protest in the city square\",\n    \"Demonstrators march through downtown\",\n    \"Quiet evening in the plaza\"\n]\nembeddings = model.encode(captions)\n\n# Calculate similarity matrix\nsimilarity_matrix = cosine_similarity(embeddings)\n\n# Build graph with similarity threshold\nG = nx.Graph()\nG.add_nodes_from(range(len(captions)))\n\nthreshold = 0.5\nfor i in range(len(captions)):\n    for j in range(i+1, len(captions)):\n        if similarity_matrix[i][j] &gt; threshold:\n            G.add_edge(i, j, weight=similarity_matrix[i][j])\n\n# Visualize\npos = nx.spring_layout(G)\nedge_weights = [G[u][v]['weight'] for u, v in G.edges()]\nnx.draw(G, pos, with_labels=True, edge_color=edge_weights,\n        edge_cmap=plt.cm.Blues, width=3, node_color='lightblue', node_size=500)\nplt.show()\n</code></pre>"},{"location":"model_deployment/#building-networks","title":"Building Networks","text":""},{"location":"model_deployment/#step-1-prepare-data","title":"Step 1: Prepare Data","text":"<p>Organize your generated labels and captions:</p> <pre><code>import pandas as pd\n\n# Load predictions\ndata = pd.DataFrame({\n    'image_id': ['img1', 'img2', 'img3'],\n    'labels': [['protest', 'crowd'], ['gathering'], ['march', 'crowd']],\n    'caption': [\n        'A large crowd protesting',\n        'People gathering peacefully',\n        'March through the streets'\n    ]\n})\n</code></pre>"},{"location":"model_deployment/#step-2-calculate-semantic-weights","title":"Step 2: Calculate Semantic Weights","text":"<p>Compute weights for different data types:</p>"},{"location":"model_deployment/#visual-weights","title":"Visual Weights","text":"<p>Based on image similarity (using embeddings):</p> <pre><code>import torch\nfrom torchvision.models import resnet50, ResNet50_Weights\nfrom torchvision import transforms\nfrom PIL import Image\n\n# Load pre-trained model\nweights = ResNet50_Weights.DEFAULT\nmodel = resnet50(weights=weights)\nmodel.eval()\n\n# Remove classification layer to get embeddings\nmodel = torch.nn.Sequential(*list(model.children())[:-1])\n\n# Transform for model input\ntransform = weights.transforms()\n\ndef get_image_embedding(image_path):\n    img = Image.open(image_path)\n    img_tensor = transform(img).unsqueeze(0)\n\n    with torch.no_grad():\n        embedding = model(img_tensor)\n\n    return embedding.squeeze().numpy()\n\n# Calculate visual similarity\nimage_paths = ['img1.jpg', 'img2.jpg', 'img3.jpg']\nembeddings = [get_image_embedding(path) for path in image_paths]\n\nfrom sklearn.metrics.pairwise import cosine_similarity\nvisual_similarity = cosine_similarity(embeddings)\n</code></pre>"},{"location":"model_deployment/#label-weights","title":"Label Weights","text":"<p>Based on co-occurrence and semantic similarity:</p> <pre><code>from sklearn.preprocessing import MultiLabelBinarizer\n\n# Create label matrix\nmlb = MultiLabelBinarizer()\nlabel_matrix = mlb.fit_transform(data['labels'])\n\n# Calculate Jaccard similarity\nfrom sklearn.metrics import jaccard_score\n\ndef jaccard_similarity_matrix(label_matrix):\n    n = label_matrix.shape[0]\n    similarity = np.zeros((n, n))\n\n    for i in range(n):\n        for j in range(i, n):\n            similarity[i, j] = jaccard_score(label_matrix[i], label_matrix[j])\n            similarity[j, i] = similarity[i, j]\n\n    return similarity\n\nlabel_similarity = jaccard_similarity_matrix(label_matrix)\n</code></pre>"},{"location":"model_deployment/#caption-weights","title":"Caption Weights","text":"<p>Based on semantic embeddings:</p> <pre><code>from sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\ncaption_embeddings = model.encode(data['caption'].tolist())\ncaption_similarity = cosine_similarity(caption_embeddings)\n</code></pre>"},{"location":"model_deployment/#step-3-combine-weights","title":"Step 3: Combine Weights","text":"<p>Create composite similarity scores:</p> <pre><code># Weighted combination of different similarities\nalpha = 0.4  # Visual weight\nbeta = 0.3   # Label weight\ngamma = 0.3  # Caption weight\n\ncombined_similarity = (\n    alpha * visual_similarity +\n    beta * label_similarity +\n    gamma * caption_similarity\n)\n\n# Build final graph\nG = nx.Graph()\nn_images = len(data)\n\n# Add nodes\nfor i, row in data.iterrows():\n    G.add_node(i, image_id=row['image_id'], caption=row['caption'])\n\n# Add edges with combined weights\nthreshold = 0.6\nfor i in range(n_images):\n    for j in range(i+1, n_images):\n        if combined_similarity[i, j] &gt; threshold:\n            G.add_edge(i, j, weight=combined_similarity[i, j])\n</code></pre>"},{"location":"model_deployment/#community-detection","title":"Community Detection","text":"<p>Identify clusters of related content:</p>"},{"location":"model_deployment/#leiden-algorithm","title":"Leiden Algorithm","text":"<p>State-of-the-art community detection:</p> <pre><code>import igraph as ig\nimport leidenalg\n\n# Convert NetworkX graph to igraph\nedges = list(G.edges())\ng = ig.Graph(edges=edges)\n\n# Run Leiden algorithm\npartition = leidenalg.find_partition(g, leidenalg.ModularityVertexPartition)\n\n# Assign communities back to NetworkX graph\nfor i, community in enumerate(partition):\n    for node in community:\n        G.nodes[node]['community'] = i\n\nprint(f\"Found {len(partition)} communities\")\nprint(f\"Modularity: {partition.modularity:.4f}\")\n</code></pre>"},{"location":"model_deployment/#louvain-algorithm","title":"Louvain Algorithm","text":"<p>Alternative community detection:</p> <pre><code>from networkx.algorithms import community\n\n# Compute best partition\ncommunities = community.greedy_modularity_communities(G)\n\n# Assign to nodes\nfor i, comm in enumerate(communities):\n    for node in comm:\n        G.nodes[node]['community'] = i\n</code></pre>"},{"location":"model_deployment/#visualize-communities","title":"Visualize Communities","text":"<pre><code>import matplotlib.pyplot as plt\n\n# Get community assignments\ncommunity_map = nx.get_node_attributes(G, 'community')\ncommunities = list(set(community_map.values()))\n\n# Create color map\ncolors = plt.cm.rainbow(np.linspace(0, 1, len(communities)))\nnode_colors = [colors[community_map[node]] for node in G.nodes()]\n\n# Draw graph\npos = nx.spring_layout(G, k=0.5, iterations=50)\nplt.figure(figsize=(12, 8))\nnx.draw(G, pos, node_color=node_colors, with_labels=True,\n        node_size=500, font_size=10, font_weight='bold')\n\n# Add legend\nfor i, color in enumerate(colors):\n    plt.scatter([], [], c=[color], label=f'Community {i}')\nplt.legend()\nplt.title('Network Communities')\nplt.show()\n</code></pre>"},{"location":"model_deployment/#centrality-measures","title":"Centrality Measures","text":"<p>Identify important nodes in the network:</p>"},{"location":"model_deployment/#degree-centrality","title":"Degree Centrality","text":"<p>Number of connections:</p> <pre><code>degree_centrality = nx.degree_centrality(G)\n\n# Find most connected nodes\nsorted_nodes = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)\nprint(\"Top 5 nodes by degree centrality:\")\nfor node, centrality in sorted_nodes[:5]:\n    print(f\"Node {node}: {centrality:.4f}\")\n</code></pre>"},{"location":"model_deployment/#betweenness-centrality","title":"Betweenness Centrality","text":"<p>Nodes that bridge communities:</p> <pre><code>betweenness_centrality = nx.betweenness_centrality(G)\n\nsorted_nodes = sorted(betweenness_centrality.items(), key=lambda x: x[1], reverse=True)\nprint(\"Top 5 nodes by betweenness centrality:\")\nfor node, centrality in sorted_nodes[:5]:\n    print(f\"Node {node}: {centrality:.4f}\")\n</code></pre>"},{"location":"model_deployment/#eigenvector-centrality","title":"Eigenvector Centrality","text":"<p>Importance based on connections to important nodes:</p> <pre><code>eigenvector_centrality = nx.eigenvector_centrality(G, max_iter=1000)\n\nsorted_nodes = sorted(eigenvector_centrality.items(), key=lambda x: x[1], reverse=True)\nprint(\"Top 5 nodes by eigenvector centrality:\")\nfor node, centrality in sorted_nodes[:5]:\n    print(f\"Node {node}: {centrality:.4f}\")\n</code></pre>"},{"location":"model_deployment/#visualize-centrality","title":"Visualize Centrality","text":"<pre><code># Visualize with node size based on centrality\npos = nx.spring_layout(G)\nnode_sizes = [eigenvector_centrality[node] * 3000 for node in G.nodes()]\n\nplt.figure(figsize=(12, 8))\nnx.draw(G, pos, node_size=node_sizes, node_color='lightblue',\n        with_labels=True, font_size=10, font_weight='bold')\nplt.title('Network with Node Sizes by Eigenvector Centrality')\nplt.show()\n</code></pre>"},{"location":"model_deployment/#advanced-analysis","title":"Advanced Analysis","text":""},{"location":"model_deployment/#ergm-exponential-random-graph-models","title":"ERGM (Exponential Random Graph Models)","text":"<p>Model network formation processes:</p> <pre><code># Note: ERGM typically requires specialized software like statnet (R)\n# Python implementation example using ergm package\n\n# This is a conceptual example\n# Install: pip install ergm\n\n# Define ERGM terms\n# ergm_model = ergm(network ~ edges + nodematch('community') + triangle)\n</code></pre>"},{"location":"model_deployment/#narrative-structure-analysis","title":"Narrative Structure Analysis","text":"<p>Identify story arcs and themes:</p> <pre><code># Temporal analysis if timestamps available\ndef analyze_temporal_patterns(G, timestamps):\n    # Group by time periods\n    time_groups = {}\n\n    for node, ts in timestamps.items():\n        period = ts // 3600  # Hour bins\n        if period not in time_groups:\n            time_groups[period] = []\n        time_groups[period].append(node)\n\n    # Analyze evolution\n    for period in sorted(time_groups.keys()):\n        subgraph = G.subgraph(time_groups[period])\n        print(f\"Period {period}: {subgraph.number_of_nodes()} nodes, \"\n              f\"{subgraph.number_of_edges()} edges\")\n\n# Theme extraction\ndef extract_themes(G, captions):\n    from sklearn.feature_extraction.text import TfidfVectorizer\n\n    vectorizer = TfidfVectorizer(max_features=20, stop_words='english')\n    tfidf_matrix = vectorizer.fit_transform(captions)\n\n    # Get top terms\n    feature_names = vectorizer.get_feature_names_out()\n    print(\"Top themes:\", feature_names)\n\n    return feature_names\n</code></pre>"},{"location":"model_deployment/#interactive-visualization","title":"Interactive Visualization","text":""},{"location":"model_deployment/#using-plotly","title":"Using Plotly","text":"<pre><code>import plotly.graph_objects as go\n\n# Get positions\npos = nx.spring_layout(G)\n\n# Extract node positions\nnode_x = [pos[node][0] for node in G.nodes()]\nnode_y = [pos[node][1] for node in G.nodes()]\n\n# Extract edge positions\nedge_x = []\nedge_y = []\nfor edge in G.edges():\n    x0, y0 = pos[edge[0]]\n    x1, y1 = pos[edge[1]]\n    edge_x.extend([x0, x1, None])\n    edge_y.extend([y0, y1, None])\n\n# Create figure\nfig = go.Figure()\n\n# Add edges\nfig.add_trace(go.Scatter(x=edge_x, y=edge_y, mode='lines',\n                         line=dict(width=0.5, color='#888'),\n                         hoverinfo='none'))\n\n# Add nodes\nfig.add_trace(go.Scatter(x=node_x, y=node_y, mode='markers',\n                         marker=dict(size=10, color='lightblue'),\n                         text=list(G.nodes()),\n                         hoverinfo='text'))\n\nfig.update_layout(title='Interactive Network Graph',\n                  showlegend=False,\n                  hovermode='closest',\n                  xaxis=dict(showgrid=False, zeroline=False),\n                  yaxis=dict(showgrid=False, zeroline=False))\nfig.show()\n</code></pre>"},{"location":"model_deployment/#using-pyvis","title":"Using Pyvis","text":"<pre><code>from pyvis.network import Network\n\n# Create interactive network\nnet = Network(height='750px', width='100%', bgcolor='#222222', font_color='white')\n\n# Add nodes\nfor node in G.nodes():\n    net.add_node(node, label=str(node), title=f\"Node {node}\")\n\n# Add edges\nfor edge in G.edges(data=True):\n    net.add_edge(edge[0], edge[1], value=edge[2].get('weight', 1))\n\n# Save and display\nnet.show('network.html')\n</code></pre>"},{"location":"model_deployment/#best-practices","title":"Best Practices","text":""},{"location":"model_deployment/#graph-construction","title":"Graph Construction","text":"<ol> <li>Choose appropriate similarity thresholds</li> <li>Consider edge weight normalization</li> <li>Handle disconnected components</li> <li>Validate graph structure</li> </ol>"},{"location":"model_deployment/#community-detection_1","title":"Community Detection","text":"<ol> <li>Try multiple algorithms</li> <li>Compare results qualitatively</li> <li>Validate with domain knowledge</li> <li>Check community sizes</li> </ol>"},{"location":"model_deployment/#visualization","title":"Visualization","text":"<ol> <li>Use layouts appropriate for graph structure</li> <li>Limit node labels for large graphs</li> <li>Use interactive tools for exploration</li> <li>Export high-quality figures</li> </ol>"},{"location":"model_deployment/#outputs","title":"Outputs","text":"<p>By the end of this module, you should have:</p> <ul> <li> Semantic similarity networks</li> <li> Community structure analysis</li> <li> Centrality measures for all nodes</li> <li> Network visualizations</li> <li> Thematic analysis report</li> </ul>"},{"location":"model_deployment/#interpretation","title":"Interpretation","text":""},{"location":"model_deployment/#analyzing-results","title":"Analyzing Results","text":"<p>Questions to ask:</p> <ol> <li>Community Structure: What themes emerge from communities?</li> <li>Central Nodes: Which images/concepts are most influential?</li> <li>Bridges: Which nodes connect different communities?</li> <li>Patterns: Are there temporal or spatial patterns?</li> </ol>"},{"location":"model_deployment/#reporting-findings","title":"Reporting Findings","text":"<p>Document your analysis:</p> <pre><code># Network Analysis Report\n\n## Overview\n- Total nodes: X\n- Total edges: Y\n- Number of communities: Z\n\n## Key Findings\n1. Community 1: [Theme description]\n2. Community 2: [Theme description]\n\n## Central Concepts\n- Most connected: [Node X]\n- Key bridges: [Nodes Y, Z]\n\n## Narrative Structure\n[Description of overall patterns and story]\n</code></pre>"},{"location":"model_deployment/#additional-resources","title":"Additional Resources","text":"<ul> <li>NetworkX Documentation</li> <li>igraph Documentation</li> <li>Leiden Algorithm Paper</li> <li>Network Science Book</li> </ul>"},{"location":"model_deployment/#troubleshooting","title":"Troubleshooting","text":""},{"location":"model_deployment/#graph-too-dense","title":"Graph Too Dense","text":"<ul> <li>Increase similarity threshold</li> <li>Use sparse graph representations</li> <li>Sample edges</li> </ul>"},{"location":"model_deployment/#poor-community-detection","title":"Poor Community Detection","text":"<ul> <li>Adjust resolution parameter</li> <li>Try different algorithms</li> <li>Check data quality</li> </ul>"},{"location":"model_deployment/#visualization-issues","title":"Visualization Issues","text":"<ul> <li>Reduce number of nodes shown</li> <li>Use hierarchical layouts</li> <li>Export to specialized tools (Gephi, Cytoscape)</li> </ul> <p>Congratulations on completing the workshop! Check the CyVerse Learning Center for more resources.</p>"},{"location":"model_development/","title":"Model Development","text":"<p>In this module, you'll develop and evaluate Multimodal Large Language Models (M-LLMs) to generate labels and captions for synthetic images. This stage combines computer vision and natural language processing.</p>"},{"location":"model_development/#overview","title":"Overview","text":"<p>This module covers:</p> <ul> <li>Using M-LLMs for image understanding</li> <li>Generating labels and captions</li> <li>Text preprocessing and normalization</li> <li>Model evaluation metrics</li> <li>Performance optimization</li> </ul>"},{"location":"model_development/#what-are-multimodal-llms","title":"What are Multimodal-LLMs?","text":"<p>Multimodal Large Language Models can process and understand multiple types of data:</p> <ul> <li>Visual input: Images, videos, diagrams</li> <li>Text input: Prompts, questions, descriptions</li> <li>Combined output: Text descriptions, classifications, captions</li> </ul> <p>Examples: GPT-4 Vision, Google Gemini Pro Vision, LLaVA, CLIP</p>"},{"location":"model_development/#workflow","title":"Workflow","text":""},{"location":"model_development/#step-1-prepare-your-data","title":"Step 1: Prepare Your Data","text":"<p>Ensure you have: - Generated synthetic images from Data Collection - Organized images in structured directories - Created metadata files - Split data into training/validation/test sets</p>"},{"location":"model_development/#step-2-image-label-generation","title":"Step 2: Image Label Generation","text":""},{"location":"model_development/#understanding-labels","title":"Understanding Labels","text":"<p>Labels categorize images into predefined classes: - Scene type (protest, gathering, march) - Emotion (peaceful, tense, celebratory) - Density (crowded, sparse, intimate) - Time (day, night, dawn/dusk)</p>"},{"location":"model_development/#using-m-llms-for-labeling","title":"Using M-LLMs for Labeling","text":"<pre><code>import google.generativeai as genai\nfrom PIL import Image\nimport os\n\n# Configure Gemini\ngenai.configure(api_key=os.getenv('GOOGLE_API_KEY'))\nmodel = genai.GenerativeModel('gemini-pro-vision')\n\ndef generate_labels(image_path):\n    # Load image\n    img = Image.open(image_path)\n\n    # Create prompt\n    prompt = \"\"\"\n    Analyze this image and provide labels for:\n    1. Scene type (e.g., protest, gathering, march)\n    2. Emotional tone (e.g., peaceful, tense, celebratory)\n    3. Crowd density (e.g., sparse, moderate, dense)\n    4. Time of day (e.g., day, night, dusk)\n\n    Provide only the labels, separated by commas.\n    \"\"\"\n\n    # Generate labels\n    response = model.generate_content([prompt, img])\n    labels = response.text.strip().split(',')\n\n    return [label.strip() for label in labels]\n\n# Example usage\nimage_labels = generate_labels('path/to/image.jpg')\nprint(image_labels)\n</code></pre>"},{"location":"model_development/#step-3-caption-generation","title":"Step 3: Caption Generation","text":""},{"location":"model_development/#understanding-captions","title":"Understanding Captions","text":"<p>Captions provide descriptive text about image content: - Who: People and their actions - What: Objects and events - Where: Setting and location - When: Time and temporal context - How: Manner and atmosphere</p>"},{"location":"model_development/#generating-descriptive-captions","title":"Generating Descriptive Captions","text":"<pre><code>def generate_caption(image_path):\n    img = Image.open(image_path)\n\n    prompt = \"\"\"\n    Write a detailed caption for this image describing:\n    - The scene and setting\n    - People and their actions\n    - The overall atmosphere\n    - Any notable objects or elements\n\n    Keep the caption concise (2-3 sentences) and objective.\n    \"\"\"\n\n    response = model.generate_content([prompt, img])\n    return response.text.strip()\n\n# Example usage\ncaption = generate_caption('path/to/image.jpg')\nprint(caption)\n</code></pre>"},{"location":"model_development/#step-4-text-processing","title":"Step 4: Text Processing","text":"<p>Clean and normalize generated text for consistency.</p>"},{"location":"model_development/#lemmatization","title":"Lemmatization","text":"<p>Convert words to their base form:</p> <pre><code>from nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nimport nltk\n\n# Download required NLTK data\nnltk.download('wordnet')\nnltk.download('punkt')\n\nlemmatizer = WordNetLemmatizer()\n\ndef lemmatize_text(text):\n    tokens = word_tokenize(text.lower())\n    lemmatized = [lemmatizer.lemmatize(token) for token in tokens]\n    return ' '.join(lemmatized)\n\n# Example\ntext = \"People are gathering and protesting\"\nlemmatized = lemmatize_text(text)  # \"people be gather and protest\"\n</code></pre>"},{"location":"model_development/#emoji-translation","title":"Emoji Translation","text":"<p>Convert emojis to text descriptions:</p> <pre><code>import emoji\n\ndef translate_emojis(text):\n    return emoji.demojize(text, delimiters=(\" \", \" \"))\n\n# Example\ntext_with_emoji = \"Great protest today! \ud83c\udf89\"\ntranslated = translate_emojis(text_with_emoji)  # \"Great protest today!  party_popper \"\n</code></pre>"},{"location":"model_development/#hashtag-normalization","title":"Hashtag Normalization","text":"<p>Process hashtags for analysis:</p> <pre><code>import re\n\ndef normalize_hashtags(text):\n    # Split camelCase hashtags\n    def split_camel(hashtag):\n        return re.sub('([a-z])([A-Z])', r'\\1 \\2', hashtag)\n\n    # Find and process hashtags\n    hashtags = re.findall(r'#\\w+', text)\n    for hashtag in hashtags:\n        normalized = split_camel(hashtag[1:]).lower()  # Remove # and split\n        text = text.replace(hashtag, normalized)\n\n    return text\n\n# Example\ntext = \"Join the #ClimateStrike #SaveOurPlanet\"\nnormalized = normalize_hashtags(text)  # \"Join the climate strike save our planet\"\n</code></pre>"},{"location":"model_development/#complete-text-pipeline","title":"Complete Text Pipeline","text":"<pre><code>def preprocess_text(text):\n    # Translate emojis\n    text = translate_emojis(text)\n\n    # Normalize hashtags\n    text = normalize_hashtags(text)\n\n    # Lemmatize\n    text = lemmatize_text(text)\n\n    # Remove extra whitespace\n    text = ' '.join(text.split())\n\n    return text\n</code></pre>"},{"location":"model_development/#step-5-multilingual-support","title":"Step 5: Multilingual Support","text":"<p>Use sentence transformers for multilingual text understanding:</p>"},{"location":"model_development/#hugging-face-sentence-transformers","title":"Hugging Face Sentence Transformers","text":"<pre><code>from sentence_transformers import SentenceTransformer\n\n# Load multilingual model\nmodel = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n\n# Encode sentences\nsentences = [\n    \"A peaceful protest in the city square\",\n    \"Una protesta pac\u00edfica en la plaza de la ciudad\",\n    \"Une manifestation pacifique sur la place de la ville\"\n]\n\nembeddings = model.encode(sentences)\n\n# Calculate similarity\nfrom sklearn.metrics.pairwise import cosine_similarity\nsimilarity_matrix = cosine_similarity(embeddings)\nprint(similarity_matrix)\n</code></pre>"},{"location":"model_development/#labse-language-agnostic-bert-sentence-encoder","title":"LaBSE (Language-agnostic BERT Sentence Encoder)","text":"<pre><code>import tensorflow_hub as hub\nimport numpy as np\n\n# Load LaBSE model\nlabse_model = hub.load('https://tfhub.dev/google/LaBSE/2')\n\n# Encode multilingual sentences\nsentences = [\n    \"People gathering for a cause\",\n    \"Gente reunida por una causa\"\n]\n\nembeddings = labse_model(sentences)\n\n# Compute similarity\nsimilarity = np.inner(embeddings[0], embeddings[1])\nprint(f\"Similarity: {similarity:.4f}\")\n</code></pre>"},{"location":"model_development/#model-evaluation","title":"Model Evaluation","text":""},{"location":"model_development/#classification-metrics","title":"Classification Metrics","text":"<p>For label prediction tasks:</p> <pre><code>from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, hamming_loss\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef evaluate_classification(y_true, y_pred):\n    # Calculate metrics\n    metrics = {\n        'accuracy': accuracy_score(y_true, y_pred),\n        'precision': precision_score(y_true, y_pred, average='weighted'),\n        'recall': recall_score(y_true, y_pred, average='weighted'),\n        'f1_score': f1_score(y_true, y_pred, average='weighted'),\n        'hamming_loss': hamming_loss(y_true, y_pred)\n    }\n\n    # Confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n\n    # Visualize\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n    plt.title('Confusion Matrix')\n    plt.ylabel('True Label')\n    plt.xlabel('Predicted Label')\n    plt.show()\n\n    return metrics\n\n# Example usage\n# y_true = true_labels\n# y_pred = predicted_labels\n# metrics = evaluate_classification(y_true, y_pred)\n</code></pre>"},{"location":"model_development/#semantic-similarity-metrics","title":"Semantic Similarity Metrics","text":"<p>For caption generation tasks:</p>"},{"location":"model_development/#bertscore","title":"BERTScore","text":"<pre><code>from bert_score import score\n\ndef calculate_bertscore(candidates, references):\n    P, R, F1 = score(candidates, references, lang='en', verbose=True)\n\n    return {\n        'precision': P.mean().item(),\n        'recall': R.mean().item(),\n        'f1': F1.mean().item()\n    }\n\n# Example\ncandidates = [\"A large crowd protesting in the streets\"]\nreferences = [\"Many people demonstrating on the street\"]\nbert_metrics = calculate_bertscore(candidates, references)\n</code></pre>"},{"location":"model_development/#sentence-transformer-cosine-similarity","title":"Sentence Transformer Cosine Similarity","text":"<pre><code>from sentence_transformers import SentenceTransformer, util\n\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\ndef calculate_semantic_similarity(text1, text2):\n    embeddings = model.encode([text1, text2])\n    similarity = util.cos_sim(embeddings[0], embeddings[1])\n    return similarity.item()\n\n# Example\ngenerated = \"A peaceful protest in the city\"\nreference = \"A calm demonstration in town\"\nsimilarity = calculate_semantic_similarity(generated, reference)\nprint(f\"Similarity: {similarity:.4f}\")\n</code></pre>"},{"location":"model_development/#model-training-and-fine-tuning","title":"Model Training and Fine-Tuning","text":""},{"location":"model_development/#data-preparation","title":"Data Preparation","text":"<pre><code>import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\n\nclass ImageCaptionDataset(Dataset):\n    def __init__(self, image_paths, captions, transform=None):\n        self.image_paths = image_paths\n        self.captions = captions\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        image = Image.open(self.image_paths[idx])\n        if self.transform:\n            image = self.transform(image)\n\n        caption = self.captions[idx]\n        return image, caption\n\n# Define transforms\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225])\n])\n\n# Create dataset and dataloader\ndataset = ImageCaptionDataset(image_paths, captions, transform=transform)\ndataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n</code></pre>"},{"location":"model_development/#tracking-performance","title":"Tracking Performance","text":"<pre><code>import wandb  # Weights &amp; Biases for experiment tracking\n\n# Initialize tracking\nwandb.init(project=\"ml-pipeline-workshop\")\n\n# Log metrics during training\nfor epoch in range(num_epochs):\n    train_loss = train_epoch(model, dataloader)\n    val_metrics = evaluate(model, val_dataloader)\n\n    wandb.log({\n        'epoch': epoch,\n        'train_loss': train_loss,\n        'val_accuracy': val_metrics['accuracy'],\n        'val_f1': val_metrics['f1_score']\n    })\n</code></pre>"},{"location":"model_development/#required-resources","title":"Required Resources","text":"<ul> <li>GPU Access: Jetstream2 allocation through CyVerse</li> <li>LLM Access: Google Gemini API or VERDE platform</li> <li>Hugging Face Account: For model downloads</li> <li>Storage: Sufficient space for models and data</li> </ul>"},{"location":"model_development/#best-practices","title":"Best Practices","text":""},{"location":"model_development/#model-selection","title":"Model Selection","text":"<ol> <li>Start with pre-trained models</li> <li>Fine-tune on your specific domain</li> <li>Compare multiple models</li> <li>Consider computational constraints</li> </ol>"},{"location":"model_development/#prompt-engineering","title":"Prompt Engineering","text":"<ol> <li>Clear and specific instructions</li> <li>Include examples (few-shot prompting)</li> <li>Iterate based on outputs</li> <li>Document successful prompts</li> </ol>"},{"location":"model_development/#evaluation-strategy","title":"Evaluation Strategy","text":"<ol> <li>Use multiple metrics</li> <li>Include human evaluation</li> <li>Test on diverse examples</li> <li>Monitor for biases</li> </ol>"},{"location":"model_development/#outputs","title":"Outputs","text":"<p>By the end of this module, you should have:</p> <ul> <li> Generated labels for all images</li> <li> Generated captions for all images</li> <li> Preprocessed and normalized text data</li> <li> Evaluation metrics and visualizations</li> <li> Model performance documentation</li> </ul>"},{"location":"model_development/#next-steps","title":"Next Steps","text":"<p>Once model development is complete:</p> <ol> <li>Analyze model outputs for patterns</li> <li>Proceed to Model Deployment</li> <li>Build semantic networks from generated data</li> </ol>"},{"location":"model_development/#additional-resources","title":"Additional Resources","text":"<ul> <li>Hugging Face Transformers Documentation</li> <li>Sentence Transformers Documentation</li> <li>BERTScore Paper</li> <li>PyTorch Tutorials</li> </ul>"},{"location":"model_development/#troubleshooting","title":"Troubleshooting","text":""},{"location":"model_development/#memory-issues","title":"Memory Issues","text":"<ul> <li>Reduce batch size</li> <li>Use gradient accumulation</li> <li>Enable mixed precision training</li> <li>Clear GPU cache regularly</li> </ul>"},{"location":"model_development/#poor-model-performance","title":"Poor Model Performance","text":"<ul> <li>Increase training data</li> <li>Adjust learning rate</li> <li>Try different model architectures</li> <li>Review data quality</li> </ul>"},{"location":"model_development/#api-rate-limits","title":"API Rate Limits","text":"<ul> <li>Implement exponential backoff</li> <li>Cache responses</li> <li>Batch requests efficiently</li> <li>Monitor quota usage</li> </ul> <p>Questions? Check the CyVerse Learning Center or ask during the workshop.</p>"},{"location":"setup/","title":"Workshop Setup Guide","text":"<p>This guide will help you set up your environment for the AI/ML Pipeline Workshop on CyVerse Discovery Environment.</p>"},{"location":"setup/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, make sure you have:</p> <ul> <li> Created a CyVerse account (instructions here)</li> <li> Registered for the workshop at https://user.cyverse.org/workshops/208</li> <li> A Google Gemini API key (we'll cover how to get this)</li> </ul>"},{"location":"setup/#step-1-access-cyverse-discovery-environment","title":"Step 1: Access CyVerse Discovery Environment","text":"<ol> <li>Navigate to https://de.cyverse.org</li> <li>Log in with your CyVerse credentials</li> <li>You should see the Discovery Environment dashboard</li> </ol>"},{"location":"setup/#step-2-launch-jupyter-lab-pytorch-gpu","title":"Step 2: Launch Jupyter Lab PyTorch GPU","text":"<p>This workshop requires GPU acceleration for model training and inference.</p> <ol> <li>In the Discovery Environment, click \"Apps\" in the left sidebar</li> <li>Look for the \"Instant Launches\" section</li> <li>Click on \"Jupyter Lab PyTorch GPU\"</li> <li>Wait for the environment to initialize (this may take 2-5 minutes)</li> </ol> <p>The Jupyter Lab interface will open in a new browser tab once ready.</p>"},{"location":"setup/#step-3-clone-the-workshop-repository","title":"Step 3: Clone the Workshop Repository","text":"<p>In Jupyter Lab:</p>"},{"location":"setup/#option-a-using-terminal","title":"Option A: Using Terminal","text":"<ol> <li>Click the Terminal icon in the Launcher (or File &gt; New &gt; Terminal)</li> <li>Navigate to your desired directory:    <pre><code>cd /home/jovyan/data-store\n</code></pre></li> <li>Clone the repository:    <pre><code>git clone https://github.com/lwdozal/AI-ML_PipelineWorkshop.git\n</code></pre></li> <li>Navigate into the repository:    <pre><code>cd AI-ML_PipelineWorkshop\n</code></pre></li> </ol>"},{"location":"setup/#option-b-using-git-extension","title":"Option B: Using Git Extension","text":"<ol> <li>Click the Git icon in the left sidebar</li> <li>Click \"Clone a Repository\"</li> <li>Enter the repository URL: <code>https://github.com/lwdozal/AI-ML_PipelineWorkshop.git</code></li> <li>Choose the destination folder: <code>/home/jovyan/data-store</code></li> <li>Click \"Clone\"</li> </ol>"},{"location":"setup/#step-4-create-a-virtual-environment","title":"Step 4: Create a Virtual Environment","text":"<p>Creating a virtual environment isolates your project dependencies:</p> <pre><code># Navigate to the repository directory\ncd /home/jovyan/data-store/AI-ML_PipelineWorkshop\n\n# Create a virtual environment named 'workshop_env'\npython -m venv workshop_env\n\n# Activate the virtual environment\nsource workshop_env/bin/activate\n</code></pre> <p>You should see <code>(workshop_env)</code> in your terminal prompt, indicating the environment is active.</p>"},{"location":"setup/#step-5-install-required-packages","title":"Step 5: Install Required Packages","text":"<p>Install all necessary Python packages using pip:</p> <pre><code>pip install --upgrade pip\npip install -r requirements.txt\n</code></pre>"},{"location":"setup/#key-packages-installed","title":"Key Packages Installed","text":"<p>The <code>requirements.txt</code> file includes:</p> <ul> <li>torch &amp; torchvision: PyTorch for deep learning</li> <li>transformers: Hugging Face transformers library</li> <li>sentence-transformers: For semantic similarity</li> <li>google-generativeai: Google Gemini API client</li> <li>PIL (Pillow): Image processing</li> <li>opencv-python: Computer vision tasks</li> <li>langchain: LLM orchestration</li> <li>pandas &amp; numpy: Data manipulation</li> <li>matplotlib &amp; seaborn: Data visualization</li> <li>networkx: Network analysis</li> <li>jupyter: Notebook interface</li> </ul>"},{"location":"setup/#step-6-set-up-google-gemini-api-key","title":"Step 6: Set Up Google Gemini API Key","text":"<p>You'll need a Google Gemini API key to generate synthetic images.</p>"},{"location":"setup/#get-your-api-key","title":"Get Your API Key","text":"<ol> <li>Go to https://makersuite.google.com/app/apikey</li> <li>Sign in with your Google account</li> <li>Click \"Create API Key\"</li> <li>Copy your API key</li> </ol>"},{"location":"setup/#configure-your-api-key","title":"Configure Your API Key","text":"<p>Create a <code>.env</code> file in the repository root:</p> <pre><code># In the terminal, navigate to repository root\ncd /home/jovyan/data-store/AI-ML_PipelineWorkshop\n\n# Create .env file\nnano .env\n</code></pre> <p>Add your API key:</p> <pre><code>GOOGLE_API_KEY=your_api_key_here\n</code></pre> <p>Save and exit (Ctrl+X, then Y, then Enter).</p> <p>Security Note: Never commit your <code>.env</code> file to Git. The repository includes a <code>.gitignore</code> file to prevent this.</p>"},{"location":"setup/#verify-installation","title":"Verify Installation","text":"<p>Test your setup by running this Python code in a new notebook:</p> <pre><code>import os\nfrom dotenv import load_dotenv\nimport google.generativeai as genai\n\n# Load environment variables\nload_dotenv()\n\n# Configure Gemini\ngenai.configure(api_key=os.getenv('GOOGLE_API_KEY'))\n\n# Test connection\nmodel = genai.GenerativeModel('gemini-pro')\nresponse = model.generate_content(\"Hello, this is a test.\")\nprint(response.text)\n</code></pre> <p>If this runs without errors, your setup is complete.</p>"},{"location":"setup/#step-7-verify-gpu-access","title":"Step 7: Verify GPU Access","text":"<p>Verify that PyTorch can access the GPU:</p> <pre><code>import torch\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"CUDA version: {torch.version.cuda}\")\nif torch.cuda.is_available():\n    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n</code></pre> <p>Expected output should show <code>CUDA available: True</code> and list the GPU device.</p>"},{"location":"setup/#directory-structure","title":"Directory Structure","text":"<p>After setup, your directory should look like this:</p> <pre><code>AI-ML_PipelineWorkshop/\n\u251c\u2500\u2500 DataCollection/          # Synthetic data generation notebooks\n\u251c\u2500\u2500 docs/                    # Documentation (this site)\n\u251c\u2500\u2500 workshop_env/            # Virtual environment (not tracked by git)\n\u251c\u2500\u2500 .env                     # API keys (not tracked by git)\n\u251c\u2500\u2500 .gitignore              # Git ignore rules\n\u251c\u2500\u2500 requirements.txt         # Python dependencies\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 AUTHORS.md\n\u2514\u2500\u2500 LICENSE\n</code></pre>"},{"location":"setup/#troubleshooting","title":"Troubleshooting","text":""},{"location":"setup/#virtual-environment-issues","title":"Virtual Environment Issues","text":"<p>If you can't activate the virtual environment:</p> <pre><code># Make sure you're in the right directory\ncd /home/jovyan/data-store/AI-ML_PipelineWorkshop\n\n# Try creating it again\npython3 -m venv workshop_env\nsource workshop_env/bin/activate\n</code></pre>"},{"location":"setup/#package-installation-errors","title":"Package Installation Errors","text":"<p>If pip install fails:</p> <pre><code># Upgrade pip first\npip install --upgrade pip\n\n# Try installing packages individually\npip install torch torchvision\npip install transformers sentence-transformers\npip install google-generativeai\n</code></pre>"},{"location":"setup/#gpu-not-available","title":"GPU Not Available","text":"<p>If CUDA is not available:</p> <ol> <li>Verify you launched the PyTorch GPU version of Jupyter Lab</li> <li>Restart the kernel: Kernel &gt; Restart Kernel</li> <li>Check CyVerse status page for GPU availability</li> <li>Contact workshop support if issues persist</li> </ol>"},{"location":"setup/#api-key-issues","title":"API Key Issues","text":"<p>If Gemini API doesn't work:</p> <ol> <li>Verify your API key is correct in the <code>.env</code> file</li> <li>Check you've enabled the Gemini API in Google Cloud Console</li> <li>Ensure you have API quota remaining</li> <li>Test with a simple curl request:    <pre><code>curl -H 'Content-Type: application/json' \\\n     -d '{\"contents\":[{\"parts\":[{\"text\":\"Hello\"}]}]}' \\\n     \"https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent?key=YOUR_API_KEY\"\n</code></pre></li> </ol>"},{"location":"setup/#additional-resources","title":"Additional Resources","text":"<ul> <li>CyVerse Learning Center</li> <li>PyTorch Documentation</li> <li>Hugging Face Documentation</li> <li>Google Gemini API Documentation</li> </ul>"},{"location":"setup/#getting-help","title":"Getting Help","text":"<ul> <li>During the workshop: Ask instructors or teaching assistants</li> <li>CyVerse support: support@cyverse.org</li> <li>Repository issues: GitHub Issues</li> </ul> <p>Once your setup is complete, you're ready to start with Data Collection.</p>"},{"location":"notebooks/01_setup_and_test/","title":"1. Setup and Test","text":"In\u00a0[\u00a0]: Copied! <pre>import sys\nfrom pathlib import Path\n\n# Check Python version\nprint(f\"Python version: {sys.version}\")\nprint(f\"Python executable: {sys.executable}\")\n\n# List of required packages\nrequired_packages = [\n    'google.generativeai',\n    'pandas',\n    'numpy',\n    'PIL',\n    'cv2',\n    'dotenv',\n    'yaml',\n    'tqdm',\n    'matplotlib',\n    'seaborn',\n    'requests',\n    'bs4'\n]\n\nprint(\"\\nChecking required packages...\")\nmissing_packages = []\n\nfor package in required_packages:\n    try:\n        __import__(package)\n        print(f\"  \u2713 {package}\")\n    except ImportError:\n        print(f\"  \u2717 {package} - MISSING\")\n        missing_packages.append(package)\n\nif missing_packages:\n    print(f\"\\n\u26a0 Warning: {len(missing_packages)} packages missing!\")\n    print(\"Please run: pip install -r requirements.txt\")\nelse:\n    print(\"\\n\u2713 All required packages are installed!\")\n</pre> import sys from pathlib import Path  # Check Python version print(f\"Python version: {sys.version}\") print(f\"Python executable: {sys.executable}\")  # List of required packages required_packages = [     'google.generativeai',     'pandas',     'numpy',     'PIL',     'cv2',     'dotenv',     'yaml',     'tqdm',     'matplotlib',     'seaborn',     'requests',     'bs4' ]  print(\"\\nChecking required packages...\") missing_packages = []  for package in required_packages:     try:         __import__(package)         print(f\"  \u2713 {package}\")     except ImportError:         print(f\"  \u2717 {package} - MISSING\")         missing_packages.append(package)  if missing_packages:     print(f\"\\n\u26a0 Warning: {len(missing_packages)} packages missing!\")     print(\"Please run: pip install -r requirements.txt\") else:     print(\"\\n\u2713 All required packages are installed!\") In\u00a0[\u00a0]: Copied! <pre># Add parent directory to path\nparent_dir = Path.cwd().parent\nif str(parent_dir) not in sys.path:\n    sys.path.insert(0, str(parent_dir))\n\ntry:\n    from src import config, gemini_client, data_loader, prompt_builder, output_handler, validation\n    print(\"\u2713 All custom modules imported successfully!\")\nexcept ImportError as e:\n    print(f\"\u2717 Failed to import custom modules: {e}\")\n    print(\"\\nTroubleshooting:\")\n    print(\"1. Make sure you're running from the notebooks/ directory\")\n    print(\"2. Verify src/ directory exists with all .py files\")\n    print(\"3. Check for syntax errors in src modules\")\n</pre> # Add parent directory to path parent_dir = Path.cwd().parent if str(parent_dir) not in sys.path:     sys.path.insert(0, str(parent_dir))  try:     from src import config, gemini_client, data_loader, prompt_builder, output_handler, validation     print(\"\u2713 All custom modules imported successfully!\") except ImportError as e:     print(f\"\u2717 Failed to import custom modules: {e}\")     print(\"\\nTroubleshooting:\")     print(\"1. Make sure you're running from the notebooks/ directory\")     print(\"2. Verify src/ directory exists with all .py files\")     print(\"3. Check for syntax errors in src modules\") In\u00a0[\u00a0]: Copied! <pre>try:\n    # Load configuration\n    cfg = config.load_config()\n    print(\"\u2713 Configuration loaded successfully!\")\n    print(f\"\\nConfiguration: {cfg}\")\n    \n    # Display key settings\n    print(\"\\nGeneration Settings:\")\n    print(f\"  Number of images: {cfg.generation['num_images']}\")\n    print(f\"  Batch size: {cfg.generation['batch_size']}\")\n    print(f\"  Resolution: {cfg.generation['resolution']}\")\n    print(f\"  Model: {cfg.generation['model']}\")\n    \n    print(\"\\nRate Limiting:\")\n    print(f\"  Requests/minute: {cfg.rate_limiting['requests_per_minute']}\")\n    print(f\"  Requests/day: {cfg.rate_limiting['requests_per_day']}\")\n    \nexcept FileNotFoundError as e:\n    print(f\"\u2717 Configuration file not found: {e}\")\n    print(\"\\nTroubleshooting:\")\n    print(\"1. Verify config/generation_config.yaml exists\")\n    print(\"2. Check file permissions\")\nexcept Exception as e:\n    print(f\"\u2717 Configuration error: {e}\")\n</pre> try:     # Load configuration     cfg = config.load_config()     print(\"\u2713 Configuration loaded successfully!\")     print(f\"\\nConfiguration: {cfg}\")          # Display key settings     print(\"\\nGeneration Settings:\")     print(f\"  Number of images: {cfg.generation['num_images']}\")     print(f\"  Batch size: {cfg.generation['batch_size']}\")     print(f\"  Resolution: {cfg.generation['resolution']}\")     print(f\"  Model: {cfg.generation['model']}\")          print(\"\\nRate Limiting:\")     print(f\"  Requests/minute: {cfg.rate_limiting['requests_per_minute']}\")     print(f\"  Requests/day: {cfg.rate_limiting['requests_per_day']}\")      except FileNotFoundError as e:     print(f\"\u2717 Configuration file not found: {e}\")     print(\"\\nTroubleshooting:\")     print(\"1. Verify config/generation_config.yaml exists\")     print(\"2. Check file permissions\") except Exception as e:     print(f\"\u2717 Configuration error: {e}\") In\u00a0[\u00a0]: Copied! <pre>try:\n    api_key = cfg.api_key\n    print(\"\u2713 API key found!\")\n    print(f\"  Key preview: {api_key[:10]}...{api_key[-4:]}\")\n    \nexcept ValueError as e:\n    print(f\"\u2717 API key not found: {e}\")\n    print(\"\\nSetup Instructions:\")\n    print(\"1. Get your API key from: https://makersuite.google.com/app/apikey\")\n    print(\"2. Copy config/.env.example to config/.env\")\n    print(\"3. Add your API key to config/.env: GOOGLE_API_KEY=your_key_here\")\n    print(\"4. Restart this notebook\")\n    \n    # Stop execution if no API key\n    raise\n</pre> try:     api_key = cfg.api_key     print(\"\u2713 API key found!\")     print(f\"  Key preview: {api_key[:10]}...{api_key[-4:]}\")      except ValueError as e:     print(f\"\u2717 API key not found: {e}\")     print(\"\\nSetup Instructions:\")     print(\"1. Get your API key from: https://makersuite.google.com/app/apikey\")     print(\"2. Copy config/.env.example to config/.env\")     print(\"3. Add your API key to config/.env: GOOGLE_API_KEY=your_key_here\")     print(\"4. Restart this notebook\")          # Stop execution if no API key     raise In\u00a0[\u00a0]: Copied! <pre>import google.generativeai as genai\n\ntry:\n    # Configure API\n    genai.configure(api_key=cfg.api_key)\n    \n    # List available models\n    print(\"Testing API connection...\")\n    models = [m.name for m in genai.list_models()]\n    \n    print(\"\\n\u2713 Successfully connected to Gemini API!\")\n    print(f\"  Available models: {len(models)}\")\n    \n    # Check if our model is available\n    target_model = cfg.generation['model']\n    if any(target_model in m for m in models):\n        print(f\"  \u2713 Target model '{target_model}' is available\")\n    else:\n        print(f\"  \u26a0 Warning: Target model '{target_model}' not found in available models\")\n        print(f\"  Available image models: {[m for m in models if 'image' in m.lower()]}\")\n        \nexcept Exception as e:\n    print(f\"\u2717 API connection failed: {e}\")\n    print(\"\\nTroubleshooting:\")\n    print(\"1. Verify your API key is correct\")\n    print(\"2. Check your internet connection\")\n    print(\"3. Ensure you have API access enabled\")\n    raise\n</pre> import google.generativeai as genai  try:     # Configure API     genai.configure(api_key=cfg.api_key)          # List available models     print(\"Testing API connection...\")     models = [m.name for m in genai.list_models()]          print(\"\\n\u2713 Successfully connected to Gemini API!\")     print(f\"  Available models: {len(models)}\")          # Check if our model is available     target_model = cfg.generation['model']     if any(target_model in m for m in models):         print(f\"  \u2713 Target model '{target_model}' is available\")     else:         print(f\"  \u26a0 Warning: Target model '{target_model}' not found in available models\")         print(f\"  Available image models: {[m for m in models if 'image' in m.lower()]}\")          except Exception as e:     print(f\"\u2717 API connection failed: {e}\")     print(\"\\nTroubleshooting:\")     print(\"1. Verify your API key is correct\")     print(\"2. Check your internet connection\")     print(\"3. Ensure you have API access enabled\")     raise In\u00a0[\u00a0]: Copied! <pre>from IPython.display import display\nimport time\n\nprint(\"Generating test image...\")\nprint(\"(This may take 10-30 seconds)\\n\")\n\ntry:\n    # Initialize rate limiter\n    rate_limiter = gemini_client.RateLimiter(\n        requests_per_minute=cfg.rate_limiting['requests_per_minute'],\n        requests_per_day=cfg.rate_limiting['requests_per_day']\n    )\n    \n    # Initialize image generator\n    generator = gemini_client.GeminiImageGenerator(\n        api_key=cfg.api_key,\n        rate_limiter=rate_limiter,\n        model=cfg.generation['model'],\n        resolution=cfg.generation['resolution']\n    )\n    \n    # Simple test prompt\n    test_prompt = (\n        \"Photorealistic image of a peaceful civic gathering in an urban setting. \"\n        \"Diverse crowd of people holding signs, organized demonstration, \"\n        \"clear daytime lighting, high quality.\"\n    )\n    \n    print(f\"Test prompt: {test_prompt}\\n\")\n    \n    # Generate image\n    start_time = time.time()\n    result = generator.generate_image(test_prompt)\n    elapsed = time.time() - start_time\n    \n    print(f\"\\n\u2713 Test image generated successfully in {elapsed:.1f}s!\")\n    print(f\"  Image size: {result['metadata']['image_size']}\")\n    print(f\"  Image mode: {result['metadata']['image_mode']}\")\n    \n    # Display image\n    print(\"\\nGenerated Image:\")\n    display(result['image'])\n    \nexcept Exception as e:\n    print(f\"\\n\u2717 Image generation failed: {e}\")\n    print(\"\\nTroubleshooting:\")\n    print(\"1. Check API quota limits\")\n    print(\"2. Verify model name is correct\")\n    print(\"3. Try a simpler prompt\")\n    raise\n</pre> from IPython.display import display import time  print(\"Generating test image...\") print(\"(This may take 10-30 seconds)\\n\")  try:     # Initialize rate limiter     rate_limiter = gemini_client.RateLimiter(         requests_per_minute=cfg.rate_limiting['requests_per_minute'],         requests_per_day=cfg.rate_limiting['requests_per_day']     )          # Initialize image generator     generator = gemini_client.GeminiImageGenerator(         api_key=cfg.api_key,         rate_limiter=rate_limiter,         model=cfg.generation['model'],         resolution=cfg.generation['resolution']     )          # Simple test prompt     test_prompt = (         \"Photorealistic image of a peaceful civic gathering in an urban setting. \"         \"Diverse crowd of people holding signs, organized demonstration, \"         \"clear daytime lighting, high quality.\"     )          print(f\"Test prompt: {test_prompt}\\n\")          # Generate image     start_time = time.time()     result = generator.generate_image(test_prompt)     elapsed = time.time() - start_time          print(f\"\\n\u2713 Test image generated successfully in {elapsed:.1f}s!\")     print(f\"  Image size: {result['metadata']['image_size']}\")     print(f\"  Image mode: {result['metadata']['image_mode']}\")          # Display image     print(\"\\nGenerated Image:\")     display(result['image'])      except Exception as e:     print(f\"\\n\u2717 Image generation failed: {e}\")     print(\"\\nTroubleshooting:\")     print(\"1. Check API quota limits\")     print(\"2. Verify model name is correct\")     print(\"3. Try a simpler prompt\")     raise In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\n\nprint(\"Cost Estimation for Different Batch Sizes\")\nprint(\"=\" * 80)\n\n# Test different image counts\ntest_counts = [10, 20, 50, 100, 200]\nestimates = []\n\nfor count in test_counts:\n    # Temporarily set count\n    cfg.set('generation.num_images', count)\n    cost_est = cfg.estimate_cost()\n    estimates.append(cost_est)\n\n# Create comparison table\ndf = pd.DataFrame(estimates)\ndf = df[['num_images', 'resolution', 'image_generation', 'captions', 'labels', 'comments', 'total_estimated']]\ndf.columns = ['Images', 'Resolution', 'Image Gen', 'Captions', 'Labels', 'Comments', 'Total (USD)']\n\nprint(df.to_string(index=False))\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"\\nNotes:\")\nprint(\"- Costs are estimates based on current Gemini API pricing\")\nprint(\"- Actual costs may vary based on prompt complexity and API changes\")\nprint(\"- Free tier has usage limits - start with smaller batches\")\nprint(\"\\nRecommendation: Start with 10-20 images for testing\")\n</pre> import pandas as pd  print(\"Cost Estimation for Different Batch Sizes\") print(\"=\" * 80)  # Test different image counts test_counts = [10, 20, 50, 100, 200] estimates = []  for count in test_counts:     # Temporarily set count     cfg.set('generation.num_images', count)     cost_est = cfg.estimate_cost()     estimates.append(cost_est)  # Create comparison table df = pd.DataFrame(estimates) df = df[['num_images', 'resolution', 'image_generation', 'captions', 'labels', 'comments', 'total_estimated']] df.columns = ['Images', 'Resolution', 'Image Gen', 'Captions', 'Labels', 'Comments', 'Total (USD)']  print(df.to_string(index=False))  print(\"\\n\" + \"=\" * 80) print(\"\\nNotes:\") print(\"- Costs are estimates based on current Gemini API pricing\") print(\"- Actual costs may vary based on prompt complexity and API changes\") print(\"- Free tier has usage limits - start with smaller batches\") print(\"\\nRecommendation: Start with 10-20 images for testing\") In\u00a0[\u00a0]: Copied! <pre>print(\"Checking directory structure...\\n\")\n\n# Reset configuration\ncfg = config.load_config()\n\n# Check directories\ndirectories = {\n    'Output': cfg.get_output_path(),\n    'Images': cfg.get_output_path('images'),\n    'Captions': cfg.get_output_path('captions'),\n    'Labels': cfg.get_output_path('labels'),\n    'Comments': cfg.get_output_path('comments'),\n    'Metadata': cfg.get_output_path('metadata'),\n    'Raw Data': cfg.get_data_path('raw'),\n    'QA': cfg.get_data_path('../qa')\n}\n\nfor name, path in directories.items():\n    exists = path.exists()\n    status = \"\u2713\" if exists else \"\u2717\"\n    print(f\"{status} {name}: {path}\")\n\nprint(\"\\n\u2713 Directory structure ready!\")\n</pre> print(\"Checking directory structure...\\n\")  # Reset configuration cfg = config.load_config()  # Check directories directories = {     'Output': cfg.get_output_path(),     'Images': cfg.get_output_path('images'),     'Captions': cfg.get_output_path('captions'),     'Labels': cfg.get_output_path('labels'),     'Comments': cfg.get_output_path('comments'),     'Metadata': cfg.get_output_path('metadata'),     'Raw Data': cfg.get_data_path('raw'),     'QA': cfg.get_data_path('../qa') }  for name, path in directories.items():     exists = path.exists()     status = \"\u2713\" if exists else \"\u2717\"     print(f\"{status} {name}: {path}\")  print(\"\\n\u2713 Directory structure ready!\") In\u00a0[\u00a0]: Copied! <pre>print(\"\\n\" + \"=\" * 80)\nprint(\"ENVIRONMENT SETUP COMPLETE\")\nprint(\"=\" * 80)\n\nprint(\"\\n\u2713 Python packages installed\")\nprint(\"\u2713 Custom modules imported\")\nprint(\"\u2713 Configuration loaded\")\nprint(\"\u2713 API key configured\")\nprint(\"\u2713 API connection tested\")\nprint(\"\u2713 Test image generated\")\nprint(\"\u2713 Directory structure ready\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"NEXT STEPS\")\nprint(\"=\" * 80)\n\nprint(\"\\n1. Run notebook 02_prepare_source_data.ipynb to fetch source data\")\nprint(\"2. Run notebook 03_generate_images.ipynb to generate synthetic images\")\nprint(\"3. Run notebook 04_generate_metadata.ipynb for captions/labels/comments\")\nprint(\"4. Run notebook 05_quality_assurance.ipynb for QA checks\")\n\nprint(\"\\n\ud83d\udca1 Tip: Start with a small batch (10-20 images) to test the full pipeline\")\nprint(\"   You can increase the num_images in config/generation_config.yaml later\")\n</pre> print(\"\\n\" + \"=\" * 80) print(\"ENVIRONMENT SETUP COMPLETE\") print(\"=\" * 80)  print(\"\\n\u2713 Python packages installed\") print(\"\u2713 Custom modules imported\") print(\"\u2713 Configuration loaded\") print(\"\u2713 API key configured\") print(\"\u2713 API connection tested\") print(\"\u2713 Test image generated\") print(\"\u2713 Directory structure ready\")  print(\"\\n\" + \"=\" * 80) print(\"NEXT STEPS\") print(\"=\" * 80)  print(\"\\n1. Run notebook 02_prepare_source_data.ipynb to fetch source data\") print(\"2. Run notebook 03_generate_images.ipynb to generate synthetic images\") print(\"3. Run notebook 04_generate_metadata.ipynb for captions/labels/comments\") print(\"4. Run notebook 05_quality_assurance.ipynb for QA checks\")  print(\"\\n\ud83d\udca1 Tip: Start with a small batch (10-20 images) to test the full pipeline\") print(\"   You can increase the num_images in config/generation_config.yaml later\")"},{"location":"notebooks/01_setup_and_test/#setup-and-environment-test","title":"Setup and Environment Test\u00b6","text":"<p>This notebook validates your environment and tests API connectivity before running the full pipeline.</p> <p>Workshop: AI/ML Pipeline - Synthetic Data Generation Date: January 23, 2026 Platform: CyVerse Jupyter Lab PyTorch GPU</p>"},{"location":"notebooks/01_setup_and_test/#what-this-notebook-does","title":"What This Notebook Does\u00b6","text":"<ol> <li>Verifies all required packages are installed</li> <li>Tests configuration loading</li> <li>Validates API authentication</li> <li>Generates a test image</li> <li>Estimates costs for different batch sizes</li> <li>Provides troubleshooting guidance</li> </ol>"},{"location":"notebooks/01_setup_and_test/#1-package-verification","title":"1. Package Verification\u00b6","text":"<p>First, let's verify all required packages are installed.</p>"},{"location":"notebooks/01_setup_and_test/#2-import-custom-modules","title":"2. Import Custom Modules\u00b6","text":"<p>Import our custom modules from the src/ directory.</p>"},{"location":"notebooks/01_setup_and_test/#3-configuration-test","title":"3. Configuration Test\u00b6","text":"<p>Test loading configuration from files.</p>"},{"location":"notebooks/01_setup_and_test/#4-api-key-validation","title":"4. API Key Validation\u00b6","text":"<p>Check if Google Gemini API key is configured.</p>"},{"location":"notebooks/01_setup_and_test/#5-api-connection-test","title":"5. API Connection Test\u00b6","text":"<p>Test connection to Google Gemini API.</p>"},{"location":"notebooks/01_setup_and_test/#6-generate-test-image","title":"6. Generate Test Image\u00b6","text":"<p>Generate a single test image to verify everything works.</p>"},{"location":"notebooks/01_setup_and_test/#7-cost-estimation","title":"7. Cost Estimation\u00b6","text":"<p>Estimate costs for different batch sizes before running the full pipeline.</p>"},{"location":"notebooks/01_setup_and_test/#8-directory-structure-check","title":"8. Directory Structure Check\u00b6","text":"<p>Verify all output directories are ready.</p>"},{"location":"notebooks/01_setup_and_test/#9-system-summary","title":"9. System Summary\u00b6","text":"<p>Complete system check summary.</p>"},{"location":"notebooks/01_setup_and_test/#troubleshooting-guide","title":"Troubleshooting Guide\u00b6","text":""},{"location":"notebooks/01_setup_and_test/#common-issues","title":"Common Issues\u00b6","text":"<p>1. ModuleNotFoundError</p> <ul> <li>Run: <code>pip install -r requirements.txt</code> from the project root</li> <li>Ensure you're using the correct Python environment</li> </ul> <p>2. API Key Error</p> <ul> <li>Get API key from: https://makersuite.google.com/app/apikey</li> <li>Copy config/.env.example to config/.env</li> <li>Add your key: GOOGLE_API_KEY=your_key_here</li> </ul> <p>3. API Connection Failed</p> <ul> <li>Check internet connection</li> <li>Verify API key is correct</li> <li>Check API quota limits</li> </ul> <p>4. Image Generation Timeout</p> <ul> <li>Increase timeout in rate limiter settings</li> <li>Check API status: https://status.openai.com/</li> <li>Try simpler prompts</li> </ul> <p>5. Out of Memory</p> <ul> <li>Reduce batch size in config</li> <li>Close other applications</li> <li>Restart Jupyter kernel</li> </ul>"},{"location":"notebooks/01_setup_and_test/#getting-help","title":"Getting Help\u00b6","text":"<ul> <li>Workshop support: Contact instructors</li> <li>Documentation: Check README.md and CLAUDE.md</li> <li>CyVerse support: https://cyverse.org/support</li> </ul>"},{"location":"notebooks/02_prepare_source_data/","title":"2. Prepare Source Data","text":"In\u00a0[\u00a0]: Copied! <pre>import sys\nfrom pathlib import Path\nimport json\nimport pandas as pd\n\n# Add parent directory to path to import src modules\nparent_dir = Path.cwd().parent\nif str(parent_dir) not in sys.path:\n    sys.path.insert(0, str(parent_dir))\n\nfrom src import config, data_loader\n\nprint(\"\u2713 Modules imported successfully\")\n</pre> import sys from pathlib import Path import json import pandas as pd  # Add parent directory to path to import src modules parent_dir = Path.cwd().parent if str(parent_dir) not in sys.path:     sys.path.insert(0, str(parent_dir))  from src import config, data_loader  print(\"\u2713 Modules imported successfully\") In\u00a0[\u00a0]: Copied! <pre># Load configuration\ncfg = config.load_config()\n\n# Get source data configuration\nsource_config = cfg.source_data\n\nprint(\"Configuration loaded:\")\nprint(f\"  Atropia samples: {source_config['atropia']['num_samples']}\")\nprint(f\"  World Bank profiles: {source_config['worldbank']['num_profiles']}\")\nprint(f\"\\n\u2713 Configuration ready\")\n</pre> # Load configuration cfg = config.load_config()  # Get source data configuration source_config = cfg.source_data  print(\"Configuration loaded:\") print(f\"  Atropia samples: {source_config['atropia']['num_samples']}\") print(f\"  World Bank profiles: {source_config['worldbank']['num_profiles']}\") print(f\"\\n\u2713 Configuration ready\") In\u00a0[\u00a0]: Copied! <pre># Initialize Atropia data loader\ndata_dir = cfg.get_data_path('raw')\natropia_loader = data_loader.AtropiaDataLoader(\n    data_dir=data_dir,\n    num_samples=source_config['atropia']['num_samples']\n)\n\n# Fetch data\nprint(\"Fetching Atropia data...\")\natropia_data = atropia_loader.fetch_data()\n\nprint(f\"\\n\u2713 Loaded {len(atropia_data)} Atropia samples\")\nprint(f\"  Saved to: {data_dir / 'atropia_samples.json'}\")\n</pre> # Initialize Atropia data loader data_dir = cfg.get_data_path('raw') atropia_loader = data_loader.AtropiaDataLoader(     data_dir=data_dir,     num_samples=source_config['atropia']['num_samples'] )  # Fetch data print(\"Fetching Atropia data...\") atropia_data = atropia_loader.fetch_data()  print(f\"\\n\u2713 Loaded {len(atropia_data)} Atropia samples\") print(f\"  Saved to: {data_dir / 'atropia_samples.json'}\") In\u00a0[\u00a0]: Copied! <pre># Display first 3 samples\nprint(\"Sample Atropia News:\")\nprint(\"=\" * 80)\n\nfor i, sample in enumerate(atropia_data[:3], 1):\n    print(f\"\\n{i}. {sample['title']}\")\n    print(f\"   Theme: {sample['theme']}\")\n    print(f\"   Location: {sample['location']}\")\n    print(f\"   Excerpt: {sample['excerpt']}\")\n    print(\"-\" * 80)\n</pre> # Display first 3 samples print(\"Sample Atropia News:\") print(\"=\" * 80)  for i, sample in enumerate(atropia_data[:3], 1):     print(f\"\\n{i}. {sample['title']}\")     print(f\"   Theme: {sample['theme']}\")     print(f\"   Location: {sample['location']}\")     print(f\"   Excerpt: {sample['excerpt']}\")     print(\"-\" * 80) In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nfrom collections import Counter\n\n# Count themes\nthemes = [sample['theme'] for sample in atropia_data]\ntheme_counts = Counter(themes)\n\n# Plot\nplt.figure(figsize=(10, 6))\nplt.bar(theme_counts.keys(), theme_counts.values())\nplt.xlabel('Theme')\nplt.ylabel('Count')\nplt.title('Atropia Data: Theme Distribution')\nplt.xticks(rotation=45, ha='right')\nplt.tight_layout()\nplt.show()\n\nprint(\"Theme distribution:\")\nfor theme, count in theme_counts.most_common():\n    print(f\"  {theme}: {count}\")\n</pre> import matplotlib.pyplot as plt from collections import Counter  # Count themes themes = [sample['theme'] for sample in atropia_data] theme_counts = Counter(themes)  # Plot plt.figure(figsize=(10, 6)) plt.bar(theme_counts.keys(), theme_counts.values()) plt.xlabel('Theme') plt.ylabel('Count') plt.title('Atropia Data: Theme Distribution') plt.xticks(rotation=45, ha='right') plt.tight_layout() plt.show()  print(\"Theme distribution:\") for theme, count in theme_counts.most_common():     print(f\"  {theme}: {count}\") In\u00a0[\u00a0]: Copied! <pre># Initialize World Bank data loader\nworldbank_loader = data_loader.WorldBankDataLoader(\n    data_dir=data_dir,\n    num_profiles=source_config['worldbank']['num_profiles']\n)\n\n# Fetch data\nprint(\"Fetching World Bank demographics...\")\nworldbank_data = worldbank_loader.fetch_data()\n\nprint(f\"\\n\u2713 Loaded {len(worldbank_data)} demographic profiles\")\nprint(f\"  Saved to: {data_dir / 'worldbank_demographics.csv'}\")\n</pre> # Initialize World Bank data loader worldbank_loader = data_loader.WorldBankDataLoader(     data_dir=data_dir,     num_profiles=source_config['worldbank']['num_profiles'] )  # Fetch data print(\"Fetching World Bank demographics...\") worldbank_data = worldbank_loader.fetch_data()  print(f\"\\n\u2713 Loaded {len(worldbank_data)} demographic profiles\") print(f\"  Saved to: {data_dir / 'worldbank_demographics.csv'}\") In\u00a0[\u00a0]: Copied! <pre># Display first few profiles\nprint(\"Sample Demographic Profiles:\")\nprint(worldbank_data.head(10))\n\nprint(\"\\nDataset Info:\")\nprint(worldbank_data.info())\n</pre> # Display first few profiles print(\"Sample Demographic Profiles:\") print(worldbank_data.head(10))  print(\"\\nDataset Info:\") print(worldbank_data.info()) In\u00a0[\u00a0]: Copied! <pre># Create subplots for different demographic categories\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\nfig.suptitle('World Bank Demographics: Distribution Analysis', fontsize=16)\n\n# Age groups\nworldbank_data['age_group'].value_counts().plot(kind='bar', ax=axes[0, 0])\naxes[0, 0].set_title('Age Groups')\naxes[0, 0].set_xlabel('')\n\n# Occupations\nworldbank_data['occupation'].value_counts().plot(kind='bar', ax=axes[0, 1])\naxes[0, 1].set_title('Occupations')\naxes[0, 1].set_xlabel('')\naxes[0, 1].tick_params(axis='x', rotation=45)\n\n# Education levels\nworldbank_data['education_level'].value_counts().plot(kind='bar', ax=axes[0, 2])\naxes[0, 2].set_title('Education Levels')\naxes[0, 2].set_xlabel('')\n\n# Settings\nworldbank_data['setting'].value_counts().plot(kind='bar', ax=axes[1, 0])\naxes[1, 0].set_title('Settings (Urban/Rural)')\naxes[1, 0].set_xlabel('')\n\n# Household sizes\nworldbank_data['household_size'].value_counts().sort_index().plot(kind='bar', ax=axes[1, 1])\naxes[1, 1].set_title('Household Sizes')\naxes[1, 1].set_xlabel('')\n\n# Hide last subplot\naxes[1, 2].axis('off')\n\nplt.tight_layout()\nplt.show()\n</pre> # Create subplots for different demographic categories fig, axes = plt.subplots(2, 3, figsize=(15, 10)) fig.suptitle('World Bank Demographics: Distribution Analysis', fontsize=16)  # Age groups worldbank_data['age_group'].value_counts().plot(kind='bar', ax=axes[0, 0]) axes[0, 0].set_title('Age Groups') axes[0, 0].set_xlabel('')  # Occupations worldbank_data['occupation'].value_counts().plot(kind='bar', ax=axes[0, 1]) axes[0, 1].set_title('Occupations') axes[0, 1].set_xlabel('') axes[0, 1].tick_params(axis='x', rotation=45)  # Education levels worldbank_data['education_level'].value_counts().plot(kind='bar', ax=axes[0, 2]) axes[0, 2].set_title('Education Levels') axes[0, 2].set_xlabel('')  # Settings worldbank_data['setting'].value_counts().plot(kind='bar', ax=axes[1, 0]) axes[1, 0].set_title('Settings (Urban/Rural)') axes[1, 0].set_xlabel('')  # Household sizes worldbank_data['household_size'].value_counts().sort_index().plot(kind='bar', ax=axes[1, 1]) axes[1, 1].set_title('Household Sizes') axes[1, 1].set_xlabel('')  # Hide last subplot axes[1, 2].axis('off')  plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre># Initialize social media data loader\nsocialmedia_loader = data_loader.SocialMediaDataLoader(data_dir=data_dir)\n\n# Load descriptions\nprint(\"Loading social media visual references...\")\nsocialmedia_data = socialmedia_loader.load_descriptions()\n\nprint(f\"\\n\u2713 Loaded {len(socialmedia_data)} visual references\")\n</pre> # Initialize social media data loader socialmedia_loader = data_loader.SocialMediaDataLoader(data_dir=data_dir)  # Load descriptions print(\"Loading social media visual references...\") socialmedia_data = socialmedia_loader.load_descriptions()  print(f\"\\n\u2713 Loaded {len(socialmedia_data)} visual references\") In\u00a0[\u00a0]: Copied! <pre># Display first few references\nprint(\"Sample Visual References:\")\nprint(\"=\" * 80)\n\nfor i, ref in enumerate(socialmedia_data[:5], 1):\n    print(f\"\\n{i}. {ref['description']}\")\n    print(f\"   Setting: {ref.get('setting', 'N/A')}\")\n    print(f\"   Activity: {ref.get('activity_level', 'N/A')}\")\n    print(\"-\" * 80)\n</pre> # Display first few references print(\"Sample Visual References:\") print(\"=\" * 80)  for i, ref in enumerate(socialmedia_data[:5], 1):     print(f\"\\n{i}. {ref['description']}\")     print(f\"   Setting: {ref.get('setting', 'N/A')}\")     print(f\"   Activity: {ref.get('activity_level', 'N/A')}\")     print(\"-\" * 80) In\u00a0[\u00a0]: Copied! <pre># Initialize data combiner\ncombiner = data_loader.DataCombiner(\n    atropia_loader=atropia_loader,\n    worldbank_loader=worldbank_loader,\n    socialmedia_loader=socialmedia_loader\n)\n\n# Generate 5 sample combinations\nprint(\"Generating sample data combinations...\")\ncombined_samples = combiner.sample_combined(n=5)\n\nprint(f\"\\n\u2713 Generated {len(combined_samples)} combined samples\")\n</pre> # Initialize data combiner combiner = data_loader.DataCombiner(     atropia_loader=atropia_loader,     worldbank_loader=worldbank_loader,     socialmedia_loader=socialmedia_loader )  # Generate 5 sample combinations print(\"Generating sample data combinations...\") combined_samples = combiner.sample_combined(n=5)  print(f\"\\n\u2713 Generated {len(combined_samples)} combined samples\") In\u00a0[\u00a0]: Copied! <pre># Display combined samples\nprint(\"Sample Combined Data:\")\nprint(\"=\" * 80)\n\nfor i, sample in enumerate(combined_samples, 1):\n    print(f\"\\nCombination {i}:\")\n    print(f\"  Atropia Theme: {sample['atropia']['theme']}\")\n    print(f\"  Atropia Location: {sample['atropia']['location']}\")\n    print(f\"  Demographics: Age {sample['demographics']['age_group']}, {sample['demographics']['occupation']}\")\n    print(f\"  Visual Reference: {sample['visual_reference']['description']}\")\n    print(\"-\" * 80)\n</pre> # Display combined samples print(\"Sample Combined Data:\") print(\"=\" * 80)  for i, sample in enumerate(combined_samples, 1):     print(f\"\\nCombination {i}:\")     print(f\"  Atropia Theme: {sample['atropia']['theme']}\")     print(f\"  Atropia Location: {sample['atropia']['location']}\")     print(f\"  Demographics: Age {sample['demographics']['age_group']}, {sample['demographics']['occupation']}\")     print(f\"  Visual Reference: {sample['visual_reference']['description']}\")     print(\"-\" * 80) In\u00a0[\u00a0]: Copied! <pre>print(\"\\n\" + \"=\" * 80)\nprint(\"SOURCE DATA PREPARATION COMPLETE\")\nprint(\"=\" * 80)\nprint(f\"\\n\u2713 Atropia samples: {len(atropia_data)}\")\nprint(f\"\u2713 World Bank profiles: {len(worldbank_data)}\")\nprint(f\"\u2713 Visual references: {len(socialmedia_data)}\")\nprint(f\"\\nAll data saved to: {data_dir}\")\nprint(\"\\nNext step: Run notebook 03_generate_images.ipynb to create synthetic images\")\n</pre> print(\"\\n\" + \"=\" * 80) print(\"SOURCE DATA PREPARATION COMPLETE\") print(\"=\" * 80) print(f\"\\n\u2713 Atropia samples: {len(atropia_data)}\") print(f\"\u2713 World Bank profiles: {len(worldbank_data)}\") print(f\"\u2713 Visual references: {len(socialmedia_data)}\") print(f\"\\nAll data saved to: {data_dir}\") print(\"\\nNext step: Run notebook 03_generate_images.ipynb to create synthetic images\")"},{"location":"notebooks/02_prepare_source_data/#prepare-source-data","title":"Prepare Source Data\u00b6","text":"<p>This notebook fetches and prepares data from three sources:</p> <ol> <li>Atropia Data: Fictional country news samples for context</li> <li>World Bank Demographics: Synthetic demographic profiles</li> <li>Social Media References: Visual descriptions for image generation</li> </ol> <p>Workshop: AI/ML Pipeline - Synthetic Data Generation Date: January 23, 2026 Platform: CyVerse Jupyter Lab PyTorch GPU</p>"},{"location":"notebooks/02_prepare_source_data/#setup","title":"Setup\u00b6","text":"<p>Import required modules and configure paths.</p>"},{"location":"notebooks/02_prepare_source_data/#load-configuration","title":"Load Configuration\u00b6","text":"<p>Load configuration settings for data fetching.</p>"},{"location":"notebooks/02_prepare_source_data/#1-fetch-atropia-data","title":"1. Fetch Atropia Data\u00b6","text":"<p>Atropia is a fictional country used in U.S. military training scenarios. We'll fetch (or generate) news samples about political events, protests, and civil society activities.</p>"},{"location":"notebooks/02_prepare_source_data/#preview-atropia-data","title":"Preview Atropia Data\u00b6","text":"<p>Let's look at a few samples to understand the data structure.</p>"},{"location":"notebooks/02_prepare_source_data/#analyze-atropia-themes","title":"Analyze Atropia Themes\u00b6","text":"<p>Let's see the distribution of themes in our dataset.</p>"},{"location":"notebooks/02_prepare_source_data/#2-fetch-world-bank-demographics","title":"2. Fetch World Bank Demographics\u00b6","text":"<p>Generate synthetic demographic profiles based on World Bank data patterns.</p>"},{"location":"notebooks/02_prepare_source_data/#preview-world-bank-demographics","title":"Preview World Bank Demographics\u00b6","text":""},{"location":"notebooks/02_prepare_source_data/#analyze-demographics-distribution","title":"Analyze Demographics Distribution\u00b6","text":""},{"location":"notebooks/02_prepare_source_data/#3-load-social-media-references","title":"3. Load Social Media References\u00b6","text":"<p>Load visual descriptions or reference images for prompt construction.</p>"},{"location":"notebooks/02_prepare_source_data/#preview-visual-references","title":"Preview Visual References\u00b6","text":""},{"location":"notebooks/02_prepare_source_data/#4-combine-data-sources","title":"4. Combine Data Sources\u00b6","text":"<p>Demonstrate how data from all three sources can be combined for prompt generation.</p>"},{"location":"notebooks/02_prepare_source_data/#preview-combined-data","title":"Preview Combined Data\u00b6","text":""},{"location":"notebooks/02_prepare_source_data/#summary","title":"Summary\u00b6","text":"<p>All source data has been prepared and is ready for image generation!</p>"},{"location":"notebooks/03_generate_images/","title":"3. Generate Images","text":"In\u00a0[\u00a0]: Copied! <pre>import sys\nfrom pathlib import Path\nimport time\nfrom datetime import datetime\nfrom IPython.display import display, clear_output\nfrom tqdm.notebook import tqdm\n\n# Add parent directory to path\nparent_dir = Path.cwd().parent\nif str(parent_dir) not in sys.path:\n    sys.path.insert(0, str(parent_dir))\n\nfrom src import config, gemini_client, data_loader, prompt_builder, output_handler\n\nprint(\"\u2713 All modules imported successfully\")\nprint(f\"  Working directory: {Path.cwd()}\")\n</pre> import sys from pathlib import Path import time from datetime import datetime from IPython.display import display, clear_output from tqdm.notebook import tqdm  # Add parent directory to path parent_dir = Path.cwd().parent if str(parent_dir) not in sys.path:     sys.path.insert(0, str(parent_dir))  from src import config, gemini_client, data_loader, prompt_builder, output_handler  print(\"\u2713 All modules imported successfully\") print(f\"  Working directory: {Path.cwd()}\") In\u00a0[\u00a0]: Copied! <pre># Load configuration\ncfg = config.load_config()\n\nprint(\"Current Configuration:\")\nprint(\"=\" * 80)\nprint(f\"\\nImages to generate: {cfg.generation['num_images']}\")\nprint(f\"Batch size: {cfg.generation['batch_size']}\")\nprint(f\"Resolution: {cfg.generation['resolution']}\")\nprint(f\"Model: {cfg.generation['model']}\")\nprint(f\"\\nPrompt style: {cfg.prompts['style']}\")\nprint(f\"Prompt complexity: {cfg.prompts['complexity']}\")\nprint(f\"\\nRate limit: {cfg.rate_limiting['requests_per_minute']} requests/minute\")\n\nprint(\"\\n\" + \"=\" * 80)\n</pre> # Load configuration cfg = config.load_config()  print(\"Current Configuration:\") print(\"=\" * 80) print(f\"\\nImages to generate: {cfg.generation['num_images']}\") print(f\"Batch size: {cfg.generation['batch_size']}\") print(f\"Resolution: {cfg.generation['resolution']}\") print(f\"Model: {cfg.generation['model']}\") print(f\"\\nPrompt style: {cfg.prompts['style']}\") print(f\"Prompt complexity: {cfg.prompts['complexity']}\") print(f\"\\nRate limit: {cfg.rate_limiting['requests_per_minute']} requests/minute\")  print(\"\\n\" + \"=\" * 80) In\u00a0[\u00a0]: Copied! <pre># OPTIONAL: Adjust number of images\n# cfg.set('generation.num_images', 20)  # Change to desired number\n\n# OPTIONAL: Adjust batch size\n# cfg.set('generation.batch_size', 5)  # Smaller batches for testing\n\nprint(\"Settings adjusted (if any changes made above)\")\n</pre> # OPTIONAL: Adjust number of images # cfg.set('generation.num_images', 20)  # Change to desired number  # OPTIONAL: Adjust batch size # cfg.set('generation.batch_size', 5)  # Smaller batches for testing  print(\"Settings adjusted (if any changes made above)\") In\u00a0[\u00a0]: Copied! <pre>cost_estimate = cfg.estimate_cost()\n\nprint(\"Cost Estimation:\")\nprint(\"=\" * 80)\nprint(f\"\\nNumber of images: {cost_estimate['num_images']}\")\nprint(f\"Resolution: {cost_estimate['resolution']}\")\nprint(f\"\\nEstimated Costs (USD):\")\nprint(f\"  Image generation: ${cost_estimate['image_generation']:.4f}\")\nprint(f\"  Captions: ${cost_estimate['captions']:.4f}\")\nprint(f\"  Labels: ${cost_estimate['labels']:.4f}\")\nprint(f\"  Comments: ${cost_estimate['comments']:.4f}\")\nprint(f\"\\n  TOTAL: ${cost_estimate['total_estimated']:.4f}\")\nprint(\"\\n\" + \"=\" * 80)\nprint(\"\\nNote: Actual costs may vary. These are estimates based on typical usage.\")\nprint(\"Free tier limits apply - start with small batches to avoid quota issues.\")\n</pre> cost_estimate = cfg.estimate_cost()  print(\"Cost Estimation:\") print(\"=\" * 80) print(f\"\\nNumber of images: {cost_estimate['num_images']}\") print(f\"Resolution: {cost_estimate['resolution']}\") print(f\"\\nEstimated Costs (USD):\") print(f\"  Image generation: ${cost_estimate['image_generation']:.4f}\") print(f\"  Captions: ${cost_estimate['captions']:.4f}\") print(f\"  Labels: ${cost_estimate['labels']:.4f}\") print(f\"  Comments: ${cost_estimate['comments']:.4f}\") print(f\"\\n  TOTAL: ${cost_estimate['total_estimated']:.4f}\") print(\"\\n\" + \"=\" * 80) print(\"\\nNote: Actual costs may vary. These are estimates based on typical usage.\") print(\"Free tier limits apply - start with small batches to avoid quota issues.\") In\u00a0[\u00a0]: Copied! <pre>print(\"Loading source data...\\n\")\n\ndata_dir = cfg.get_data_path('raw')\n\n# Initialize data loaders\natropia_loader = data_loader.AtropiaDataLoader(data_dir=data_dir)\nworldbank_loader = data_loader.WorldBankDataLoader(data_dir=data_dir)\nsocialmedia_loader = data_loader.SocialMediaDataLoader(data_dir=data_dir)\n\n# Load data\natropia_data = atropia_loader.load_data()\nprint(f\"\u2713 Loaded {len(atropia_data)} Atropia samples\")\n\nworldbank_data = worldbank_loader.load_data()\nprint(f\"\u2713 Loaded {len(worldbank_data)} World Bank profiles\")\n\nsocialmedia_data = socialmedia_loader.load_descriptions()\nprint(f\"\u2713 Loaded {len(socialmedia_data)} visual references\")\n\n# Initialize combiner\ncombiner = data_loader.DataCombiner(\n    atropia_loader=atropia_loader,\n    worldbank_loader=worldbank_loader,\n    socialmedia_loader=socialmedia_loader\n)\n\nprint(\"\\n\u2713 All source data loaded and ready\")\n</pre> print(\"Loading source data...\\n\")  data_dir = cfg.get_data_path('raw')  # Initialize data loaders atropia_loader = data_loader.AtropiaDataLoader(data_dir=data_dir) worldbank_loader = data_loader.WorldBankDataLoader(data_dir=data_dir) socialmedia_loader = data_loader.SocialMediaDataLoader(data_dir=data_dir)  # Load data atropia_data = atropia_loader.load_data() print(f\"\u2713 Loaded {len(atropia_data)} Atropia samples\")  worldbank_data = worldbank_loader.load_data() print(f\"\u2713 Loaded {len(worldbank_data)} World Bank profiles\")  socialmedia_data = socialmedia_loader.load_descriptions() print(f\"\u2713 Loaded {len(socialmedia_data)} visual references\")  # Initialize combiner combiner = data_loader.DataCombiner(     atropia_loader=atropia_loader,     worldbank_loader=worldbank_loader,     socialmedia_loader=socialmedia_loader )  print(\"\\n\u2713 All source data loaded and ready\") In\u00a0[\u00a0]: Copied! <pre>print(\"Building prompts...\\n\")\n\n# Initialize prompt builder\nbuilder = prompt_builder.PromptBuilder(\n    style=cfg.prompts['style'],\n    complexity=cfg.prompts['complexity'],\n    include_temporal=cfg.prompts['include_temporal_context'],\n    include_demographics=cfg.prompts['include_demographics'],\n    themes=cfg.prompts['themes'],\n    settings=cfg.prompts['settings']\n)\n\n# Generate combined data samples\nnum_images = cfg.generation['num_images']\ncombined_samples = combiner.sample_combined(n=num_images)\n\n# Build prompts\nprompts_data = builder.build_batch_prompts(combined_samples)\n\nprint(f\"\u2713 Built {len(prompts_data)} prompts\")\nprint(f\"  Style: {cfg.prompts['style']}\")\nprint(f\"  Complexity: {cfg.prompts['complexity']}\")\n</pre> print(\"Building prompts...\\n\")  # Initialize prompt builder builder = prompt_builder.PromptBuilder(     style=cfg.prompts['style'],     complexity=cfg.prompts['complexity'],     include_temporal=cfg.prompts['include_temporal_context'],     include_demographics=cfg.prompts['include_demographics'],     themes=cfg.prompts['themes'],     settings=cfg.prompts['settings'] )  # Generate combined data samples num_images = cfg.generation['num_images'] combined_samples = combiner.sample_combined(n=num_images)  # Build prompts prompts_data = builder.build_batch_prompts(combined_samples)  print(f\"\u2713 Built {len(prompts_data)} prompts\") print(f\"  Style: {cfg.prompts['style']}\") print(f\"  Complexity: {cfg.prompts['complexity']}\") In\u00a0[\u00a0]: Copied! <pre>print(\"Sample Prompts:\")\nprint(\"=\" * 80)\n\nfor i in range(min(3, len(prompts_data))):\n    prompt_info = prompts_data[i]\n    print(f\"\\nPrompt {i+1}:\")\n    print(f\"  {prompt_info['prompt'][:200]}...\")\n    print(f\"\\n  Source - Theme: {prompt_info['source_data']['atropia']['theme']}\")\n    print(f\"  Source - Demographics: Age {prompt_info['source_data']['demographics']['age_group']}, \"\n          f\"{prompt_info['source_data']['demographics']['occupation']}\")\n    print(\"-\" * 80)\n</pre> print(\"Sample Prompts:\") print(\"=\" * 80)  for i in range(min(3, len(prompts_data))):     prompt_info = prompts_data[i]     print(f\"\\nPrompt {i+1}:\")     print(f\"  {prompt_info['prompt'][:200]}...\")     print(f\"\\n  Source - Theme: {prompt_info['source_data']['atropia']['theme']}\")     print(f\"  Source - Demographics: Age {prompt_info['source_data']['demographics']['age_group']}, \"           f\"{prompt_info['source_data']['demographics']['occupation']}\")     print(\"-\" * 80) In\u00a0[\u00a0]: Copied! <pre>print(\"Initializing generation pipeline...\\n\")\n\n# Initialize rate limiter\nrate_limiter = gemini_client.RateLimiter(\n    requests_per_minute=cfg.rate_limiting['requests_per_minute'],\n    requests_per_day=cfg.rate_limiting['requests_per_day'],\n    enable_backoff=cfg.rate_limiting['enable_exponential_backoff'],\n    initial_delay=cfg.rate_limiting['initial_retry_delay'],\n    backoff_multiplier=cfg.rate_limiting['backoff_multiplier'],\n    max_retries=cfg.rate_limiting['max_retries']\n)\nprint(\"\u2713 Rate limiter initialized\")\n\n# Initialize image generator\ngenerator = gemini_client.GeminiImageGenerator(\n    api_key=cfg.api_key,\n    rate_limiter=rate_limiter,\n    model=cfg.generation['model'],\n    resolution=cfg.generation['resolution'],\n    aspect_ratio=cfg.generation['aspect_ratio']\n)\nprint(f\"\u2713 Image generator initialized (model: {cfg.generation['model']})\")\n\n# Initialize output handler\noutput_dir = cfg.get_output_path()\nhandler = output_handler.OutputHandler(\n    output_dir=output_dir,\n    image_format=cfg.output['image_format'],\n    metadata_format=cfg.output['metadata_format'],\n    export_csv=cfg.output['export_csv_summaries'],\n    date_organized=cfg.output['date_organized']\n)\nprint(f\"\u2713 Output handler initialized (output: {output_dir})\")\n\n# Initialize checkpoint manager\ncheckpoint_path = parent_dir / cfg.generation['checkpoint_file']\ncheckpoint_mgr = gemini_client.CheckpointManager(checkpoint_path)\nprint(f\"\u2713 Checkpoint manager initialized\")\n\nprint(\"\\n\u2713 Pipeline ready to generate images\")\n</pre> print(\"Initializing generation pipeline...\\n\")  # Initialize rate limiter rate_limiter = gemini_client.RateLimiter(     requests_per_minute=cfg.rate_limiting['requests_per_minute'],     requests_per_day=cfg.rate_limiting['requests_per_day'],     enable_backoff=cfg.rate_limiting['enable_exponential_backoff'],     initial_delay=cfg.rate_limiting['initial_retry_delay'],     backoff_multiplier=cfg.rate_limiting['backoff_multiplier'],     max_retries=cfg.rate_limiting['max_retries'] ) print(\"\u2713 Rate limiter initialized\")  # Initialize image generator generator = gemini_client.GeminiImageGenerator(     api_key=cfg.api_key,     rate_limiter=rate_limiter,     model=cfg.generation['model'],     resolution=cfg.generation['resolution'],     aspect_ratio=cfg.generation['aspect_ratio'] ) print(f\"\u2713 Image generator initialized (model: {cfg.generation['model']})\")  # Initialize output handler output_dir = cfg.get_output_path() handler = output_handler.OutputHandler(     output_dir=output_dir,     image_format=cfg.output['image_format'],     metadata_format=cfg.output['metadata_format'],     export_csv=cfg.output['export_csv_summaries'],     date_organized=cfg.output['date_organized'] ) print(f\"\u2713 Output handler initialized (output: {output_dir})\")  # Initialize checkpoint manager checkpoint_path = parent_dir / cfg.generation['checkpoint_file'] checkpoint_mgr = gemini_client.CheckpointManager(checkpoint_path) print(f\"\u2713 Checkpoint manager initialized\")  print(\"\\n\u2713 Pipeline ready to generate images\") In\u00a0[\u00a0]: Copied! <pre># Check for existing checkpoint\ncheckpoint = checkpoint_mgr.load_checkpoint()\n\nif checkpoint:\n    print(\"Found existing checkpoint!\")\n    print(f\"  Completed: {checkpoint.get('completed', 0)}/{checkpoint.get('total', 0)} images\")\n    print(f\"  Last checkpoint: {checkpoint.get('timestamp', 'unknown')}\")\n    \n    # Ask user if they want to resume\n    print(\"\\n\u26a0 Set RESUME = True in the next cell to continue, or False to start fresh\")\nelse:\n    print(\"No existing checkpoint found. Starting fresh generation.\")\n</pre> # Check for existing checkpoint checkpoint = checkpoint_mgr.load_checkpoint()  if checkpoint:     print(\"Found existing checkpoint!\")     print(f\"  Completed: {checkpoint.get('completed', 0)}/{checkpoint.get('total', 0)} images\")     print(f\"  Last checkpoint: {checkpoint.get('timestamp', 'unknown')}\")          # Ask user if they want to resume     print(\"\\n\u26a0 Set RESUME = True in the next cell to continue, or False to start fresh\") else:     print(\"No existing checkpoint found. Starting fresh generation.\") In\u00a0[\u00a0]: Copied! <pre># Set this to True to resume from checkpoint, False to start fresh\nRESUME = False\n\nif RESUME and checkpoint:\n    start_index = checkpoint.get('completed', 0)\n    print(f\"Resuming from image {start_index + 1}\")\nelse:\n    start_index = 0\n    checkpoint_mgr.clear_checkpoint()\n    print(\"Starting fresh generation\")\n</pre> # Set this to True to resume from checkpoint, False to start fresh RESUME = False  if RESUME and checkpoint:     start_index = checkpoint.get('completed', 0)     print(f\"Resuming from image {start_index + 1}\") else:     start_index = 0     checkpoint_mgr.clear_checkpoint()     print(\"Starting fresh generation\") In\u00a0[\u00a0]: Copied! <pre>print(\"\\n\" + \"=\" * 80)\nprint(\"STARTING IMAGE GENERATION\")\nprint(\"=\" * 80)\n\n# Generation tracking\nbatch_size = cfg.generation['batch_size']\ntotal_images = len(prompts_data)\ngenerated_count = start_index\nerror_count = 0\nstart_time = time.time()\n\nprint(f\"\\nGenerating {total_images - start_index} images in batches of {batch_size}\")\nprint(f\"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\nprint(\"\\nProgress:\")\n\n# Main generation loop\nfor i in tqdm(range(start_index, total_images), initial=start_index, total=total_images):\n    try:\n        prompt_data = prompts_data[i]\n        \n        # Generate image\n        result = generator.generate_image(prompt_data['prompt'])\n        \n        # Save image and metadata\n        image_path = handler.save_image(\n            image=result['image'],\n            index=i + 1,\n            prompt_data=prompt_data\n        )\n        \n        generated_count += 1\n        \n        # Display latest image (every 5 images)\n        if (i + 1) % 5 == 0:\n            clear_output(wait=True)\n            print(f\"\\nProgress: {generated_count}/{total_images} images\")\n            print(f\"Latest image: {image_path.name}\")\n            display(result['image'].resize((256, 256)))  # Display smaller preview\n        \n        # Save checkpoint after each batch\n        if (i + 1) % batch_size == 0:\n            checkpoint_data = {\n                'completed': generated_count,\n                'total': total_images,\n                'timestamp': datetime.now().isoformat(),\n                'last_index': i\n            }\n            checkpoint_mgr.save_checkpoint(checkpoint_data)\n            print(f\"\\n\u2713 Checkpoint saved: {generated_count}/{total_images} images\")\n        \n    except Exception as e:\n        error_count += 1\n        print(f\"\\n\u2717 Error generating image {i+1}: {e}\")\n        \n        # Continue with next image\n        continue\n\n# Final statistics\nelapsed_time = time.time() - start_time\nminutes = int(elapsed_time // 60)\nseconds = int(elapsed_time % 60)\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"GENERATION COMPLETE\")\nprint(\"=\" * 80)\nprint(f\"\\nSuccessfully generated: {generated_count} images\")\nprint(f\"Errors encountered: {error_count}\")\nprint(f\"Total time: {minutes}m {seconds}s\")\nprint(f\"Average time per image: {elapsed_time/generated_count:.1f}s\")\n\n# Clear checkpoint on successful completion\nif error_count == 0:\n    checkpoint_mgr.clear_checkpoint()\n    print(\"\\n\u2713 Checkpoint cleared (generation completed successfully)\")\n</pre> print(\"\\n\" + \"=\" * 80) print(\"STARTING IMAGE GENERATION\") print(\"=\" * 80)  # Generation tracking batch_size = cfg.generation['batch_size'] total_images = len(prompts_data) generated_count = start_index error_count = 0 start_time = time.time()  print(f\"\\nGenerating {total_images - start_index} images in batches of {batch_size}\") print(f\"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\") print(\"\\nProgress:\")  # Main generation loop for i in tqdm(range(start_index, total_images), initial=start_index, total=total_images):     try:         prompt_data = prompts_data[i]                  # Generate image         result = generator.generate_image(prompt_data['prompt'])                  # Save image and metadata         image_path = handler.save_image(             image=result['image'],             index=i + 1,             prompt_data=prompt_data         )                  generated_count += 1                  # Display latest image (every 5 images)         if (i + 1) % 5 == 0:             clear_output(wait=True)             print(f\"\\nProgress: {generated_count}/{total_images} images\")             print(f\"Latest image: {image_path.name}\")             display(result['image'].resize((256, 256)))  # Display smaller preview                  # Save checkpoint after each batch         if (i + 1) % batch_size == 0:             checkpoint_data = {                 'completed': generated_count,                 'total': total_images,                 'timestamp': datetime.now().isoformat(),                 'last_index': i             }             checkpoint_mgr.save_checkpoint(checkpoint_data)             print(f\"\\n\u2713 Checkpoint saved: {generated_count}/{total_images} images\")              except Exception as e:         error_count += 1         print(f\"\\n\u2717 Error generating image {i+1}: {e}\")                  # Continue with next image         continue  # Final statistics elapsed_time = time.time() - start_time minutes = int(elapsed_time // 60) seconds = int(elapsed_time % 60)  print(\"\\n\" + \"=\" * 80) print(\"GENERATION COMPLETE\") print(\"=\" * 80) print(f\"\\nSuccessfully generated: {generated_count} images\") print(f\"Errors encountered: {error_count}\") print(f\"Total time: {minutes}m {seconds}s\") print(f\"Average time per image: {elapsed_time/generated_count:.1f}s\")  # Clear checkpoint on successful completion if error_count == 0:     checkpoint_mgr.clear_checkpoint()     print(\"\\n\u2713 Checkpoint cleared (generation completed successfully)\") In\u00a0[\u00a0]: Copied! <pre># Save generation log\nlog_path = handler.save_generation_log()\nprint(f\"\u2713 Generation log saved to: {log_path}\")\n\n# Get and display summary\nsummary = handler.get_summary()\n\nprint(\"\\nGeneration Summary:\")\nprint(\"=\" * 80)\nprint(f\"Output directory: {summary['output_directory']}\")\nprint(f\"Images saved: {summary['images_saved']}\")\nprint(f\"Image format: {summary['image_format']}\")\nprint(f\"\\nDirectory structure:\")\nfor name, path in summary['directories'].items():\n    print(f\"  {name}: {path}\")\n</pre> # Save generation log log_path = handler.save_generation_log() print(f\"\u2713 Generation log saved to: {log_path}\")  # Get and display summary summary = handler.get_summary()  print(\"\\nGeneration Summary:\") print(\"=\" * 80) print(f\"Output directory: {summary['output_directory']}\") print(f\"Images saved: {summary['images_saved']}\") print(f\"Image format: {summary['image_format']}\") print(f\"\\nDirectory structure:\") for name, path in summary['directories'].items():     print(f\"  {name}: {path}\") In\u00a0[\u00a0]: Copied! <pre>import random\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\n# Get all generated images\nimage_files = list(handler.images_dir.glob(f\"*.{cfg.output['image_format']}\"))\n\nif image_files:\n    # Sample 6 random images\n    sample_files = random.sample(image_files, min(6, len(image_files)))\n    \n    # Display in grid\n    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n    fig.suptitle('Sample Generated Images', fontsize=16)\n    \n    for idx, image_file in enumerate(sample_files):\n        row = idx // 3\n        col = idx % 3\n        \n        img = Image.open(image_file)\n        axes[row, col].imshow(img)\n        axes[row, col].set_title(image_file.name)\n        axes[row, col].axis('off')\n    \n    plt.tight_layout()\n    plt.show()\nelse:\n    print(\"No images found to display\")\n</pre> import random from PIL import Image import matplotlib.pyplot as plt  # Get all generated images image_files = list(handler.images_dir.glob(f\"*.{cfg.output['image_format']}\"))  if image_files:     # Sample 6 random images     sample_files = random.sample(image_files, min(6, len(image_files)))          # Display in grid     fig, axes = plt.subplots(2, 3, figsize=(15, 10))     fig.suptitle('Sample Generated Images', fontsize=16)          for idx, image_file in enumerate(sample_files):         row = idx // 3         col = idx % 3                  img = Image.open(image_file)         axes[row, col].imshow(img)         axes[row, col].set_title(image_file.name)         axes[row, col].axis('off')          plt.tight_layout()     plt.show() else:     print(\"No images found to display\")"},{"location":"notebooks/03_generate_images/#generate-synthetic-images","title":"Generate Synthetic Images\u00b6","text":"<p>This notebook generates synthetic social movement images using Google Gemini API, combining data from Atropia, World Bank demographics, and visual references.</p> <p>Workshop: AI/ML Pipeline - Synthetic Data Generation Date: January 23, 2026 Platform: CyVerse Jupyter Lab PyTorch GPU</p>"},{"location":"notebooks/03_generate_images/#pipeline-overview","title":"Pipeline Overview\u00b6","text":"<ol> <li>Load configuration and source data</li> <li>Initialize API client with rate limiting</li> <li>Build prompts from combined data sources</li> <li>Generate images in batches with checkpoints</li> <li>Save images and metadata</li> <li>Summarize results</li> </ol>"},{"location":"notebooks/03_generate_images/#setup-and-imports","title":"Setup and Imports\u00b6","text":""},{"location":"notebooks/03_generate_images/#1-load-configuration","title":"1. Load Configuration\u00b6","text":"<p>Review and adjust generation parameters if needed.</p>"},{"location":"notebooks/03_generate_images/#adjust-settings-optional","title":"Adjust Settings (Optional)\u00b6","text":"<p>You can modify settings here if needed. Otherwise, skip this cell.</p>"},{"location":"notebooks/03_generate_images/#cost-estimation","title":"Cost Estimation\u00b6","text":"<p>Review estimated costs before proceeding.</p>"},{"location":"notebooks/03_generate_images/#2-load-source-data","title":"2. Load Source Data\u00b6","text":"<p>Load data from all three sources: Atropia, World Bank, and social media references.</p>"},{"location":"notebooks/03_generate_images/#3-build-prompts","title":"3. Build Prompts\u00b6","text":"<p>Generate prompts by combining data from all sources.</p>"},{"location":"notebooks/03_generate_images/#preview-sample-prompts","title":"Preview Sample Prompts\u00b6","text":"<p>Let's review a few prompts before generation.</p>"},{"location":"notebooks/03_generate_images/#4-initialize-generation-pipeline","title":"4. Initialize Generation Pipeline\u00b6","text":"<p>Set up API client, rate limiter, and output handler.</p>"},{"location":"notebooks/03_generate_images/#5-check-for-resume","title":"5. Check for Resume\u00b6","text":"<p>Check if there's a previous interrupted generation to resume.</p>"},{"location":"notebooks/03_generate_images/#6-generate-images","title":"6. Generate Images\u00b6","text":"<p>Main generation loop with progress tracking and checkpoints.</p>"},{"location":"notebooks/03_generate_images/#7-save-generation-log","title":"7. Save Generation Log\u00b6","text":"<p>Save complete generation log for record keeping.</p>"},{"location":"notebooks/03_generate_images/#8-preview-generated-images","title":"8. Preview Generated Images\u00b6","text":"<p>Display a few random samples from the generated dataset.</p>"},{"location":"notebooks/03_generate_images/#next-steps","title":"Next Steps\u00b6","text":"<p>Images have been generated successfully! Continue with:</p> <ol> <li>Generate Metadata: Run notebook <code>04_generate_metadata.ipynb</code> to create captions, labels, and comments</li> <li>Quality Assurance: Run notebook <code>05_quality_assurance.ipynb</code> to validate the dataset</li> </ol> <p>Your generated images are saved in: <code>data/generated/images/</code></p>"},{"location":"notebooks/04_generate_metadata/","title":"4. Generate Metadata","text":"In\u00a0[\u00a0]: Copied! <pre>import sys\nfrom pathlib import Path\nimport time\nfrom datetime import datetime\nfrom IPython.display import display\nfrom tqdm.notebook import tqdm\nfrom PIL import Image\n\n# Add parent directory to path\nparent_dir = Path.cwd().parent\nif str(parent_dir) not in sys.path:\n    sys.path.insert(0, str(parent_dir))\n\nfrom src import config, gemini_client, output_handler\n\nprint(\"\u2713 All modules imported successfully\")\n</pre> import sys from pathlib import Path import time from datetime import datetime from IPython.display import display from tqdm.notebook import tqdm from PIL import Image  # Add parent directory to path parent_dir = Path.cwd().parent if str(parent_dir) not in sys.path:     sys.path.insert(0, str(parent_dir))  from src import config, gemini_client, output_handler  print(\"\u2713 All modules imported successfully\") In\u00a0[\u00a0]: Copied! <pre># Load configuration\ncfg = config.load_config()\n\n# Initialize output handler\noutput_dir = cfg.get_output_path()\nhandler = output_handler.OutputHandler(\n    output_dir=output_dir,\n    image_format=cfg.output['image_format'],\n    metadata_format=cfg.output['metadata_format'],\n    export_csv=cfg.output['export_csv_summaries']\n)\n\n# Find all generated images\nimage_files = list(handler.images_dir.glob(f\"*.{cfg.output['image_format']}\"))\nimage_files.sort()\n\nprint(f\"Found {len(image_files)} images to process\")\nprint(f\"Images directory: {handler.images_dir}\")\n\nif len(image_files) == 0:\n    print(\"\\n\u26a0 No images found! Please run notebook 03_generate_images.ipynb first.\")\n    raise FileNotFoundError(\"No images to process\")\n</pre> # Load configuration cfg = config.load_config()  # Initialize output handler output_dir = cfg.get_output_path() handler = output_handler.OutputHandler(     output_dir=output_dir,     image_format=cfg.output['image_format'],     metadata_format=cfg.output['metadata_format'],     export_csv=cfg.output['export_csv_summaries'] )  # Find all generated images image_files = list(handler.images_dir.glob(f\"*.{cfg.output['image_format']}\")) image_files.sort()  print(f\"Found {len(image_files)} images to process\") print(f\"Images directory: {handler.images_dir}\")  if len(image_files) == 0:     print(\"\\n\u26a0 No images found! Please run notebook 03_generate_images.ipynb first.\")     raise FileNotFoundError(\"No images to process\") In\u00a0[\u00a0]: Copied! <pre># Initialize rate limiter\nrate_limiter = gemini_client.RateLimiter(\n    requests_per_minute=cfg.rate_limiting['requests_per_minute'],\n    requests_per_day=cfg.rate_limiting['requests_per_day']\n)\n\n# Initialize text generator\ntext_generator = gemini_client.GeminiTextGenerator(\n    api_key=cfg.api_key,\n    rate_limiter=rate_limiter\n)\n\nprint(\"\u2713 Text generator initialized\")\nprint(f\"  Rate limit: {cfg.rate_limiting['requests_per_minute']} requests/minute\")\n</pre> # Initialize rate limiter rate_limiter = gemini_client.RateLimiter(     requests_per_minute=cfg.rate_limiting['requests_per_minute'],     requests_per_day=cfg.rate_limiting['requests_per_day'] )  # Initialize text generator text_generator = gemini_client.GeminiTextGenerator(     api_key=cfg.api_key,     rate_limiter=rate_limiter )  print(\"\u2713 Text generator initialized\") print(f\"  Rate limit: {cfg.rate_limiting['requests_per_minute']} requests/minute\") In\u00a0[\u00a0]: Copied! <pre>metadata_config = cfg.metadata\n\nprint(\"Metadata Generation Settings:\")\nprint(\"=\" * 80)\nprint(f\"Comments per image: {metadata_config['num_comments_per_image']}\")\nprint(f\"Include hashtags: {metadata_config['include_hashtags']}\")\nprint(f\"Include emojis: {metadata_config['include_emojis']}\")\nprint(f\"Label categories: {', '.join(metadata_config['label_categories'])}\")\nprint(\"=\" * 80)\n</pre> metadata_config = cfg.metadata  print(\"Metadata Generation Settings:\") print(\"=\" * 80) print(f\"Comments per image: {metadata_config['num_comments_per_image']}\") print(f\"Include hashtags: {metadata_config['include_hashtags']}\") print(f\"Include emojis: {metadata_config['include_emojis']}\") print(f\"Label categories: {', '.join(metadata_config['label_categories'])}\") print(\"=\" * 80) In\u00a0[\u00a0]: Copied! <pre>print(\"\\nGenerating captions...\\n\")\n\ncaption_errors = 0\nstart_time = time.time()\n\nfor image_file in tqdm(image_files, desc=\"Captions\"):\n    try:\n        # Extract image ID from filename\n        image_id = image_file.stem\n        \n        # Load image\n        img = Image.open(image_file)\n        \n        # Load original metadata for context\n        original_metadata = handler.load_metadata(image_id)\n        context = None\n        if original_metadata:\n            context = f\"Social movement scene: {original_metadata.get('prompt', '')[:200]}\"\n        \n        # Generate caption\n        caption = text_generator.generate_caption(img, context=context)\n        \n        # Save caption\n        handler.save_caption(\n            image_id=image_id,\n            caption=caption,\n            context={'original_prompt': context}\n        )\n        \n    except Exception as e:\n        caption_errors += 1\n        print(f\"\\nError generating caption for {image_file.name}: {e}\")\n        continue\n\nelapsed = time.time() - start_time\nprint(f\"\\n\u2713 Captions generated: {len(image_files) - caption_errors}/{len(image_files)}\")\nprint(f\"  Time: {int(elapsed//60)}m {int(elapsed%60)}s\")\nprint(f\"  Errors: {caption_errors}\")\n</pre> print(\"\\nGenerating captions...\\n\")  caption_errors = 0 start_time = time.time()  for image_file in tqdm(image_files, desc=\"Captions\"):     try:         # Extract image ID from filename         image_id = image_file.stem                  # Load image         img = Image.open(image_file)                  # Load original metadata for context         original_metadata = handler.load_metadata(image_id)         context = None         if original_metadata:             context = f\"Social movement scene: {original_metadata.get('prompt', '')[:200]}\"                  # Generate caption         caption = text_generator.generate_caption(img, context=context)                  # Save caption         handler.save_caption(             image_id=image_id,             caption=caption,             context={'original_prompt': context}         )              except Exception as e:         caption_errors += 1         print(f\"\\nError generating caption for {image_file.name}: {e}\")         continue  elapsed = time.time() - start_time print(f\"\\n\u2713 Captions generated: {len(image_files) - caption_errors}/{len(image_files)}\") print(f\"  Time: {int(elapsed//60)}m {int(elapsed%60)}s\") print(f\"  Errors: {caption_errors}\") In\u00a0[\u00a0]: Copied! <pre>import random\nimport json\n\n# Get sample captions\ncaption_files = list(handler.captions_dir.glob(\"*_caption.json\"))\nsample_captions = random.sample(caption_files, min(3, len(caption_files)))\n\nprint(\"Sample Captions:\")\nprint(\"=\" * 80)\n\nfor caption_file in sample_captions:\n    with open(caption_file, 'r') as f:\n        caption_data = json.load(f)\n    \n    print(f\"\\nImage: {caption_data['image_id']}\")\n    print(f\"Caption: {caption_data['caption']}\")\n    print(\"-\" * 80)\n</pre> import random import json  # Get sample captions caption_files = list(handler.captions_dir.glob(\"*_caption.json\")) sample_captions = random.sample(caption_files, min(3, len(caption_files)))  print(\"Sample Captions:\") print(\"=\" * 80)  for caption_file in sample_captions:     with open(caption_file, 'r') as f:         caption_data = json.load(f)          print(f\"\\nImage: {caption_data['image_id']}\")     print(f\"Caption: {caption_data['caption']}\")     print(\"-\" * 80) In\u00a0[\u00a0]: Copied! <pre>print(\"\\nGenerating labels...\\n\")\n\nlabel_errors = 0\nstart_time = time.time()\n\nlabel_categories = metadata_config['label_categories']\n\nfor image_file in tqdm(image_files, desc=\"Labels\"):\n    try:\n        image_id = image_file.stem\n        img = Image.open(image_file)\n        \n        # Load context\n        original_metadata = handler.load_metadata(image_id)\n        context = None\n        if original_metadata:\n            context = f\"Social movement context: {original_metadata.get('prompt', '')[:200]}\"\n        \n        # Generate labels\n        labels = text_generator.generate_labels(\n            img,\n            categories=label_categories,\n            context=context\n        )\n        \n        # Save labels\n        handler.save_labels(\n            image_id=image_id,\n            labels=labels,\n            context={'categories': label_categories}\n        )\n        \n    except Exception as e:\n        label_errors += 1\n        print(f\"\\nError generating labels for {image_file.name}: {e}\")\n        continue\n\nelapsed = time.time() - start_time\nprint(f\"\\n\u2713 Labels generated: {len(image_files) - label_errors}/{len(image_files)}\")\nprint(f\"  Time: {int(elapsed//60)}m {int(elapsed%60)}s\")\nprint(f\"  Errors: {label_errors}\")\n</pre> print(\"\\nGenerating labels...\\n\")  label_errors = 0 start_time = time.time()  label_categories = metadata_config['label_categories']  for image_file in tqdm(image_files, desc=\"Labels\"):     try:         image_id = image_file.stem         img = Image.open(image_file)                  # Load context         original_metadata = handler.load_metadata(image_id)         context = None         if original_metadata:             context = f\"Social movement context: {original_metadata.get('prompt', '')[:200]}\"                  # Generate labels         labels = text_generator.generate_labels(             img,             categories=label_categories,             context=context         )                  # Save labels         handler.save_labels(             image_id=image_id,             labels=labels,             context={'categories': label_categories}         )              except Exception as e:         label_errors += 1         print(f\"\\nError generating labels for {image_file.name}: {e}\")         continue  elapsed = time.time() - start_time print(f\"\\n\u2713 Labels generated: {len(image_files) - label_errors}/{len(image_files)}\") print(f\"  Time: {int(elapsed//60)}m {int(elapsed%60)}s\") print(f\"  Errors: {label_errors}\") In\u00a0[\u00a0]: Copied! <pre># Get sample labels\nlabel_files = list(handler.labels_dir.glob(\"*_labels.json\"))\nsample_labels = random.sample(label_files, min(3, len(label_files)))\n\nprint(\"Sample Labels:\")\nprint(\"=\" * 80)\n\nfor label_file in sample_labels:\n    with open(label_file, 'r') as f:\n        label_data = json.load(f)\n    \n    print(f\"\\nImage: {label_data['image_id']}\")\n    print(\"Labels:\")\n    for category, label in label_data['labels'].items():\n        print(f\"  {category}: {label}\")\n    print(\"-\" * 80)\n</pre> # Get sample labels label_files = list(handler.labels_dir.glob(\"*_labels.json\")) sample_labels = random.sample(label_files, min(3, len(label_files)))  print(\"Sample Labels:\") print(\"=\" * 80)  for label_file in sample_labels:     with open(label_file, 'r') as f:         label_data = json.load(f)          print(f\"\\nImage: {label_data['image_id']}\")     print(\"Labels:\")     for category, label in label_data['labels'].items():         print(f\"  {category}: {label}\")     print(\"-\" * 80) In\u00a0[\u00a0]: Copied! <pre>print(\"\\nGenerating comments...\\n\")\n\ncomment_errors = 0\nstart_time = time.time()\n\nnum_comments = metadata_config['num_comments_per_image']\ninclude_hashtags = metadata_config['include_hashtags']\ninclude_emojis = metadata_config['include_emojis']\n\nfor image_file in tqdm(image_files, desc=\"Comments\"):\n    try:\n        image_id = image_file.stem\n        img = Image.open(image_file)\n        \n        # Load context\n        original_metadata = handler.load_metadata(image_id)\n        context = None\n        if original_metadata:\n            context = f\"Social movement image. Theme: {original_metadata.get('source_data', {}).get('atropia', {}).get('theme', 'civic engagement')}\"\n        \n        # Generate comments\n        comments = text_generator.generate_comments(\n            img,\n            num_comments=num_comments,\n            include_hashtags=include_hashtags,\n            include_emojis=include_emojis,\n            context=context\n        )\n        \n        # Save comments\n        handler.save_comments(\n            image_id=image_id,\n            comments=comments,\n            context={'num_requested': num_comments}\n        )\n        \n    except Exception as e:\n        comment_errors += 1\n        print(f\"\\nError generating comments for {image_file.name}: {e}\")\n        continue\n\nelapsed = time.time() - start_time\nprint(f\"\\n\u2713 Comments generated: {len(image_files) - comment_errors}/{len(image_files)}\")\nprint(f\"  Time: {int(elapsed//60)}m {int(elapsed%60)}s\")\nprint(f\"  Errors: {comment_errors}\")\n</pre> print(\"\\nGenerating comments...\\n\")  comment_errors = 0 start_time = time.time()  num_comments = metadata_config['num_comments_per_image'] include_hashtags = metadata_config['include_hashtags'] include_emojis = metadata_config['include_emojis']  for image_file in tqdm(image_files, desc=\"Comments\"):     try:         image_id = image_file.stem         img = Image.open(image_file)                  # Load context         original_metadata = handler.load_metadata(image_id)         context = None         if original_metadata:             context = f\"Social movement image. Theme: {original_metadata.get('source_data', {}).get('atropia', {}).get('theme', 'civic engagement')}\"                  # Generate comments         comments = text_generator.generate_comments(             img,             num_comments=num_comments,             include_hashtags=include_hashtags,             include_emojis=include_emojis,             context=context         )                  # Save comments         handler.save_comments(             image_id=image_id,             comments=comments,             context={'num_requested': num_comments}         )              except Exception as e:         comment_errors += 1         print(f\"\\nError generating comments for {image_file.name}: {e}\")         continue  elapsed = time.time() - start_time print(f\"\\n\u2713 Comments generated: {len(image_files) - comment_errors}/{len(image_files)}\") print(f\"  Time: {int(elapsed//60)}m {int(elapsed%60)}s\") print(f\"  Errors: {comment_errors}\") In\u00a0[\u00a0]: Copied! <pre># Get sample comments\ncomment_files = list(handler.comments_dir.glob(\"*_comments.json\"))\nsample_comments = random.sample(comment_files, min(3, len(comment_files)))\n\nprint(\"Sample Comments:\")\nprint(\"=\" * 80)\n\nfor comment_file in sample_comments:\n    with open(comment_file, 'r') as f:\n        comment_data = json.load(f)\n    \n    print(f\"\\nImage: {comment_data['image_id']}\")\n    print(\"Comments:\")\n    for i, comment in enumerate(comment_data['comments'], 1):\n        print(f\"  {i}. {comment}\")\n    print(\"-\" * 80)\n</pre> # Get sample comments comment_files = list(handler.comments_dir.glob(\"*_comments.json\")) sample_comments = random.sample(comment_files, min(3, len(comment_files)))  print(\"Sample Comments:\") print(\"=\" * 80)  for comment_file in sample_comments:     with open(comment_file, 'r') as f:         comment_data = json.load(f)          print(f\"\\nImage: {comment_data['image_id']}\")     print(\"Comments:\")     for i, comment in enumerate(comment_data['comments'], 1):         print(f\"  {i}. {comment}\")     print(\"-\" * 80) In\u00a0[\u00a0]: Copied! <pre>print(\"Exporting CSV summaries...\\n\")\n\n# Export captions\ncaptions_csv = handler.export_captions_csv()\nif captions_csv:\n    print(f\"\u2713 Captions exported to: {captions_csv}\")\n\n# Export labels\nlabels_csv = handler.export_labels_csv()\nif labels_csv:\n    print(f\"\u2713 Labels exported to: {labels_csv}\")\n\n# Export comments\ncomments_csv = handler.export_comments_csv()\nif comments_csv:\n    print(f\"\u2713 Comments exported to: {comments_csv}\")\n</pre> print(\"Exporting CSV summaries...\\n\")  # Export captions captions_csv = handler.export_captions_csv() if captions_csv:     print(f\"\u2713 Captions exported to: {captions_csv}\")  # Export labels labels_csv = handler.export_labels_csv() if labels_csv:     print(f\"\u2713 Labels exported to: {labels_csv}\")  # Export comments comments_csv = handler.export_comments_csv() if comments_csv:     print(f\"\u2713 Comments exported to: {comments_csv}\") In\u00a0[\u00a0]: Copied! <pre>completeness = handler.check_completeness()\n\nprint(\"\\nDataset Completeness Check:\")\nprint(\"=\" * 80)\nprint(f\"Total images: {completeness['total_images']}\")\nprint(f\"\\nMetadata Coverage:\")\nprint(f\"  Captions: {completeness['images_with_captions']} ({completeness['caption_coverage']:.1f}%)\")\nprint(f\"  Labels: {completeness['images_with_labels']} ({completeness['label_coverage']:.1f}%)\")\nprint(f\"  Comments: {completeness['images_with_comments']} ({completeness['comment_coverage']:.1f}%)\")\n\nif completeness['fully_complete']:\n    print(\"\\n\u2713 Dataset is fully complete! All images have all metadata.\")\nelse:\n    print(\"\\n\u26a0 Some images are missing metadata. Check errors above.\")\n\nprint(\"=\" * 80)\n</pre> completeness = handler.check_completeness()  print(\"\\nDataset Completeness Check:\") print(\"=\" * 80) print(f\"Total images: {completeness['total_images']}\") print(f\"\\nMetadata Coverage:\") print(f\"  Captions: {completeness['images_with_captions']} ({completeness['caption_coverage']:.1f}%)\") print(f\"  Labels: {completeness['images_with_labels']} ({completeness['label_coverage']:.1f}%)\") print(f\"  Comments: {completeness['images_with_comments']} ({completeness['comment_coverage']:.1f}%)\")  if completeness['fully_complete']:     print(\"\\n\u2713 Dataset is fully complete! All images have all metadata.\") else:     print(\"\\n\u26a0 Some images are missing metadata. Check errors above.\")  print(\"=\" * 80)"},{"location":"notebooks/04_generate_metadata/#generate-metadata-captions-labels-comments","title":"Generate Metadata (Captions, Labels, Comments)\u00b6","text":"<p>This notebook generates captions, labels, and social media-style comments for all generated images.</p> <p>Workshop: AI/ML Pipeline - Synthetic Data Generation Date: January 23, 2026 Platform: CyVerse Jupyter Lab PyTorch GPU</p>"},{"location":"notebooks/04_generate_metadata/#what-this-notebook-does","title":"What This Notebook Does\u00b6","text":"<p>For each generated image:</p> <ol> <li>Generates descriptive captions</li> <li>Generates semantic and categorical labels</li> <li>Generates social media-style comments</li> <li>Saves all metadata with image associations</li> <li>Exports CSV summaries for analysis</li> </ol>"},{"location":"notebooks/04_generate_metadata/#setup-and-imports","title":"Setup and Imports\u00b6","text":""},{"location":"notebooks/04_generate_metadata/#1-load-configuration-and-find-images","title":"1. Load Configuration and Find Images\u00b6","text":""},{"location":"notebooks/04_generate_metadata/#2-initialize-text-generator","title":"2. Initialize Text Generator\u00b6","text":"<p>Set up Gemini API client for text generation.</p>"},{"location":"notebooks/04_generate_metadata/#3-get-metadata-configuration","title":"3. Get Metadata Configuration\u00b6","text":""},{"location":"notebooks/04_generate_metadata/#4-generate-captions","title":"4. Generate Captions\u00b6","text":"<p>Generate descriptive captions for all images.</p>"},{"location":"notebooks/04_generate_metadata/#preview-sample-captions","title":"Preview Sample Captions\u00b6","text":""},{"location":"notebooks/04_generate_metadata/#5-generate-labels","title":"5. Generate Labels\u00b6","text":"<p>Generate semantic and categorical labels for all images.</p>"},{"location":"notebooks/04_generate_metadata/#preview-sample-labels","title":"Preview Sample Labels\u00b6","text":""},{"location":"notebooks/04_generate_metadata/#6-generate-comments","title":"6. Generate Comments\u00b6","text":"<p>Generate social media-style comments for all images.</p>"},{"location":"notebooks/04_generate_metadata/#preview-sample-comments","title":"Preview Sample Comments\u00b6","text":""},{"location":"notebooks/04_generate_metadata/#7-export-csv-summaries","title":"7. Export CSV Summaries\u00b6","text":"<p>Export all metadata to CSV files for easy analysis.</p>"},{"location":"notebooks/04_generate_metadata/#8-check-completeness","title":"8. Check Completeness\u00b6","text":"<p>Verify all images have complete metadata.</p>"},{"location":"notebooks/04_generate_metadata/#summary","title":"Summary\u00b6","text":"<p>Metadata generation complete! Your dataset now includes:</p> <ul> <li>Descriptive captions for each image</li> <li>Semantic and categorical labels</li> <li>Social media-style comments</li> <li>CSV exports for easy analysis</li> </ul> <p>Next Step: Run notebook <code>05_quality_assurance.ipynb</code> to validate the complete dataset.</p>"},{"location":"notebooks/05_quality_assurance/","title":"5. Quality Assurance","text":"In\u00a0[\u00a0]: Copied! <pre>import sys\nfrom pathlib import Path\nimport json\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import display, HTML\nfrom collections import Counter\n\n# Add parent directory to path\nparent_dir = Path.cwd().parent\nif str(parent_dir) not in sys.path:\n    sys.path.insert(0, str(parent_dir))\n\nfrom src import config, output_handler, validation\n\nprint(\"\u2713 All modules imported successfully\")\n\n# Set plot style\nsns.set_style('whitegrid')\nplt.rcParams['figure.figsize'] = (12, 6)\n</pre> import sys from pathlib import Path import json import pandas as pd import matplotlib.pyplot as plt import seaborn as sns from IPython.display import display, HTML from collections import Counter  # Add parent directory to path parent_dir = Path.cwd().parent if str(parent_dir) not in sys.path:     sys.path.insert(0, str(parent_dir))  from src import config, output_handler, validation  print(\"\u2713 All modules imported successfully\")  # Set plot style sns.set_style('whitegrid') plt.rcParams['figure.figsize'] = (12, 6) In\u00a0[\u00a0]: Copied! <pre># Load configuration\ncfg = config.load_config()\n\n# Initialize output handler\noutput_dir = cfg.get_output_path()\nhandler = output_handler.OutputHandler(\n    output_dir=output_dir,\n    image_format=cfg.output['image_format']\n)\n\n# Get dataset summary\nsummary = handler.get_summary()\n\nprint(\"Dataset Summary:\")\nprint(\"=\" * 80)\nprint(f\"Output directory: {summary['output_directory']}\")\nprint(f\"Images: {summary['images_saved']}\")\nprint(f\"Captions: {summary['captions_saved']}\")\nprint(f\"Labels: {summary['labels_saved']}\")\nprint(f\"Comments: {summary['comments_saved']}\")\nprint(\"=\" * 80)\n</pre> # Load configuration cfg = config.load_config()  # Initialize output handler output_dir = cfg.get_output_path() handler = output_handler.OutputHandler(     output_dir=output_dir,     image_format=cfg.output['image_format'] )  # Get dataset summary summary = handler.get_summary()  print(\"Dataset Summary:\") print(\"=\" * 80) print(f\"Output directory: {summary['output_directory']}\") print(f\"Images: {summary['images_saved']}\") print(f\"Captions: {summary['captions_saved']}\") print(f\"Labels: {summary['labels_saved']}\") print(f\"Comments: {summary['comments_saved']}\") print(\"=\" * 80) In\u00a0[\u00a0]: Copied! <pre>print(\"Running automated validation...\\n\")\n\n# Initialize dataset validator\nvalidator = validation.DatasetValidator(\n    output_dir=output_dir,\n    min_image_size_kb=cfg.validation.get('min_image_size_kb', 10),\n    duplicate_threshold=cfg.validation.get('duplicate_threshold', 95)\n)\n\n# Run validation\nvalidation_report = validator.validate_dataset()\n\nprint(\"\u2713 Validation complete\")\n</pre> print(\"Running automated validation...\\n\")  # Initialize dataset validator validator = validation.DatasetValidator(     output_dir=output_dir,     min_image_size_kb=cfg.validation.get('min_image_size_kb', 10),     duplicate_threshold=cfg.validation.get('duplicate_threshold', 95) )  # Run validation validation_report = validator.validate_dataset()  print(\"\u2713 Validation complete\") In\u00a0[\u00a0]: Copied! <pre>summary_data = validation_report['summary']\n\nprint(\"Validation Summary:\")\nprint(\"=\" * 80)\nprint(f\"\\nTotal Images: {summary_data['total_images']}\")\nprint(f\"Valid Images: {summary_data['valid_images']}\")\nprint(f\"Validation Rate: {summary_data['validation_rate']:.1f}%\")\nprint(f\"\\nMetadata Completeness:\")\nprint(f\"  Metadata: {'\u2713 Complete' if summary_data['metadata_complete'] else '\u2717 Incomplete'}\")\nprint(f\"  Captions: {'\u2713 Complete' if summary_data['captions_complete'] else '\u2717 Incomplete'}\")\nprint(f\"  Labels: {'\u2713 Complete' if summary_data['labels_complete'] else '\u2717 Incomplete'}\")\nprint(f\"  Comments: {'\u2713 Complete' if summary_data['comments_complete'] else '\u2717 Incomplete'}\")\nprint(f\"\\nDuplicate Detection:\")\nprint(f\"  Duplicates Found: {summary_data['duplicates_found']}\")\nprint(f\"\\nOverall Quality: {summary_data['dataset_quality']}\")\nprint(\"=\" * 80)\n</pre> summary_data = validation_report['summary']  print(\"Validation Summary:\") print(\"=\" * 80) print(f\"\\nTotal Images: {summary_data['total_images']}\") print(f\"Valid Images: {summary_data['valid_images']}\") print(f\"Validation Rate: {summary_data['validation_rate']:.1f}%\") print(f\"\\nMetadata Completeness:\") print(f\"  Metadata: {'\u2713 Complete' if summary_data['metadata_complete'] else '\u2717 Incomplete'}\") print(f\"  Captions: {'\u2713 Complete' if summary_data['captions_complete'] else '\u2717 Incomplete'}\") print(f\"  Labels: {'\u2713 Complete' if summary_data['labels_complete'] else '\u2717 Incomplete'}\") print(f\"  Comments: {'\u2713 Complete' if summary_data['comments_complete'] else '\u2717 Incomplete'}\") print(f\"\\nDuplicate Detection:\") print(f\"  Duplicates Found: {summary_data['duplicates_found']}\") print(f\"\\nOverall Quality: {summary_data['dataset_quality']}\") print(\"=\" * 80) In\u00a0[\u00a0]: Copied! <pre>images_report = validation_report['images']\n\nprint(f\"\\nImage Validation Details:\")\nprint(f\"  Total: {images_report['total_images']}\")\nprint(f\"  Valid: {images_report['valid_images']}\")\nprint(f\"  Invalid: {images_report['invalid_images']}\")\nprint(f\"  Rate: {images_report['validation_rate']:.1f}%\")\n\n# Show any invalid images\nif images_report['invalid_images'] &gt; 0:\n    print(\"\\n\u26a0 Invalid Images:\")\n    for result in images_report['results']:\n        if not result['valid']:\n            print(f\"  - {Path(result['file_path']).name}: {', '.join(result['issues'])}\")\n</pre> images_report = validation_report['images']  print(f\"\\nImage Validation Details:\") print(f\"  Total: {images_report['total_images']}\") print(f\"  Valid: {images_report['valid_images']}\") print(f\"  Invalid: {images_report['invalid_images']}\") print(f\"  Rate: {images_report['validation_rate']:.1f}%\")  # Show any invalid images if images_report['invalid_images'] &gt; 0:     print(\"\\n\u26a0 Invalid Images:\")     for result in images_report['results']:         if not result['valid']:             print(f\"  - {Path(result['file_path']).name}: {', '.join(result['issues'])}\") In\u00a0[\u00a0]: Copied! <pre>duplicates = validation_report['duplicates']\n\nprint(f\"\\nDuplicate Detection:\")\nprint(f\"  Number of duplicate pairs: {duplicates['num_duplicates']}\")\n\nif duplicates['num_duplicates'] &gt; 0:\n    print(\"\\n\u26a0 Potential Duplicates:\")\n    for img1, img2, similarity in duplicates['duplicate_pairs'][:10]:  # Show first 10\n        print(f\"  - {Path(img1).name} \u2194 {Path(img2).name} (similarity: {similarity:.1f}%)\")\nelse:\n    print(\"\\n\u2713 No duplicates detected\")\n</pre> duplicates = validation_report['duplicates']  print(f\"\\nDuplicate Detection:\") print(f\"  Number of duplicate pairs: {duplicates['num_duplicates']}\")  if duplicates['num_duplicates'] &gt; 0:     print(\"\\n\u26a0 Potential Duplicates:\")     for img1, img2, similarity in duplicates['duplicate_pairs'][:10]:  # Show first 10         print(f\"  - {Path(img1).name} \u2194 {Path(img2).name} (similarity: {similarity:.1f}%)\") else:     print(\"\\n\u2713 No duplicates detected\") In\u00a0[\u00a0]: Copied! <pre># Load all labels\nlabels_csv = output_dir / \"all_labels.csv\"\n\nif labels_csv.exists():\n    labels_df = pd.read_csv(labels_csv)\n    \n    print(\"Label Distribution Analysis:\")\n    print(\"=\" * 80)\n    \n    # Analyze each label category\n    label_cols = [col for col in labels_df.columns if col != 'image_id']\n    \n    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n    fig.suptitle('Label Distribution by Category', fontsize=16)\n    \n    for idx, col in enumerate(label_cols[:6]):  # Plot first 6 categories\n        row = idx // 3\n        col_idx = idx % 3\n        \n        # Count values\n        value_counts = labels_df[col].value_counts()\n        \n        # Plot\n        value_counts.plot(kind='bar', ax=axes[row, col_idx])\n        axes[row, col_idx].set_title(col.replace('_', ' ').title())\n        axes[row, col_idx].set_xlabel('')\n        axes[row, col_idx].tick_params(axis='x', rotation=45)\n    \n    # Hide unused subplots\n    for idx in range(len(label_cols), 6):\n        row = idx // 3\n        col_idx = idx % 3\n        axes[row, col_idx].axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Print statistics\n    print(\"\\nLabel Statistics:\")\n    for col in label_cols:\n        print(f\"\\n{col.replace('_', ' ').title()}:\")\n        print(labels_df[col].value_counts().to_string())\nelse:\n    print(\"\u26a0 Labels CSV not found. Please run notebook 04 to generate metadata.\")\n</pre> # Load all labels labels_csv = output_dir / \"all_labels.csv\"  if labels_csv.exists():     labels_df = pd.read_csv(labels_csv)          print(\"Label Distribution Analysis:\")     print(\"=\" * 80)          # Analyze each label category     label_cols = [col for col in labels_df.columns if col != 'image_id']          fig, axes = plt.subplots(2, 3, figsize=(15, 10))     fig.suptitle('Label Distribution by Category', fontsize=16)          for idx, col in enumerate(label_cols[:6]):  # Plot first 6 categories         row = idx // 3         col_idx = idx % 3                  # Count values         value_counts = labels_df[col].value_counts()                  # Plot         value_counts.plot(kind='bar', ax=axes[row, col_idx])         axes[row, col_idx].set_title(col.replace('_', ' ').title())         axes[row, col_idx].set_xlabel('')         axes[row, col_idx].tick_params(axis='x', rotation=45)          # Hide unused subplots     for idx in range(len(label_cols), 6):         row = idx // 3         col_idx = idx % 3         axes[row, col_idx].axis('off')          plt.tight_layout()     plt.show()          # Print statistics     print(\"\\nLabel Statistics:\")     for col in label_cols:         print(f\"\\n{col.replace('_', ' ').title()}:\")         print(labels_df[col].value_counts().to_string()) else:     print(\"\u26a0 Labels CSV not found. Please run notebook 04 to generate metadata.\") In\u00a0[\u00a0]: Copied! <pre># Load all captions\ncaptions_csv = output_dir / \"all_captions.csv\"\n\nif captions_csv.exists():\n    captions_df = pd.read_csv(captions_csv)\n    \n    # Calculate caption lengths\n    captions_df['caption_length'] = captions_df['caption'].str.len()\n    captions_df['word_count'] = captions_df['caption'].str.split().str.len()\n    \n    print(\"Caption Statistics:\")\n    print(\"=\" * 80)\n    print(f\"Total captions: {len(captions_df)}\")\n    print(f\"\\nCharacter Length:\")\n    print(f\"  Mean: {captions_df['caption_length'].mean():.1f}\")\n    print(f\"  Median: {captions_df['caption_length'].median():.1f}\")\n    print(f\"  Min: {captions_df['caption_length'].min()}\")\n    print(f\"  Max: {captions_df['caption_length'].max()}\")\n    print(f\"\\nWord Count:\")\n    print(f\"  Mean: {captions_df['word_count'].mean():.1f}\")\n    print(f\"  Median: {captions_df['word_count'].median():.1f}\")\n    print(f\"  Min: {captions_df['word_count'].min()}\")\n    print(f\"  Max: {captions_df['word_count'].max()}\")\n    \n    # Plot distributions\n    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n    \n    captions_df['caption_length'].hist(bins=20, ax=axes[0])\n    axes[0].set_title('Caption Length Distribution (characters)')\n    axes[0].set_xlabel('Characters')\n    axes[0].set_ylabel('Frequency')\n    \n    captions_df['word_count'].hist(bins=20, ax=axes[1])\n    axes[1].set_title('Caption Word Count Distribution')\n    axes[1].set_xlabel('Words')\n    axes[1].set_ylabel('Frequency')\n    \n    plt.tight_layout()\n    plt.show()\nelse:\n    print(\"\u26a0 Captions CSV not found. Please run notebook 04 to generate metadata.\")\n</pre> # Load all captions captions_csv = output_dir / \"all_captions.csv\"  if captions_csv.exists():     captions_df = pd.read_csv(captions_csv)          # Calculate caption lengths     captions_df['caption_length'] = captions_df['caption'].str.len()     captions_df['word_count'] = captions_df['caption'].str.split().str.len()          print(\"Caption Statistics:\")     print(\"=\" * 80)     print(f\"Total captions: {len(captions_df)}\")     print(f\"\\nCharacter Length:\")     print(f\"  Mean: {captions_df['caption_length'].mean():.1f}\")     print(f\"  Median: {captions_df['caption_length'].median():.1f}\")     print(f\"  Min: {captions_df['caption_length'].min()}\")     print(f\"  Max: {captions_df['caption_length'].max()}\")     print(f\"\\nWord Count:\")     print(f\"  Mean: {captions_df['word_count'].mean():.1f}\")     print(f\"  Median: {captions_df['word_count'].median():.1f}\")     print(f\"  Min: {captions_df['word_count'].min()}\")     print(f\"  Max: {captions_df['word_count'].max()}\")          # Plot distributions     fig, axes = plt.subplots(1, 2, figsize=(15, 5))          captions_df['caption_length'].hist(bins=20, ax=axes[0])     axes[0].set_title('Caption Length Distribution (characters)')     axes[0].set_xlabel('Characters')     axes[0].set_ylabel('Frequency')          captions_df['word_count'].hist(bins=20, ax=axes[1])     axes[1].set_title('Caption Word Count Distribution')     axes[1].set_xlabel('Words')     axes[1].set_ylabel('Frequency')          plt.tight_layout()     plt.show() else:     print(\"\u26a0 Captions CSV not found. Please run notebook 04 to generate metadata.\") In\u00a0[\u00a0]: Copied! <pre>import random\nfrom PIL import Image\n\n# Get all images\nimage_files = list(handler.images_dir.glob(f\"*.{cfg.output['image_format']}\"))\n\n# Sample for review (5-10% or minimum 10 images)\nsample_size = max(10, int(len(image_files) * 0.05))\nsample_images = random.sample(image_files, min(sample_size, len(image_files)))\n\nprint(f\"Reviewing {len(sample_images)} randomly sampled images:\")\nprint(\"=\" * 80)\n\n# Create review interface\nfor idx, image_file in enumerate(sample_images, 1):\n    image_id = image_file.stem\n    \n    print(f\"\\n{'='*80}\")\n    print(f\"Review {idx}/{len(sample_images)}: {image_id}\")\n    print(f\"{'='*80}\")\n    \n    # Load and display image\n    img = Image.open(image_file)\n    display(img.resize((512, 512)))\n    \n    # Load metadata\n    metadata = handler.load_metadata(image_id)\n    if metadata:\n        print(f\"\\nOriginal Prompt (excerpt):\")\n        print(f\"  {metadata.get('prompt', 'N/A')[:200]}...\")\n        print(f\"\\nSource Theme: {metadata.get('source_data', {}).get('atropia', {}).get('theme', 'N/A')}\")\n    \n    # Load caption\n    caption_file = handler.captions_dir / f\"{image_id}_caption.json\"\n    if caption_file.exists():\n        with open(caption_file, 'r') as f:\n            caption_data = json.load(f)\n        print(f\"\\nGenerated Caption:\")\n        print(f\"  {caption_data['caption']}\")\n    \n    # Load labels\n    labels_file = handler.labels_dir / f\"{image_id}_labels.json\"\n    if labels_file.exists():\n        with open(labels_file, 'r') as f:\n            labels_data = json.load(f)\n        print(f\"\\nGenerated Labels:\")\n        for category, label in labels_data['labels'].items():\n            print(f\"  {category}: {label}\")\n    \n    # Load sample comments\n    comments_file = handler.comments_dir / f\"{image_id}_comments.json\"\n    if comments_file.exists():\n        with open(comments_file, 'r') as f:\n            comments_data = json.load(f)\n        print(f\"\\nSample Comments (first 3):\")\n        for i, comment in enumerate(comments_data['comments'][:3], 1):\n            print(f\"  {i}. {comment}\")\n    \n    print(f\"\\n{'='*80}\\n\")\n</pre> import random from PIL import Image  # Get all images image_files = list(handler.images_dir.glob(f\"*.{cfg.output['image_format']}\"))  # Sample for review (5-10% or minimum 10 images) sample_size = max(10, int(len(image_files) * 0.05)) sample_images = random.sample(image_files, min(sample_size, len(image_files)))  print(f\"Reviewing {len(sample_images)} randomly sampled images:\") print(\"=\" * 80)  # Create review interface for idx, image_file in enumerate(sample_images, 1):     image_id = image_file.stem          print(f\"\\n{'='*80}\")     print(f\"Review {idx}/{len(sample_images)}: {image_id}\")     print(f\"{'='*80}\")          # Load and display image     img = Image.open(image_file)     display(img.resize((512, 512)))          # Load metadata     metadata = handler.load_metadata(image_id)     if metadata:         print(f\"\\nOriginal Prompt (excerpt):\")         print(f\"  {metadata.get('prompt', 'N/A')[:200]}...\")         print(f\"\\nSource Theme: {metadata.get('source_data', {}).get('atropia', {}).get('theme', 'N/A')}\")          # Load caption     caption_file = handler.captions_dir / f\"{image_id}_caption.json\"     if caption_file.exists():         with open(caption_file, 'r') as f:             caption_data = json.load(f)         print(f\"\\nGenerated Caption:\")         print(f\"  {caption_data['caption']}\")          # Load labels     labels_file = handler.labels_dir / f\"{image_id}_labels.json\"     if labels_file.exists():         with open(labels_file, 'r') as f:             labels_data = json.load(f)         print(f\"\\nGenerated Labels:\")         for category, label in labels_data['labels'].items():             print(f\"  {category}: {label}\")          # Load sample comments     comments_file = handler.comments_dir / f\"{image_id}_comments.json\"     if comments_file.exists():         with open(comments_file, 'r') as f:             comments_data = json.load(f)         print(f\"\\nSample Comments (first 3):\")         for i, comment in enumerate(comments_data['comments'][:3], 1):             print(f\"  {i}. {comment}\")          print(f\"\\n{'='*80}\\n\") In\u00a0[\u00a0]: Copied! <pre># Save validation report\nreport_path = validator.save_report(validation_report)\n\nprint(f\"\u2713 QA report saved to: {report_path}\")\nprint(f\"\\nReport includes:\")\nprint(f\"  - Image validation results\")\nprint(f\"  - Metadata completeness checks\")\nprint(f\"  - Duplicate detection results\")\nprint(f\"  - Overall quality assessment\")\n</pre> # Save validation report report_path = validator.save_report(validation_report)  print(f\"\u2713 QA report saved to: {report_path}\") print(f\"\\nReport includes:\") print(f\"  - Image validation results\") print(f\"  - Metadata completeness checks\") print(f\"  - Duplicate detection results\") print(f\"  - Overall quality assessment\") In\u00a0[\u00a0]: Copied! <pre>print(\"\\n\" + \"=\" * 80)\nprint(\"QUALITY ASSURANCE COMPLETE\")\nprint(\"=\" * 80)\n\nquality = summary_data['dataset_quality']\nvalidation_rate = summary_data['validation_rate']\n\nprint(f\"\\nOverall Dataset Quality: {quality}\")\nprint(f\"Validation Rate: {validation_rate:.1f}%\")\n\nprint(\"\\nDataset is ready for:\")\nif quality in ['Excellent', 'Good']:\n    print(\"  \u2713 Model training and development\")\n    print(\"  \u2713 Semantic similarity analysis\")\n    print(\"  \u2713 Network structure analysis\")\n    print(\"  \u2713 Publication and sharing\")\nelse:\n    print(\"  \u26a0 Review and address issues before proceeding\")\n    print(\"  \u26a0 Consider regenerating problematic images\")\n\nprint(\"\\nNext Steps:\")\nprint(\"  1. Review QA report for detailed findings\")\nprint(\"  2. Address any issues identified\")\nprint(\"  3. Package dataset for Model Development phase\")\nprint(\"  4. Document generation methodology\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(f\"Dataset Location: {output_dir}\")\nprint(f\"QA Report: {report_path}\")\nprint(\"=\" * 80)\n</pre> print(\"\\n\" + \"=\" * 80) print(\"QUALITY ASSURANCE COMPLETE\") print(\"=\" * 80)  quality = summary_data['dataset_quality'] validation_rate = summary_data['validation_rate']  print(f\"\\nOverall Dataset Quality: {quality}\") print(f\"Validation Rate: {validation_rate:.1f}%\")  print(\"\\nDataset is ready for:\") if quality in ['Excellent', 'Good']:     print(\"  \u2713 Model training and development\")     print(\"  \u2713 Semantic similarity analysis\")     print(\"  \u2713 Network structure analysis\")     print(\"  \u2713 Publication and sharing\") else:     print(\"  \u26a0 Review and address issues before proceeding\")     print(\"  \u26a0 Consider regenerating problematic images\")  print(\"\\nNext Steps:\") print(\"  1. Review QA report for detailed findings\") print(\"  2. Address any issues identified\") print(\"  3. Package dataset for Model Development phase\") print(\"  4. Document generation methodology\")  print(\"\\n\" + \"=\" * 80) print(f\"Dataset Location: {output_dir}\") print(f\"QA Report: {report_path}\") print(\"=\" * 80)"},{"location":"notebooks/05_quality_assurance/#quality-assurance-and-validation","title":"Quality Assurance and Validation\u00b6","text":"<p>This notebook performs comprehensive quality assurance checks on the generated synthetic dataset.</p> <p>Workshop: AI/ML Pipeline - Synthetic Data Generation Date: January 23, 2026 Platform: CyVerse Jupyter Lab PyTorch GPU</p>"},{"location":"notebooks/05_quality_assurance/#qa-checklist","title":"QA Checklist\u00b6","text":"<ol> <li>Image Validation: Verify all images are valid and readable</li> <li>Metadata Completeness: Check all images have captions, labels, comments</li> <li>Duplicate Detection: Identify potentially duplicate images</li> <li>Label Distribution Analysis: Check for bias in labels</li> <li>Human Review Interface: Sample random images for manual inspection</li> <li>QA Report Generation: Create comprehensive quality report</li> </ol>"},{"location":"notebooks/05_quality_assurance/#setup-and-imports","title":"Setup and Imports\u00b6","text":""},{"location":"notebooks/05_quality_assurance/#1-load-configuration-and-dataset","title":"1. Load Configuration and Dataset\u00b6","text":""},{"location":"notebooks/05_quality_assurance/#2-automated-validation","title":"2. Automated Validation\u00b6","text":"<p>Run comprehensive automated validation checks.</p>"},{"location":"notebooks/05_quality_assurance/#validation-results-summary","title":"Validation Results Summary\u00b6","text":""},{"location":"notebooks/05_quality_assurance/#image-validation-details","title":"Image Validation Details\u00b6","text":""},{"location":"notebooks/05_quality_assurance/#duplicate-detection","title":"Duplicate Detection\u00b6","text":""},{"location":"notebooks/05_quality_assurance/#3-statistical-analysis","title":"3. Statistical Analysis\u00b6","text":"<p>Analyze label distribution and caption characteristics.</p>"},{"location":"notebooks/05_quality_assurance/#label-distribution-analysis","title":"Label Distribution Analysis\u00b6","text":""},{"location":"notebooks/05_quality_assurance/#caption-length-analysis","title":"Caption Length Analysis\u00b6","text":""},{"location":"notebooks/05_quality_assurance/#4-human-review-interface","title":"4. Human Review Interface\u00b6","text":"<p>Random sample of images for manual inspection.</p>"},{"location":"notebooks/05_quality_assurance/#manual-review-checklist","title":"Manual Review Checklist\u00b6","text":"<p>For each image reviewed above, consider:</p> <ul> <li>\u2713 Image quality and clarity</li> <li>\u2713 Relevance to social movement theme</li> <li>\u2713 Caption accuracy and descriptiveness</li> <li>\u2713 Label appropriateness</li> <li>\u2713 Comment realism and diversity</li> </ul> <p>Note any issues below:</p> <p>(Space for notes)</p>"},{"location":"notebooks/05_quality_assurance/#5-save-qa-report","title":"5. Save QA Report\u00b6","text":"<p>Save comprehensive quality assurance report.</p>"},{"location":"notebooks/05_quality_assurance/#6-final-summary-and-recommendations","title":"6. Final Summary and Recommendations\u00b6","text":""},{"location":"notebooks/05_quality_assurance/#workshop-complete","title":"Workshop Complete!\u00b6","text":"<p>Congratulations! You have successfully:</p> <ol> <li>\u2713 Set up and tested the environment</li> <li>\u2713 Prepared source data from three sources</li> <li>\u2713 Generated synthetic social movement images</li> <li>\u2713 Created captions, labels, and comments</li> <li>\u2713 Validated and assessed dataset quality</li> </ol> <p>Your synthetic dataset is ready for the Model Development phase of the workshop!</p>"},{"location":"notebooks/05_quality_assurance/#resources","title":"Resources\u00b6","text":"<ul> <li>Dataset: <code>data/generated/</code></li> <li>QA Report: <code>data/qa/</code></li> <li>Documentation: <code>README.md</code> and <code>CLAUDE.md</code></li> <li>Support: CyVerse and workshop instructors</li> </ul>"}]}