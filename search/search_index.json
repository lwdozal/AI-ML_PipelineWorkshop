{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"AI/ML Pipeline Workshop","text":""},{"location":"#synthetic-data-generation-multimodal-llm-evaluation-and-network-analysis","title":"Synthetic Data Generation, Multimodal LLM Evaluation, and Network Analysis","text":"<p>Workshop Date: January 23, 2026 Funded by: Jetstream2 Platform: CyVerse Discovery Environment</p>"},{"location":"#about-this-workshop","title":"About This Workshop","text":"<p>Welcome to the AI Workbench for Synthetic Data Generation, Generative M-LLM Comparison, and Network Building! This hands-on workshop teaches you how to build an AI/MLOps pipeline that:</p> <ol> <li>Generates synthetic images using Google Gemini API</li> <li>Uses Multimodal Large Language Models (M-LLMs) to generate labels and captions</li> <li>Performs semantic evaluation of generated content</li> <li>Builds network structures to understand thematic representations</li> </ol> <p>This workshop focuses on a social movement case study, using synthetic data generation to address data privacy concerns while maintaining research value.</p>"},{"location":"#workshop-registration","title":"Workshop Registration","text":"<p>Enroll in the Workshop to access the CyVerse Discovery Environment and all workshop resources.</p>"},{"location":"#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this workshop, you will understand:</p> <ol> <li>AI/MLOps pipeline creation using Open Source CARE principles</li> <li>Synthetic data generation techniques</li> <li>Multimodal Large Language Models (M-LLMs)</li> <li>Centrality measures for network analysis</li> <li>Human-in-the-Loop evaluation methods</li> </ol>"},{"location":"#workshop-structure","title":"Workshop Structure","text":"<p>The workshop follows three main stages:</p>"},{"location":"#1-data-collection-and-evaluation","title":"1. Data Collection and Evaluation","text":"<p>Generate synthetic images based on a fictional scenario combining: - Atropia data (fictional country news from U.S. military training) - World Bank synthetic data for an imaginary country - Public social movement images as visual references</p>"},{"location":"#2-model-development","title":"2. Model Development","text":"<ul> <li>Train and fine-tune models on data subsets</li> <li>Generate image labels and captions using M-LLMs</li> <li>Evaluate model performance using multiple metrics</li> <li>Process text data (lemmatization, emoji translation, hashtag normalization)</li> </ul>"},{"location":"#3-model-deployment","title":"3. Model Deployment","text":"<ul> <li>Build semantic similarity networks</li> <li>Perform community detection analysis</li> <li>Visualize thematic patterns and relationships</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<ol> <li>Create a CyVerse Account</li> <li>Set up your environment</li> <li>Follow the workflow tutorials</li> </ol>"},{"location":"#additional-resources","title":"Additional Resources","text":"<ul> <li>CyVerse Learning Center</li> <li>Workshop Registration</li> <li>Project Repository</li> </ul>"},{"location":"#about-the-author","title":"About the Author","text":"<p>Laura Dozal PhD Candidate, College of Information University of Arizona</p>"},{"location":"#support-and-acknowledgments","title":"Support and Acknowledgments","text":"<p>This workshop is supported by: - Jetstream2 - CyVerse - The Data Science Institute at the University of Arizona</p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the GNU General Public License v3.0.</p> <p>Note: This workshop uses synthetic data due to Instagram privacy policies. Original social movement data is not shared to protect user privacy while maintaining educational value.</p>"},{"location":"Data_Management_Plan/","title":"Data Management Plan","text":"<p>This document takes into consideration data that would be collected and made available for dplPy and dendrochronologist to use.</p>"},{"location":"Data_Management_Plan/#1-data-description","title":"1. Data Description","text":"<p>We will collect tree ring measurements from various tree species in our study area. Data will include tree ring width measurements, corresponding years, and species information. These measurements will be stored in structured CSV and RWL files, with metadata indicating sample location, collection date, and field methods.</p>"},{"location":"Data_Management_Plan/#2-data-collection","title":"2. Data Collection","text":"<p>Tree cores will be extracted from trees using increment borers. Ring width measurements will be taken using specialized equipment. Species identification will be done based on field observations and confirmed through lab analysis.</p>"},{"location":"Data_Management_Plan/#3-data-storage-and-backup","title":"3. Data Storage and Backup","text":"<p>Tree ring data and associated metadata will be stored in a dedicated directory on our secure research server. Automated daily backups will be conducted to ensure data integrity.</p>"},{"location":"Data_Management_Plan/#4-data-organization-and-documentation","title":"4. Data Organization and Documentation","text":"<p>Data will be organized by sample location, species, and collection year. Each dataset will have an accompanying metadata file detailing data structure, variable definitions, and data processing steps. </p> <p>(These metadata files are obtainable when downloading data through the NOAA tree ring archive.)</p>"},{"location":"Data_Management_Plan/#5-data-preservation-and-long-term-access","title":"5. Data Preservation and Long-Term Access","text":"<p>Upon project completion, the dataset will be deposited in a recognized tree ring database or repository, ensuring long-term preservation and discoverability (NOAA). The repository will assign a Digital Object Identifier (DOI) for reliable referencing.</p>"},{"location":"Data_Management_Plan/#6-data-sharing-and-access","title":"6. Data Sharing and Access","text":"<p>After a brief embargo period to allow for primary analysis, the dataset will be openly accessible through the repository under a Creative Commons Attribution (CC-BY) license, promoting data reuse with proper attribution.</p>"},{"location":"Data_Management_Plan/#7-data-ethics-and-legal-compliance","title":"7. Data Ethics and Legal Compliance","text":"<p>Ethics approval has been obtained for the study. All data collection follows ethical guidelines, and any potential impact on tree populations or ecosystems will be minimized.</p>"},{"location":"Data_Management_Plan/#8-roles-and-responsibilities","title":"8. Roles and Responsibilities","text":"<p>The lead dendrochronologist will oversee data management, ensuring accuracy and quality. Field technicians will collect core samples and measurements. The institution's data management team will facilitate repository submission.</p>"},{"location":"Data_Management_Plan/#9-budget-and-resources","title":"9. Budget and Resources","text":"<p>Funding has been allocated for specialized dendrochronology equipment, data storage, repository fees, and data management software.</p> <p>This Data Management Plan ensures responsible collection, storage, and sharing of tree ring data, in adherence to dendrochronological best practices and ethical standards. It promotes the transparency, reproducibility, and accessibility of tree ring research.</p>"},{"location":"Governance_Operations/","title":"dplPy-for-FOSS Governance and Operations Document","text":"<p>This is a living document. Changes are expected throughout the life of the project.</p>"},{"location":"Governance_Operations/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction<ul> <li>Project Overview and Objectives<ul> <li>dplPy</li> <li>FOSS Reference Hub</li> </ul> </li> <li>Our Team<ul> <li>Organizational Structure</li> </ul> </li> </ul> </li> <li>Operations<ul> <li>Communications</li> <li>Procedures</li> </ul> </li> <li>Community Practices<ul> <li>Open Science Commitment</li> <li>Land Acknowledgement</li> <li>Diversity Statement</li> <li>Code of Conduct</li> </ul> </li> <li>Attribution, Authorship, and Ownership</li> </ol>"},{"location":"Governance_Operations/#introduction","title":"Introduction","text":"<p>This Project Governance document defines operating procedures and rules aimed to clarify and support the involvement of dplPy in FOSS and the FOSS reference hub.</p>"},{"location":"Governance_Operations/#project-overview-and-objectives","title":"Project Overview and Objectives","text":"<p>This section highights the dplPy project on its own and its implementation in the FOSS Reference Hub.</p>"},{"location":"Governance_Operations/#dplpy","title":"dplPy","text":"<p>dplPy is the Python implementation of the OpenDendro project, an open-source framework that aims to provide dendrochronologists with the required analytical software necessary for tree-ring related research. The majority of the code for dplPy was developed by Ifeoluwa Ale, a student Computer Scienctist at the University of Arizona, and supervised by Michele Cosi, Dr. Tyson Swetnam, and Dr. Kevin Anchukaitis. OpenDendro is supervised by Dr. Kevin Anchukaitis, Andy Bunn, Dr. Tyson Swetnam and Dr. Edward Cook.</p>"},{"location":"Governance_Operations/#foss-reference-hub","title":"FOSS Reference Hub","text":"<p>The FOSS reference hub is a reference repository for the Foundational Open Science Skills (FOSS) workshop series. Together with dplPy, the aim of this repository is to serve as a reference for the capstone, which FOSS attendees are expected to complete within the duration of the FOSS workshop.</p> <p>The dplPy project was selected to enhance the FOSS Reference Hub with a reliable codebase, ensuring it mirrors a genuine repository that participants can use as a model when attempting to replicate similar setups for their individual projects.</p> <p>The goals of the FOSS Reference Hub are to: - Serve as an exemplary reference for a well structured research object. - Guide FOSS attendees through weekly exercises designed to progressively enhance their project repositories.</p>"},{"location":"Governance_Operations/#our-team","title":"Our Team","text":"<p>Meet the education team at CyVerse/Data Science Institute:</p> <ul> <li>Tyson Swetnam: Co-PI of CyVerse and Director of Open Science, Institute for Computation and Data-enabled Insight (ICDI) at the University of Arizona</li> <li>Jeff Gillan: Data Scientist for CyVerse with relative research in the field of GIS/remote sensing.</li> <li>Carlos Lizarraga: Educator in Computational and Data Science at the University of Arizona Data Science Institute. Carlos' background is in applied research scientist as a Professor from the Physics Department at the University of Sonora, where he taught Computational Physics.</li> <li>Michele Cosi: Science Analyst for CyVerse with a background in genetics and genomics of rice.</li> <li>Tina Lee: Head of User Engagement at CyVerse.</li> </ul> <p>With the collaboration of the dplPy team: </p> <ul> <li>Andy Bunn: Environmental scientist interested in climate and energy from the College of Environment at Western Washington University, founding director of the Institute for Energy Studies, and main writer of OpenDendro software dplR and xDateR. </li> <li>Edward Cook: expert in the dendrochronology field and main writer of the tool ARSTAN. </li> <li>Kevin Anchukaitis: Climate scientist, paleoclimatologist, dendrochronologist, faculty member  in Geosciences and the Laboratory of Tree-Ring Research at the University of Arizona, leading the efforts behind the OpenDendro platform.</li> <li>Ifeuwa Ale: Junior Computer Science student at the University of Arizona and main contributor to the dplPy codebase for OpenDendro.</li> </ul> <p>FOSS will also be hosting the following guest speakers:</p> <ul> <li>Jason Williams: Assistant Director, Diversity and Research Readiness at the  DNA Learning Center of Cold Spring Harbor.</li> <li>Wade Bishop: Professor in the School of Information Sciences at the University of Tennessee-Knoxville. He is the Director of Graduate Studies as well as the Research Data Management Certificate Coordinator</li> </ul>"},{"location":"Governance_Operations/#organizational-structure","title":"Organizational Structure","text":"<pre><code>FOSS\n\u251c\u2500\u2500 CyVerse/Data Science Institute\n\u2502   \u251c\u2500\u2500 Tyson Swetnam \n\u2502   \u251c\u2500\u2500 Jeff Gillan\n\u2502   \u251c\u2500\u2500 Carlos Lizarraga\n\u2502   \u251c\u2500\u2500 Tina Lee\n\u2502   \u2514\u2500\u2500 Michele Cosi\n\u251c\u2500\u2500 FOSS Reference Hub\n\u2502   \u2514\u2500\u2500 dplPy\n\u2502        \u251c\u2500\u2500 Kevin Anchukaitis\n\u2502        \u251c\u2500\u2500 Edward Cook\n\u2502        \u251c\u2500\u2500 Andy Bunn\n\u2502        \u2514\u2500\u2500 Ifeuwa Ale\n\u2514\u2500\u2500 Guest Speakers\n    \u251c\u2500\u2500 Jason Williams\n    \u2514\u2500\u2500 Wade Bishop\n</code></pre>"},{"location":"Governance_Operations/#operations","title":"Operations","text":"<p>In order to ensure well distributed work loads across FOSS and the FOSS teaching materials including the FOSS Reference Hub, we have outlined communications and procedure components in order to maximize effectiveness and efficiency.</p>"},{"location":"Governance_Operations/#communications","title":"Communications","text":"<p>Internal communications: communications within the CyVerse/Data Science Institute team is carried out through private Slack channels (CyVerse, CyVerseLearning, Data7) and Zoom meetings.</p> <p>External communications: communications to the dplPy team is carried out through a private Slack channel (openDendro), whilst communication to the guest speakers is done through email (Wade Bishop) and the CyVerse/CyverseLearning Slack Channel (Jason Williams).</p> <p>Note keeping &amp; attendees: communications to FOSS attendees is achieved through the CyVerseLearning Slack channel. This will allow attendees to share thoughts and notes on lectures and materials during and after the workshop. Note keeping during the workshop is carried out through HackMD, which links are shared during the workshop sessions. All workshops will be done through Zoom, creating a virtual classroom environment allowing remote attendees to take part to the workshop. Recordings of these workshop sessions will be made available to attendees through a private YouTube playlist. </p>"},{"location":"Governance_Operations/#procedures","title":"Procedures","text":"<p>All of the code base required for FOSS, the FOSS Reference Hub and the dplPy project is stored in their respective GitHub repositories:</p> <ul> <li>FOSS</li> <li>FOSS Reference Hub</li> <li>dplPy </li> </ul> <p>All of these repositories can only be modified by the CyVerse/Data Science Institute team (or the dplPy team regarding the dplPy repository) or through a reviewed pull request. Websites for FOSS (foss.cyverse.org) and OpenDendro (opendendro.org) are created using MkDocs-Material and hosted on GitHub Pages, built through automated GitHub Actions.</p>"},{"location":"Governance_Operations/#community-practices","title":"Community Practices","text":""},{"location":"Governance_Operations/#open-science-commitment","title":"Open Science Commitment","text":"<p>We are committed to Open Science as we operate through transparency, collaboration, and accessibility, guided by theFAIR (Findable, Accessible, Interoperable, Reusable) and CARE (Collective Benefit, Accountability, Responsibility, Equity) principles.</p> <p>Our educational materials and publications are thoughtfully curated to align with Open Science ideals. We seek to empower others to adopt and implement Open Science practices, making research more accessible and reproducible.</p>"},{"location":"Governance_Operations/#land-acknowledgement","title":"Land Acknowledgement","text":"<p>As part of the University of Arizona, we acknowledge and respect the land and territories on which we are able to perform our duties as educators with the following statement: We respectfully acknowledge the University of Arizona is on the land and territories of Indigenous peoples. Today, Arizona is home to 22 federally recognized tribes, with Tucson being home to the O\u2019odham and the Yaqui. Committed to diversity and inclusion, the University strives to build sustainable relationships with sovereign Native Nations and Indigenous communities through education offerings, partnerships, and community service.</p>"},{"location":"Governance_Operations/#diversity-statement","title":"Diversity Statement","text":"<p>We encourage everyone to participate and are committed to building a community for all.  Although we will fail attimes, we seek to treat everyone as fairly and equally as possible. Whenever a participant has made a mistake, we expect them to take responsibility for it. If someone has been harmed or offended, if you are a witness to an event of this nature, it is your responsibility to both listen carefully and respectfully to the victims, as well as to take action to bring attention to the behaviour, and to make every effort to stop the behaviour. </p> <p>It is also our responsibility to do our best to right the wrong.  Although this list cannot be exhaustive, we explicitly honor diversity in age, gender, gender identity or expression, culture, ethnicity, language, national origin, political beliefs, profession, race, religion, sexual orientation, socioeconomic status, and technical ability. We will not tolerate discrimination based on any of the protected characteristics above, including participants with disabilities.</p>"},{"location":"Governance_Operations/#code-of-conduct","title":"Code of Conduct","text":"<p>In conjunction with for using CyVerse cyberinfrastructure, this Code of Conduct applies to all Event participants and their activities while using CyVerse resources and/or attending the Event.</p> <p>CyVerse is dedicated to providing professional computational research and educational experiences for all of our users, regardless of domain focus, academic status, educational level, gender/gender identity/expression, age, sexual orientation, mental or physical ability, physical appearance, body size, race, ethnicity, religion (or lack thereof), technology choices, dietary preferences, or any other personal characteristic.</p> <p>When using CyVerse or participating at an Event, we expect you to:</p> <p>Interact with others and use CyVerse professionally and ethically by complying with our Policies. Constructively critize ideas and processes, not people. Follow the Golden Rule (treat others as you want to be treated) when interacting online or in-person with collaborators, trainers, and support staff. Comply with this Code in spirit as much as the letter, as it is neither exhaustive nor complete in identifying any and all possible unacceptable conduct. We do not tolerate harassment of other users or staff in any form (including, but not limited to, violent threats or language, derogatory language or jokes, doxing, insults, advocating for or encouraging any of these behaviors). Sexual language and imagery are not appropriate at any time (excludes Protected Health Information in compliance with HIPAA). Any user violating this Code may be expelled from the platform and the workshop at CyVerse's sole discretion without warning.</p> <p>To report a violation of this Code, directly message a trainer via Slack or email info@cyverse.org with the following information:</p> <p>Your contact information Names (real, username, pseudonyms) of any individuals involved, and or witness(es) if any. Your account of what occurred and if the incident is ongoing. If there is a publicly available record (a tweet, public chat log, etc.), please include a link or attachment. Any additional information that may be helpful in resolving the issue.</p>"},{"location":"Governance_Operations/#attribution-authorship-and-ownership","title":"Attribution, Authorship, and Ownership","text":"<p>FOSS teaching materials and FOSS Reference Hub are made available under the CC BY 4.0 License: https://creativecommons.org/licenses/by/4.0/legalcode.</p> <p>dplPy and OpenDendro are made available under the GNU General Public Licence v3.0: https://creativecommons.org/licenses/by/4.0/legalcode</p> <p>We kindly ask to respect the Licences by which this material is developed.</p>"},{"location":"cyverse_account/","title":"Creating a CyVerse Account","text":"<p>CyVerse provides powerful cyberinfrastructure for data-driven discovery. To participate in this workshop, you'll need a CyVerse account to access the Discovery Environment where we'll run our AI/ML pipeline.</p>"},{"location":"cyverse_account/#what-is-cyverse","title":"What is CyVerse?","text":"<p>CyVerse is a cyberinfrastructure platform funded by the National Science Foundation that provides:</p> <ul> <li>Computational resources for data analysis</li> <li>Cloud storage for research data</li> <li>Pre-configured analysis environments</li> <li>Access to high-performance computing through partnerships with Jetstream2</li> <li>Educational resources and training materials</li> </ul>"},{"location":"cyverse_account/#account-creation-steps","title":"Account Creation Steps","text":""},{"location":"cyverse_account/#1-navigate-to-cyverse-user-portal","title":"1. Navigate to CyVerse User Portal","text":"<p>Visit https://user.cyverse.org</p>"},{"location":"cyverse_account/#2-click-sign-up","title":"2. Click \"Sign Up\"","text":"<p>Look for the registration or sign-up button on the homepage.</p>"},{"location":"cyverse_account/#3-complete-the-registration-form","title":"3. Complete the Registration Form","text":"<p>You'll need to provide:</p> <ul> <li>Username: Choose a unique username (alphanumeric characters and underscores)</li> <li>Email address: Use a valid email you can access</li> <li>Password: Create a strong password</li> <li>First and Last Name: Your full name</li> <li>Institution: Your affiliated organization or institution</li> <li>Optional information: Research interests, department, etc.</li> </ul>"},{"location":"cyverse_account/#4-verify-your-email","title":"4. Verify Your Email","text":"<p>After submitting the form:</p> <ol> <li>Check your email inbox for a verification message from CyVerse</li> <li>Click the verification link in the email</li> <li>Your account will be activated</li> </ol>"},{"location":"cyverse_account/#5-complete-your-profile","title":"5. Complete Your Profile","text":"<p>Once verified, log in and complete your user profile:</p> <ul> <li>Add additional contact information</li> <li>Describe your research interests</li> <li>Accept the terms of service</li> </ul>"},{"location":"cyverse_account/#workshop-registration","title":"Workshop Registration","text":"<p>After creating your CyVerse account, you must register for the workshop:</p> <p>Register for the AI/ML Pipeline Workshop</p> <p>Workshop registration ensures you have:</p> <ul> <li>Access to workshop-specific resources</li> <li>Allocated compute time on Jetstream2</li> <li>Workshop materials and datasets</li> <li>Support during the workshop</li> </ul>"},{"location":"cyverse_account/#accessing-the-discovery-environment","title":"Accessing the Discovery Environment","text":"<p>Once your account is created and you're registered for the workshop:</p> <ol> <li>Navigate to https://de.cyverse.org</li> <li>Log in with your CyVerse credentials</li> <li>You'll see the Discovery Environment dashboard</li> </ol>"},{"location":"cyverse_account/#launch-jupyter-lab","title":"Launch Jupyter Lab","text":"<p>For this workshop, you'll use the Jupyter Lab PyTorch GPU environment:</p> <ol> <li>Click on \"Apps\" in the left sidebar</li> <li>Look for \"Instant Launches\" section</li> <li>Find and click \"Jupyter Lab PyTorch GPU\"</li> <li>Wait for the environment to load (this may take a few minutes)</li> </ol>"},{"location":"cyverse_account/#troubleshooting","title":"Troubleshooting","text":""},{"location":"cyverse_account/#email-verification-issues","title":"Email Verification Issues","text":"<p>If you don't receive the verification email:</p> <ul> <li>Check your spam/junk folder</li> <li>Make sure you entered the correct email address</li> <li>Request a new verification email through the user portal</li> <li>Contact CyVerse support at support@cyverse.org</li> </ul>"},{"location":"cyverse_account/#login-problems","title":"Login Problems","text":"<p>If you can't log in:</p> <ul> <li>Verify your username and password</li> <li>Try resetting your password</li> <li>Clear your browser cache and cookies</li> <li>Contact support if problems persist</li> </ul>"},{"location":"cyverse_account/#workshop-access-issues","title":"Workshop Access Issues","text":"<p>If you can't access workshop resources:</p> <ul> <li>Confirm you've registered for the workshop at the registration link</li> <li>Allow up to 24 hours for registration processing</li> <li>Contact the workshop organizer or CyVerse support</li> </ul>"},{"location":"cyverse_account/#additional-resources","title":"Additional Resources","text":"<ul> <li>CyVerse Learning Center: Tutorials and documentation</li> <li>CyVerse User Manual: Comprehensive guides</li> <li>CyVerse Support: Technical assistance</li> <li>CyVerse Status: Check system status</li> </ul>"},{"location":"cyverse_account/#getting-help","title":"Getting Help","text":"<p>If you need assistance:</p> <ol> <li>During the workshop: Ask the instructor or teaching assistants</li> <li>Email support: support@cyverse.org</li> <li>Documentation: Check the CyVerse Learning Center</li> <li>Community: Join the CyVerse community forums</li> </ol>"},{"location":"cyverse_account/#data-privacy-and-security","title":"Data Privacy and Security","text":"<p>CyVerse takes data privacy seriously:</p> <ul> <li>Your data is stored securely</li> <li>You control who has access to your files</li> <li>Read the CyVerse Data Policy for details</li> <li>Follow best practices for sensitive data handling</li> </ul> <p>Once you've created your account and registered for the workshop, proceed to the Setup Guide to configure your environment.</p>"},{"location":"data_collection/","title":"Data Collection and Synthetic Data Generation","text":"<p>The first stage of our AI/ML pipeline focuses on generating synthetic image data using Google Gemini API. This approach allows us to create a realistic dataset while respecting privacy concerns.</p>"},{"location":"data_collection/#overview","title":"Overview","text":"<p>In this module, you'll learn how to:</p> <ul> <li>Generate synthetic images based on a fictional scenario</li> <li>Create descriptive prompts for image generation</li> <li>Evaluate synthetic data quality</li> <li>Prepare datasets for downstream ML tasks</li> </ul>"},{"location":"data_collection/#why-synthetic-data","title":"Why Synthetic Data?","text":"<p>This workshop uses synthetic data for several important reasons:</p> <ol> <li>Privacy Protection: Original social movement data from Instagram cannot be shared due to privacy policies</li> <li>Ethical Research: Synthetic data allows teaching without exposing real individuals</li> <li>Controlled Scenarios: We can create specific scenarios for educational purposes</li> <li>Reproducibility: Everyone generates similar datasets following the same process</li> </ol>"},{"location":"data_collection/#data-sources","title":"Data Sources","text":"<p>Our synthetic scenario combines three data sources:</p>"},{"location":"data_collection/#1-atropia-data","title":"1. Atropia Data","text":"<p>Fictional country news and events from U.S. military training exercises. Atropia is an imaginary country used for military simulations, providing realistic geopolitical scenarios.</p>"},{"location":"data_collection/#2-world-bank-synthetic-data","title":"2. World Bank Synthetic Data","text":"<p>Economic and demographic data for an imaginary country, providing context for social conditions and potential triggers for social movements.</p>"},{"location":"data_collection/#3-public-social-movement-images","title":"3. Public Social Movement Images","text":"<p>Real public images from various social movements serve as visual references for the types of scenes, compositions, and elements to include in synthetic images.</p>"},{"location":"data_collection/#workflow","title":"Workflow","text":""},{"location":"data_collection/#step-1-set-up-data-collection-environment","title":"Step 1: Set Up Data Collection Environment","text":"<p>Navigate to the DataCollection folder:</p> <pre><code>cd /home/jovyan/data-store/AI-ML_PipelineWorkshop/DataCollection\n</code></pre> <p>Ensure your virtual environment is activated and Google Gemini API key is configured (see Setup Guide).</p>"},{"location":"data_collection/#step-2-open-the-generation-notebook","title":"Step 2: Open the Generation Notebook","text":"<p>Open <code>Generate_images.ipynb</code> in Jupyter Lab:</p> <ol> <li>In the file browser, navigate to <code>DataCollection/</code></li> <li>Double-click <code>Generate_images.ipynb</code></li> <li>Select your kernel (workshop_env)</li> </ol>"},{"location":"data_collection/#step-3-understand-the-generation-process","title":"Step 3: Understand the Generation Process","text":"<p>The notebook guides you through:</p>"},{"location":"data_collection/#a-scenario-development","title":"A. Scenario Development","text":"<ul> <li>Review the fictional Atropia scenario</li> <li>Understand the social movement context</li> <li>Identify key visual elements to generate</li> </ul>"},{"location":"data_collection/#b-prompt-engineering","title":"B. Prompt Engineering","text":"<p>Learn to craft effective prompts for image generation:</p> <ul> <li>Specificity: Detailed descriptions produce better results</li> <li>Context: Include setting, atmosphere, and mood</li> <li>Visual elements: Specify people, objects, actions, and composition</li> <li>Style considerations: Artistic style, lighting, perspective</li> </ul> <p>Example prompt structure: <pre><code>A [setting] with [people/objects] [performing action].\n[Lighting/atmosphere]. [Composition details].\nStyle: [photorealistic/artistic style].\n</code></pre></p>"},{"location":"data_collection/#c-image-generation","title":"C. Image Generation","text":"<p>Use Google Gemini API to generate images:</p> <pre><code>import google.generativeai as genai\nimport os\nfrom dotenv import load_dotenv\n\n# Load API key\nload_dotenv()\ngenai.configure(api_key=os.getenv('GOOGLE_API_KEY'))\n\n# Configure model\nmodel = genai.GenerativeModel('gemini-pro-vision')\n\n# Generate image\nprompt = \"Your detailed prompt here\"\nresponse = model.generate_content(prompt)\n\n# Save image\nwith open('output_image.png', 'wb') as f:\n    f.write(response.image)\n</code></pre>"},{"location":"data_collection/#d-batch-generation","title":"D. Batch Generation","text":"<p>Generate multiple images with variation:</p> <ul> <li>Create prompt templates</li> <li>Add variation parameters</li> <li>Generate image sets</li> <li>Organize output files</li> </ul>"},{"location":"data_collection/#step-4-quality-assessment","title":"Step 4: Quality Assessment","text":"<p>Evaluate generated images using these criteria:</p>"},{"location":"data_collection/#visual-quality","title":"Visual Quality","text":"<ul> <li>Resolution and clarity</li> <li>Realistic appearance</li> <li>Appropriate composition</li> <li>Consistent style</li> </ul>"},{"location":"data_collection/#content-relevance","title":"Content Relevance","text":"<ul> <li>Matches scenario description</li> <li>Includes specified elements</li> <li>Appropriate context and setting</li> <li>Culturally appropriate</li> </ul>"},{"location":"data_collection/#dataset-balance","title":"Dataset Balance","text":"<ul> <li>Variety of scenes and scenarios</li> <li>Different perspectives and compositions</li> <li>Range of lighting conditions</li> <li>Diverse representation</li> </ul>"},{"location":"data_collection/#step-5-data-organization","title":"Step 5: Data Organization","text":"<p>Organize your generated dataset:</p> <pre><code>DataCollection/\n\u251c\u2500\u2500 generated_images/\n\u2502   \u251c\u2500\u2500 protest_scenes/\n\u2502   \u251c\u2500\u2500 gatherings/\n\u2502   \u251c\u2500\u2500 individual_portraits/\n\u2502   \u2514\u2500\u2500 contextual_scenes/\n\u251c\u2500\u2500 metadata/\n\u2502   \u251c\u2500\u2500 image_prompts.json\n\u2502   \u251c\u2500\u2500 generation_parameters.json\n\u2502   \u2514\u2500\u2500 quality_scores.csv\n\u2514\u2500\u2500 documentation/\n    \u2514\u2500\u2500 generation_log.md\n</code></pre>"},{"location":"data_collection/#evaluation-methods","title":"Evaluation Methods","text":""},{"location":"data_collection/#spot-checking","title":"Spot Checking","text":"<p>Manually review a sample of generated images:</p> <ol> <li>Select 10-15 random images</li> <li>Evaluate against quality criteria</li> <li>Identify common issues</li> <li>Refine prompts if needed</li> </ol>"},{"location":"data_collection/#automated-metrics","title":"Automated Metrics","text":"<p>Use computational methods to assess quality:</p> <pre><code>from PIL import Image\nimport numpy as np\n\ndef assess_image_quality(image_path):\n    img = Image.open(image_path)\n\n    # Resolution check\n    width, height = img.size\n    resolution_score = min(width * height / (1920 * 1080), 1.0)\n\n    # Color distribution\n    img_array = np.array(img)\n    color_variance = np.std(img_array)\n\n    # Brightness\n    brightness = np.mean(img_array)\n\n    return {\n        'resolution_score': resolution_score,\n        'color_variance': color_variance,\n        'brightness': brightness\n    }\n</code></pre>"},{"location":"data_collection/#human-in-the-loop-evaluation","title":"Human-in-the-Loop Evaluation","text":"<p>Involve human judgment for nuanced assessment:</p> <ul> <li>Rate image relevance (1-5 scale)</li> <li>Identify inappropriate content</li> <li>Assess cultural sensitivity</li> <li>Verify scenario alignment</li> </ul>"},{"location":"data_collection/#best-practices","title":"Best Practices","text":""},{"location":"data_collection/#prompt-engineering-tips","title":"Prompt Engineering Tips","text":"<ol> <li>Be specific: \"A crowded urban plaza during sunset\" vs \"A place with people\"</li> <li>Include context: Add temporal, spatial, and emotional context</li> <li>Iterate: Refine prompts based on results</li> <li>Document: Keep track of successful prompts</li> </ol>"},{"location":"data_collection/#data-management","title":"Data Management","text":"<ol> <li>Version control: Track prompt versions and parameters</li> <li>Metadata: Record generation details for each image</li> <li>Quality logs: Document evaluation results</li> <li>Backup: Save generated data in multiple locations</li> </ol>"},{"location":"data_collection/#ethical-considerations","title":"Ethical Considerations","text":"<ol> <li>Avoid stereotypes: Ensure diverse and respectful representations</li> <li>Cultural sensitivity: Review images for cultural appropriateness</li> <li>Transparency: Clearly label data as synthetic</li> <li>Purpose limitation: Use data only for stated educational purposes</li> </ol>"},{"location":"data_collection/#outputs","title":"Outputs","text":"<p>By the end of this module, you should have:</p> <ul> <li> A dataset of 50-100 synthetic images</li> <li> Metadata file with prompts and parameters</li> <li> Quality assessment scores</li> <li> Documentation of the generation process</li> </ul>"},{"location":"data_collection/#next-steps","title":"Next Steps","text":"<p>Once you've generated and evaluated your synthetic dataset:</p> <ol> <li>Prepare data for model training</li> <li>Proceed to Model Development</li> <li>Begin generating labels and captions using M-LLMs</li> </ol>"},{"location":"data_collection/#additional-resources","title":"Additional Resources","text":"<ul> <li>Google Gemini API Documentation</li> <li>Prompt Engineering Guide</li> <li>Synthetic Data Best Practices</li> </ul>"},{"location":"data_collection/#troubleshooting","title":"Troubleshooting","text":""},{"location":"data_collection/#api-rate-limits","title":"API Rate Limits","text":"<p>If you hit rate limits: - Add delays between requests (time.sleep(2)) - Batch requests appropriately - Monitor your quota usage</p>"},{"location":"data_collection/#poor-image-quality","title":"Poor Image Quality","text":"<p>If images don't meet quality standards: - Refine prompts with more detail - Adjust generation parameters - Try different style specifications - Review example prompts in the notebook</p>"},{"location":"data_collection/#content-issues","title":"Content Issues","text":"<p>If generated content is inappropriate: - Add content filters to prompts - Specify desired tone and atmosphere - Review and reject problematic outputs - Document issues for future prompt refinement</p> <p>Questions? Check the CyVerse Learning Center or ask during the workshop.</p>"},{"location":"model_deployment/","title":"Model Deployment and Network Analysis","text":"<p>In this final module, you'll deploy your models and perform network analysis to understand semantic relationships and thematic patterns in your data. This stage transforms individual predictions into knowledge graphs.</p>"},{"location":"model_deployment/#overview","title":"Overview","text":"<p>This module covers:</p> <ul> <li>Building semantic similarity networks</li> <li>Graph construction and visualization</li> <li>Community detection algorithms</li> <li>Centrality measures</li> <li>Narrative structure analysis</li> </ul>"},{"location":"model_deployment/#what-is-network-analysis","title":"What is Network Analysis?","text":"<p>Network analysis examines relationships and connections between entities:</p> <ul> <li>Nodes: Images, labels, captions, or concepts</li> <li>Edges: Semantic similarities or co-occurrences</li> <li>Communities: Clusters of related content</li> <li>Structure: Overall patterns and themes</li> </ul>"},{"location":"model_deployment/#graph-types","title":"Graph Types","text":""},{"location":"model_deployment/#bipartite-graphs","title":"Bipartite Graphs","text":"<p>Connect two different types of nodes (e.g., images and labels):</p> <pre><code>import networkx as nx\nimport matplotlib.pyplot as plt\n\n# Create bipartite graph\nG = nx.Graph()\n\n# Add nodes with bipartite attribute\nimages = ['img1', 'img2', 'img3']\nlabels = ['protest', 'gathering', 'march']\n\nG.add_nodes_from(images, bipartite=0)  # Images\nG.add_nodes_from(labels, bipartite=1)  # Labels\n\n# Add edges (image-label associations)\nedges = [\n    ('img1', 'protest'),\n    ('img1', 'gathering'),\n    ('img2', 'gathering'),\n    ('img3', 'march')\n]\nG.add_edges_from(edges)\n\n# Visualize\npos = nx.bipartite_layout(G, images)\nnx.draw(G, pos, with_labels=True, node_color=['lightblue']*3 + ['lightgreen']*3)\nplt.show()\n</code></pre>"},{"location":"model_deployment/#multipartite-graphs","title":"Multipartite Graphs","text":"<p>Connect multiple types of nodes (images, labels, captions):</p> <pre><code># Create multipartite graph\nG = nx.Graph()\n\n# Add different node types\nimages = ['img1', 'img2']\nlabels = ['protest', 'crowd']\ncaptions = ['caption1', 'caption2']\n\nG.add_nodes_from(images, node_type='image')\nG.add_nodes_from(labels, node_type='label')\nG.add_nodes_from(captions, node_type='caption')\n\n# Add edges between different types\nG.add_edges_from([\n    ('img1', 'protest'),\n    ('img1', 'caption1'),\n    ('protest', 'caption1')\n])\n</code></pre>"},{"location":"model_deployment/#semantic-similarity-networks","title":"Semantic Similarity Networks","text":"<p>Connect nodes based on semantic similarity:</p> <pre><code>from sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\n\n# Load model\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\n# Generate embeddings\ncaptions = [\n    \"A peaceful protest in the city square\",\n    \"Demonstrators march through downtown\",\n    \"Quiet evening in the plaza\"\n]\nembeddings = model.encode(captions)\n\n# Calculate similarity matrix\nsimilarity_matrix = cosine_similarity(embeddings)\n\n# Build graph with similarity threshold\nG = nx.Graph()\nG.add_nodes_from(range(len(captions)))\n\nthreshold = 0.5\nfor i in range(len(captions)):\n    for j in range(i+1, len(captions)):\n        if similarity_matrix[i][j] &gt; threshold:\n            G.add_edge(i, j, weight=similarity_matrix[i][j])\n\n# Visualize\npos = nx.spring_layout(G)\nedge_weights = [G[u][v]['weight'] for u, v in G.edges()]\nnx.draw(G, pos, with_labels=True, edge_color=edge_weights,\n        edge_cmap=plt.cm.Blues, width=3, node_color='lightblue', node_size=500)\nplt.show()\n</code></pre>"},{"location":"model_deployment/#building-networks","title":"Building Networks","text":""},{"location":"model_deployment/#step-1-prepare-data","title":"Step 1: Prepare Data","text":"<p>Organize your generated labels and captions:</p> <pre><code>import pandas as pd\n\n# Load predictions\ndata = pd.DataFrame({\n    'image_id': ['img1', 'img2', 'img3'],\n    'labels': [['protest', 'crowd'], ['gathering'], ['march', 'crowd']],\n    'caption': [\n        'A large crowd protesting',\n        'People gathering peacefully',\n        'March through the streets'\n    ]\n})\n</code></pre>"},{"location":"model_deployment/#step-2-calculate-semantic-weights","title":"Step 2: Calculate Semantic Weights","text":"<p>Compute weights for different data types:</p>"},{"location":"model_deployment/#visual-weights","title":"Visual Weights","text":"<p>Based on image similarity (using embeddings):</p> <pre><code>import torch\nfrom torchvision.models import resnet50, ResNet50_Weights\nfrom torchvision import transforms\nfrom PIL import Image\n\n# Load pre-trained model\nweights = ResNet50_Weights.DEFAULT\nmodel = resnet50(weights=weights)\nmodel.eval()\n\n# Remove classification layer to get embeddings\nmodel = torch.nn.Sequential(*list(model.children())[:-1])\n\n# Transform for model input\ntransform = weights.transforms()\n\ndef get_image_embedding(image_path):\n    img = Image.open(image_path)\n    img_tensor = transform(img).unsqueeze(0)\n\n    with torch.no_grad():\n        embedding = model(img_tensor)\n\n    return embedding.squeeze().numpy()\n\n# Calculate visual similarity\nimage_paths = ['img1.jpg', 'img2.jpg', 'img3.jpg']\nembeddings = [get_image_embedding(path) for path in image_paths]\n\nfrom sklearn.metrics.pairwise import cosine_similarity\nvisual_similarity = cosine_similarity(embeddings)\n</code></pre>"},{"location":"model_deployment/#label-weights","title":"Label Weights","text":"<p>Based on co-occurrence and semantic similarity:</p> <pre><code>from sklearn.preprocessing import MultiLabelBinarizer\n\n# Create label matrix\nmlb = MultiLabelBinarizer()\nlabel_matrix = mlb.fit_transform(data['labels'])\n\n# Calculate Jaccard similarity\nfrom sklearn.metrics import jaccard_score\n\ndef jaccard_similarity_matrix(label_matrix):\n    n = label_matrix.shape[0]\n    similarity = np.zeros((n, n))\n\n    for i in range(n):\n        for j in range(i, n):\n            similarity[i, j] = jaccard_score(label_matrix[i], label_matrix[j])\n            similarity[j, i] = similarity[i, j]\n\n    return similarity\n\nlabel_similarity = jaccard_similarity_matrix(label_matrix)\n</code></pre>"},{"location":"model_deployment/#caption-weights","title":"Caption Weights","text":"<p>Based on semantic embeddings:</p> <pre><code>from sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\ncaption_embeddings = model.encode(data['caption'].tolist())\ncaption_similarity = cosine_similarity(caption_embeddings)\n</code></pre>"},{"location":"model_deployment/#step-3-combine-weights","title":"Step 3: Combine Weights","text":"<p>Create composite similarity scores:</p> <pre><code># Weighted combination of different similarities\nalpha = 0.4  # Visual weight\nbeta = 0.3   # Label weight\ngamma = 0.3  # Caption weight\n\ncombined_similarity = (\n    alpha * visual_similarity +\n    beta * label_similarity +\n    gamma * caption_similarity\n)\n\n# Build final graph\nG = nx.Graph()\nn_images = len(data)\n\n# Add nodes\nfor i, row in data.iterrows():\n    G.add_node(i, image_id=row['image_id'], caption=row['caption'])\n\n# Add edges with combined weights\nthreshold = 0.6\nfor i in range(n_images):\n    for j in range(i+1, n_images):\n        if combined_similarity[i, j] &gt; threshold:\n            G.add_edge(i, j, weight=combined_similarity[i, j])\n</code></pre>"},{"location":"model_deployment/#community-detection","title":"Community Detection","text":"<p>Identify clusters of related content:</p>"},{"location":"model_deployment/#leiden-algorithm","title":"Leiden Algorithm","text":"<p>State-of-the-art community detection:</p> <pre><code>import igraph as ig\nimport leidenalg\n\n# Convert NetworkX graph to igraph\nedges = list(G.edges())\ng = ig.Graph(edges=edges)\n\n# Run Leiden algorithm\npartition = leidenalg.find_partition(g, leidenalg.ModularityVertexPartition)\n\n# Assign communities back to NetworkX graph\nfor i, community in enumerate(partition):\n    for node in community:\n        G.nodes[node]['community'] = i\n\nprint(f\"Found {len(partition)} communities\")\nprint(f\"Modularity: {partition.modularity:.4f}\")\n</code></pre>"},{"location":"model_deployment/#louvain-algorithm","title":"Louvain Algorithm","text":"<p>Alternative community detection:</p> <pre><code>from networkx.algorithms import community\n\n# Compute best partition\ncommunities = community.greedy_modularity_communities(G)\n\n# Assign to nodes\nfor i, comm in enumerate(communities):\n    for node in comm:\n        G.nodes[node]['community'] = i\n</code></pre>"},{"location":"model_deployment/#visualize-communities","title":"Visualize Communities","text":"<pre><code>import matplotlib.pyplot as plt\n\n# Get community assignments\ncommunity_map = nx.get_node_attributes(G, 'community')\ncommunities = list(set(community_map.values()))\n\n# Create color map\ncolors = plt.cm.rainbow(np.linspace(0, 1, len(communities)))\nnode_colors = [colors[community_map[node]] for node in G.nodes()]\n\n# Draw graph\npos = nx.spring_layout(G, k=0.5, iterations=50)\nplt.figure(figsize=(12, 8))\nnx.draw(G, pos, node_color=node_colors, with_labels=True,\n        node_size=500, font_size=10, font_weight='bold')\n\n# Add legend\nfor i, color in enumerate(colors):\n    plt.scatter([], [], c=[color], label=f'Community {i}')\nplt.legend()\nplt.title('Network Communities')\nplt.show()\n</code></pre>"},{"location":"model_deployment/#centrality-measures","title":"Centrality Measures","text":"<p>Identify important nodes in the network:</p>"},{"location":"model_deployment/#degree-centrality","title":"Degree Centrality","text":"<p>Number of connections:</p> <pre><code>degree_centrality = nx.degree_centrality(G)\n\n# Find most connected nodes\nsorted_nodes = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)\nprint(\"Top 5 nodes by degree centrality:\")\nfor node, centrality in sorted_nodes[:5]:\n    print(f\"Node {node}: {centrality:.4f}\")\n</code></pre>"},{"location":"model_deployment/#betweenness-centrality","title":"Betweenness Centrality","text":"<p>Nodes that bridge communities:</p> <pre><code>betweenness_centrality = nx.betweenness_centrality(G)\n\nsorted_nodes = sorted(betweenness_centrality.items(), key=lambda x: x[1], reverse=True)\nprint(\"Top 5 nodes by betweenness centrality:\")\nfor node, centrality in sorted_nodes[:5]:\n    print(f\"Node {node}: {centrality:.4f}\")\n</code></pre>"},{"location":"model_deployment/#eigenvector-centrality","title":"Eigenvector Centrality","text":"<p>Importance based on connections to important nodes:</p> <pre><code>eigenvector_centrality = nx.eigenvector_centrality(G, max_iter=1000)\n\nsorted_nodes = sorted(eigenvector_centrality.items(), key=lambda x: x[1], reverse=True)\nprint(\"Top 5 nodes by eigenvector centrality:\")\nfor node, centrality in sorted_nodes[:5]:\n    print(f\"Node {node}: {centrality:.4f}\")\n</code></pre>"},{"location":"model_deployment/#visualize-centrality","title":"Visualize Centrality","text":"<pre><code># Visualize with node size based on centrality\npos = nx.spring_layout(G)\nnode_sizes = [eigenvector_centrality[node] * 3000 for node in G.nodes()]\n\nplt.figure(figsize=(12, 8))\nnx.draw(G, pos, node_size=node_sizes, node_color='lightblue',\n        with_labels=True, font_size=10, font_weight='bold')\nplt.title('Network with Node Sizes by Eigenvector Centrality')\nplt.show()\n</code></pre>"},{"location":"model_deployment/#advanced-analysis","title":"Advanced Analysis","text":""},{"location":"model_deployment/#ergm-exponential-random-graph-models","title":"ERGM (Exponential Random Graph Models)","text":"<p>Model network formation processes:</p> <pre><code># Note: ERGM typically requires specialized software like statnet (R)\n# Python implementation example using ergm package\n\n# This is a conceptual example\n# Install: pip install ergm\n\n# Define ERGM terms\n# ergm_model = ergm(network ~ edges + nodematch('community') + triangle)\n</code></pre>"},{"location":"model_deployment/#narrative-structure-analysis","title":"Narrative Structure Analysis","text":"<p>Identify story arcs and themes:</p> <pre><code># Temporal analysis if timestamps available\ndef analyze_temporal_patterns(G, timestamps):\n    # Group by time periods\n    time_groups = {}\n\n    for node, ts in timestamps.items():\n        period = ts // 3600  # Hour bins\n        if period not in time_groups:\n            time_groups[period] = []\n        time_groups[period].append(node)\n\n    # Analyze evolution\n    for period in sorted(time_groups.keys()):\n        subgraph = G.subgraph(time_groups[period])\n        print(f\"Period {period}: {subgraph.number_of_nodes()} nodes, \"\n              f\"{subgraph.number_of_edges()} edges\")\n\n# Theme extraction\ndef extract_themes(G, captions):\n    from sklearn.feature_extraction.text import TfidfVectorizer\n\n    vectorizer = TfidfVectorizer(max_features=20, stop_words='english')\n    tfidf_matrix = vectorizer.fit_transform(captions)\n\n    # Get top terms\n    feature_names = vectorizer.get_feature_names_out()\n    print(\"Top themes:\", feature_names)\n\n    return feature_names\n</code></pre>"},{"location":"model_deployment/#interactive-visualization","title":"Interactive Visualization","text":""},{"location":"model_deployment/#using-plotly","title":"Using Plotly","text":"<pre><code>import plotly.graph_objects as go\n\n# Get positions\npos = nx.spring_layout(G)\n\n# Extract node positions\nnode_x = [pos[node][0] for node in G.nodes()]\nnode_y = [pos[node][1] for node in G.nodes()]\n\n# Extract edge positions\nedge_x = []\nedge_y = []\nfor edge in G.edges():\n    x0, y0 = pos[edge[0]]\n    x1, y1 = pos[edge[1]]\n    edge_x.extend([x0, x1, None])\n    edge_y.extend([y0, y1, None])\n\n# Create figure\nfig = go.Figure()\n\n# Add edges\nfig.add_trace(go.Scatter(x=edge_x, y=edge_y, mode='lines',\n                         line=dict(width=0.5, color='#888'),\n                         hoverinfo='none'))\n\n# Add nodes\nfig.add_trace(go.Scatter(x=node_x, y=node_y, mode='markers',\n                         marker=dict(size=10, color='lightblue'),\n                         text=list(G.nodes()),\n                         hoverinfo='text'))\n\nfig.update_layout(title='Interactive Network Graph',\n                  showlegend=False,\n                  hovermode='closest',\n                  xaxis=dict(showgrid=False, zeroline=False),\n                  yaxis=dict(showgrid=False, zeroline=False))\nfig.show()\n</code></pre>"},{"location":"model_deployment/#using-pyvis","title":"Using Pyvis","text":"<pre><code>from pyvis.network import Network\n\n# Create interactive network\nnet = Network(height='750px', width='100%', bgcolor='#222222', font_color='white')\n\n# Add nodes\nfor node in G.nodes():\n    net.add_node(node, label=str(node), title=f\"Node {node}\")\n\n# Add edges\nfor edge in G.edges(data=True):\n    net.add_edge(edge[0], edge[1], value=edge[2].get('weight', 1))\n\n# Save and display\nnet.show('network.html')\n</code></pre>"},{"location":"model_deployment/#best-practices","title":"Best Practices","text":""},{"location":"model_deployment/#graph-construction","title":"Graph Construction","text":"<ol> <li>Choose appropriate similarity thresholds</li> <li>Consider edge weight normalization</li> <li>Handle disconnected components</li> <li>Validate graph structure</li> </ol>"},{"location":"model_deployment/#community-detection_1","title":"Community Detection","text":"<ol> <li>Try multiple algorithms</li> <li>Compare results qualitatively</li> <li>Validate with domain knowledge</li> <li>Check community sizes</li> </ol>"},{"location":"model_deployment/#visualization","title":"Visualization","text":"<ol> <li>Use layouts appropriate for graph structure</li> <li>Limit node labels for large graphs</li> <li>Use interactive tools for exploration</li> <li>Export high-quality figures</li> </ol>"},{"location":"model_deployment/#outputs","title":"Outputs","text":"<p>By the end of this module, you should have:</p> <ul> <li> Semantic similarity networks</li> <li> Community structure analysis</li> <li> Centrality measures for all nodes</li> <li> Network visualizations</li> <li> Thematic analysis report</li> </ul>"},{"location":"model_deployment/#interpretation","title":"Interpretation","text":""},{"location":"model_deployment/#analyzing-results","title":"Analyzing Results","text":"<p>Questions to ask:</p> <ol> <li>Community Structure: What themes emerge from communities?</li> <li>Central Nodes: Which images/concepts are most influential?</li> <li>Bridges: Which nodes connect different communities?</li> <li>Patterns: Are there temporal or spatial patterns?</li> </ol>"},{"location":"model_deployment/#reporting-findings","title":"Reporting Findings","text":"<p>Document your analysis:</p> <pre><code># Network Analysis Report\n\n## Overview\n- Total nodes: X\n- Total edges: Y\n- Number of communities: Z\n\n## Key Findings\n1. Community 1: [Theme description]\n2. Community 2: [Theme description]\n\n## Central Concepts\n- Most connected: [Node X]\n- Key bridges: [Nodes Y, Z]\n\n## Narrative Structure\n[Description of overall patterns and story]\n</code></pre>"},{"location":"model_deployment/#additional-resources","title":"Additional Resources","text":"<ul> <li>NetworkX Documentation</li> <li>igraph Documentation</li> <li>Leiden Algorithm Paper</li> <li>Network Science Book</li> </ul>"},{"location":"model_deployment/#troubleshooting","title":"Troubleshooting","text":""},{"location":"model_deployment/#graph-too-dense","title":"Graph Too Dense","text":"<ul> <li>Increase similarity threshold</li> <li>Use sparse graph representations</li> <li>Sample edges</li> </ul>"},{"location":"model_deployment/#poor-community-detection","title":"Poor Community Detection","text":"<ul> <li>Adjust resolution parameter</li> <li>Try different algorithms</li> <li>Check data quality</li> </ul>"},{"location":"model_deployment/#visualization-issues","title":"Visualization Issues","text":"<ul> <li>Reduce number of nodes shown</li> <li>Use hierarchical layouts</li> <li>Export to specialized tools (Gephi, Cytoscape)</li> </ul> <p>Congratulations on completing the workshop! Check the CyVerse Learning Center for more resources.</p>"},{"location":"model_development/","title":"Model Development","text":"<p>In this module, you'll develop and evaluate Multimodal Large Language Models (M-LLMs) to generate labels and captions for synthetic images. This stage combines computer vision and natural language processing.</p>"},{"location":"model_development/#overview","title":"Overview","text":"<p>This module covers:</p> <ul> <li>Using M-LLMs for image understanding</li> <li>Generating labels and captions</li> <li>Text preprocessing and normalization</li> <li>Model evaluation metrics</li> <li>Performance optimization</li> </ul>"},{"location":"model_development/#what-are-multimodal-llms","title":"What are Multimodal-LLMs?","text":"<p>Multimodal Large Language Models can process and understand multiple types of data:</p> <ul> <li>Visual input: Images, videos, diagrams</li> <li>Text input: Prompts, questions, descriptions</li> <li>Combined output: Text descriptions, classifications, captions</li> </ul> <p>Examples: GPT-4 Vision, Google Gemini Pro Vision, LLaVA, CLIP</p>"},{"location":"model_development/#workflow","title":"Workflow","text":""},{"location":"model_development/#step-1-prepare-your-data","title":"Step 1: Prepare Your Data","text":"<p>Ensure you have: - Generated synthetic images from Data Collection - Organized images in structured directories - Created metadata files - Split data into training/validation/test sets</p>"},{"location":"model_development/#step-2-image-label-generation","title":"Step 2: Image Label Generation","text":""},{"location":"model_development/#understanding-labels","title":"Understanding Labels","text":"<p>Labels categorize images into predefined classes: - Scene type (protest, gathering, march) - Emotion (peaceful, tense, celebratory) - Density (crowded, sparse, intimate) - Time (day, night, dawn/dusk)</p>"},{"location":"model_development/#using-m-llms-for-labeling","title":"Using M-LLMs for Labeling","text":"<pre><code>import google.generativeai as genai\nfrom PIL import Image\nimport os\n\n# Configure Gemini\ngenai.configure(api_key=os.getenv('GOOGLE_API_KEY'))\nmodel = genai.GenerativeModel('gemini-pro-vision')\n\ndef generate_labels(image_path):\n    # Load image\n    img = Image.open(image_path)\n\n    # Create prompt\n    prompt = \"\"\"\n    Analyze this image and provide labels for:\n    1. Scene type (e.g., protest, gathering, march)\n    2. Emotional tone (e.g., peaceful, tense, celebratory)\n    3. Crowd density (e.g., sparse, moderate, dense)\n    4. Time of day (e.g., day, night, dusk)\n\n    Provide only the labels, separated by commas.\n    \"\"\"\n\n    # Generate labels\n    response = model.generate_content([prompt, img])\n    labels = response.text.strip().split(',')\n\n    return [label.strip() for label in labels]\n\n# Example usage\nimage_labels = generate_labels('path/to/image.jpg')\nprint(image_labels)\n</code></pre>"},{"location":"model_development/#step-3-caption-generation","title":"Step 3: Caption Generation","text":""},{"location":"model_development/#understanding-captions","title":"Understanding Captions","text":"<p>Captions provide descriptive text about image content: - Who: People and their actions - What: Objects and events - Where: Setting and location - When: Time and temporal context - How: Manner and atmosphere</p>"},{"location":"model_development/#generating-descriptive-captions","title":"Generating Descriptive Captions","text":"<pre><code>def generate_caption(image_path):\n    img = Image.open(image_path)\n\n    prompt = \"\"\"\n    Write a detailed caption for this image describing:\n    - The scene and setting\n    - People and their actions\n    - The overall atmosphere\n    - Any notable objects or elements\n\n    Keep the caption concise (2-3 sentences) and objective.\n    \"\"\"\n\n    response = model.generate_content([prompt, img])\n    return response.text.strip()\n\n# Example usage\ncaption = generate_caption('path/to/image.jpg')\nprint(caption)\n</code></pre>"},{"location":"model_development/#step-4-text-processing","title":"Step 4: Text Processing","text":"<p>Clean and normalize generated text for consistency.</p>"},{"location":"model_development/#lemmatization","title":"Lemmatization","text":"<p>Convert words to their base form:</p> <pre><code>from nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nimport nltk\n\n# Download required NLTK data\nnltk.download('wordnet')\nnltk.download('punkt')\n\nlemmatizer = WordNetLemmatizer()\n\ndef lemmatize_text(text):\n    tokens = word_tokenize(text.lower())\n    lemmatized = [lemmatizer.lemmatize(token) for token in tokens]\n    return ' '.join(lemmatized)\n\n# Example\ntext = \"People are gathering and protesting\"\nlemmatized = lemmatize_text(text)  # \"people be gather and protest\"\n</code></pre>"},{"location":"model_development/#emoji-translation","title":"Emoji Translation","text":"<p>Convert emojis to text descriptions:</p> <pre><code>import emoji\n\ndef translate_emojis(text):\n    return emoji.demojize(text, delimiters=(\" \", \" \"))\n\n# Example\ntext_with_emoji = \"Great protest today! \ud83c\udf89\"\ntranslated = translate_emojis(text_with_emoji)  # \"Great protest today!  party_popper \"\n</code></pre>"},{"location":"model_development/#hashtag-normalization","title":"Hashtag Normalization","text":"<p>Process hashtags for analysis:</p> <pre><code>import re\n\ndef normalize_hashtags(text):\n    # Split camelCase hashtags\n    def split_camel(hashtag):\n        return re.sub('([a-z])([A-Z])', r'\\1 \\2', hashtag)\n\n    # Find and process hashtags\n    hashtags = re.findall(r'#\\w+', text)\n    for hashtag in hashtags:\n        normalized = split_camel(hashtag[1:]).lower()  # Remove # and split\n        text = text.replace(hashtag, normalized)\n\n    return text\n\n# Example\ntext = \"Join the #ClimateStrike #SaveOurPlanet\"\nnormalized = normalize_hashtags(text)  # \"Join the climate strike save our planet\"\n</code></pre>"},{"location":"model_development/#complete-text-pipeline","title":"Complete Text Pipeline","text":"<pre><code>def preprocess_text(text):\n    # Translate emojis\n    text = translate_emojis(text)\n\n    # Normalize hashtags\n    text = normalize_hashtags(text)\n\n    # Lemmatize\n    text = lemmatize_text(text)\n\n    # Remove extra whitespace\n    text = ' '.join(text.split())\n\n    return text\n</code></pre>"},{"location":"model_development/#step-5-multilingual-support","title":"Step 5: Multilingual Support","text":"<p>Use sentence transformers for multilingual text understanding:</p>"},{"location":"model_development/#hugging-face-sentence-transformers","title":"Hugging Face Sentence Transformers","text":"<pre><code>from sentence_transformers import SentenceTransformer\n\n# Load multilingual model\nmodel = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n\n# Encode sentences\nsentences = [\n    \"A peaceful protest in the city square\",\n    \"Una protesta pac\u00edfica en la plaza de la ciudad\",\n    \"Une manifestation pacifique sur la place de la ville\"\n]\n\nembeddings = model.encode(sentences)\n\n# Calculate similarity\nfrom sklearn.metrics.pairwise import cosine_similarity\nsimilarity_matrix = cosine_similarity(embeddings)\nprint(similarity_matrix)\n</code></pre>"},{"location":"model_development/#labse-language-agnostic-bert-sentence-encoder","title":"LaBSE (Language-agnostic BERT Sentence Encoder)","text":"<pre><code>import tensorflow_hub as hub\nimport numpy as np\n\n# Load LaBSE model\nlabse_model = hub.load('https://tfhub.dev/google/LaBSE/2')\n\n# Encode multilingual sentences\nsentences = [\n    \"People gathering for a cause\",\n    \"Gente reunida por una causa\"\n]\n\nembeddings = labse_model(sentences)\n\n# Compute similarity\nsimilarity = np.inner(embeddings[0], embeddings[1])\nprint(f\"Similarity: {similarity:.4f}\")\n</code></pre>"},{"location":"model_development/#model-evaluation","title":"Model Evaluation","text":""},{"location":"model_development/#classification-metrics","title":"Classification Metrics","text":"<p>For label prediction tasks:</p> <pre><code>from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, hamming_loss\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef evaluate_classification(y_true, y_pred):\n    # Calculate metrics\n    metrics = {\n        'accuracy': accuracy_score(y_true, y_pred),\n        'precision': precision_score(y_true, y_pred, average='weighted'),\n        'recall': recall_score(y_true, y_pred, average='weighted'),\n        'f1_score': f1_score(y_true, y_pred, average='weighted'),\n        'hamming_loss': hamming_loss(y_true, y_pred)\n    }\n\n    # Confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n\n    # Visualize\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n    plt.title('Confusion Matrix')\n    plt.ylabel('True Label')\n    plt.xlabel('Predicted Label')\n    plt.show()\n\n    return metrics\n\n# Example usage\n# y_true = true_labels\n# y_pred = predicted_labels\n# metrics = evaluate_classification(y_true, y_pred)\n</code></pre>"},{"location":"model_development/#semantic-similarity-metrics","title":"Semantic Similarity Metrics","text":"<p>For caption generation tasks:</p>"},{"location":"model_development/#bertscore","title":"BERTScore","text":"<pre><code>from bert_score import score\n\ndef calculate_bertscore(candidates, references):\n    P, R, F1 = score(candidates, references, lang='en', verbose=True)\n\n    return {\n        'precision': P.mean().item(),\n        'recall': R.mean().item(),\n        'f1': F1.mean().item()\n    }\n\n# Example\ncandidates = [\"A large crowd protesting in the streets\"]\nreferences = [\"Many people demonstrating on the street\"]\nbert_metrics = calculate_bertscore(candidates, references)\n</code></pre>"},{"location":"model_development/#sentence-transformer-cosine-similarity","title":"Sentence Transformer Cosine Similarity","text":"<pre><code>from sentence_transformers import SentenceTransformer, util\n\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\ndef calculate_semantic_similarity(text1, text2):\n    embeddings = model.encode([text1, text2])\n    similarity = util.cos_sim(embeddings[0], embeddings[1])\n    return similarity.item()\n\n# Example\ngenerated = \"A peaceful protest in the city\"\nreference = \"A calm demonstration in town\"\nsimilarity = calculate_semantic_similarity(generated, reference)\nprint(f\"Similarity: {similarity:.4f}\")\n</code></pre>"},{"location":"model_development/#model-training-and-fine-tuning","title":"Model Training and Fine-Tuning","text":""},{"location":"model_development/#data-preparation","title":"Data Preparation","text":"<pre><code>import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\n\nclass ImageCaptionDataset(Dataset):\n    def __init__(self, image_paths, captions, transform=None):\n        self.image_paths = image_paths\n        self.captions = captions\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        image = Image.open(self.image_paths[idx])\n        if self.transform:\n            image = self.transform(image)\n\n        caption = self.captions[idx]\n        return image, caption\n\n# Define transforms\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225])\n])\n\n# Create dataset and dataloader\ndataset = ImageCaptionDataset(image_paths, captions, transform=transform)\ndataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n</code></pre>"},{"location":"model_development/#tracking-performance","title":"Tracking Performance","text":"<pre><code>import wandb  # Weights &amp; Biases for experiment tracking\n\n# Initialize tracking\nwandb.init(project=\"ml-pipeline-workshop\")\n\n# Log metrics during training\nfor epoch in range(num_epochs):\n    train_loss = train_epoch(model, dataloader)\n    val_metrics = evaluate(model, val_dataloader)\n\n    wandb.log({\n        'epoch': epoch,\n        'train_loss': train_loss,\n        'val_accuracy': val_metrics['accuracy'],\n        'val_f1': val_metrics['f1_score']\n    })\n</code></pre>"},{"location":"model_development/#required-resources","title":"Required Resources","text":"<ul> <li>GPU Access: Jetstream2 allocation through CyVerse</li> <li>LLM Access: Google Gemini API or VERDE platform</li> <li>Hugging Face Account: For model downloads</li> <li>Storage: Sufficient space for models and data</li> </ul>"},{"location":"model_development/#best-practices","title":"Best Practices","text":""},{"location":"model_development/#model-selection","title":"Model Selection","text":"<ol> <li>Start with pre-trained models</li> <li>Fine-tune on your specific domain</li> <li>Compare multiple models</li> <li>Consider computational constraints</li> </ol>"},{"location":"model_development/#prompt-engineering","title":"Prompt Engineering","text":"<ol> <li>Clear and specific instructions</li> <li>Include examples (few-shot prompting)</li> <li>Iterate based on outputs</li> <li>Document successful prompts</li> </ol>"},{"location":"model_development/#evaluation-strategy","title":"Evaluation Strategy","text":"<ol> <li>Use multiple metrics</li> <li>Include human evaluation</li> <li>Test on diverse examples</li> <li>Monitor for biases</li> </ol>"},{"location":"model_development/#outputs","title":"Outputs","text":"<p>By the end of this module, you should have:</p> <ul> <li> Generated labels for all images</li> <li> Generated captions for all images</li> <li> Preprocessed and normalized text data</li> <li> Evaluation metrics and visualizations</li> <li> Model performance documentation</li> </ul>"},{"location":"model_development/#next-steps","title":"Next Steps","text":"<p>Once model development is complete:</p> <ol> <li>Analyze model outputs for patterns</li> <li>Proceed to Model Deployment</li> <li>Build semantic networks from generated data</li> </ol>"},{"location":"model_development/#additional-resources","title":"Additional Resources","text":"<ul> <li>Hugging Face Transformers Documentation</li> <li>Sentence Transformers Documentation</li> <li>BERTScore Paper</li> <li>PyTorch Tutorials</li> </ul>"},{"location":"model_development/#troubleshooting","title":"Troubleshooting","text":""},{"location":"model_development/#memory-issues","title":"Memory Issues","text":"<ul> <li>Reduce batch size</li> <li>Use gradient accumulation</li> <li>Enable mixed precision training</li> <li>Clear GPU cache regularly</li> </ul>"},{"location":"model_development/#poor-model-performance","title":"Poor Model Performance","text":"<ul> <li>Increase training data</li> <li>Adjust learning rate</li> <li>Try different model architectures</li> <li>Review data quality</li> </ul>"},{"location":"model_development/#api-rate-limits","title":"API Rate Limits","text":"<ul> <li>Implement exponential backoff</li> <li>Cache responses</li> <li>Batch requests efficiently</li> <li>Monitor quota usage</li> </ul> <p>Questions? Check the CyVerse Learning Center or ask during the workshop.</p>"},{"location":"setup/","title":"Workshop Setup Guide","text":"<p>This guide will help you set up your environment for the AI/ML Pipeline Workshop on CyVerse Discovery Environment.</p>"},{"location":"setup/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, make sure you have:</p> <ul> <li> Created a CyVerse account (instructions here)</li> <li> Registered for the workshop at https://user.cyverse.org/workshops/208</li> <li> A Google Gemini API key (we'll cover how to get this)</li> </ul>"},{"location":"setup/#step-1-access-cyverse-discovery-environment","title":"Step 1: Access CyVerse Discovery Environment","text":"<ol> <li>Navigate to https://de.cyverse.org</li> <li>Log in with your CyVerse credentials</li> <li>You should see the Discovery Environment dashboard</li> </ol>"},{"location":"setup/#step-2-launch-jupyter-lab-pytorch-gpu","title":"Step 2: Launch Jupyter Lab PyTorch GPU","text":"<p>This workshop requires GPU acceleration for model training and inference.</p> <ol> <li>In the Discovery Environment, click \"Apps\" in the left sidebar</li> <li>Look for the \"Instant Launches\" section</li> <li>Click on \"Jupyter Lab PyTorch GPU\"</li> <li>Wait for the environment to initialize (this may take 2-5 minutes)</li> </ol> <p>The Jupyter Lab interface will open in a new browser tab once ready.</p>"},{"location":"setup/#step-3-clone-the-workshop-repository","title":"Step 3: Clone the Workshop Repository","text":"<p>In Jupyter Lab:</p>"},{"location":"setup/#option-a-using-terminal","title":"Option A: Using Terminal","text":"<ol> <li>Click the Terminal icon in the Launcher (or File &gt; New &gt; Terminal)</li> <li>Navigate to your desired directory:    <pre><code>cd /home/jovyan/data-store\n</code></pre></li> <li>Clone the repository:    <pre><code>git clone https://github.com/lwdozal/AI-ML_PipelineWorkshop.git\n</code></pre></li> <li>Navigate into the repository:    <pre><code>cd AI-ML_PipelineWorkshop\n</code></pre></li> </ol>"},{"location":"setup/#option-b-using-git-extension","title":"Option B: Using Git Extension","text":"<ol> <li>Click the Git icon in the left sidebar</li> <li>Click \"Clone a Repository\"</li> <li>Enter the repository URL: <code>https://github.com/lwdozal/AI-ML_PipelineWorkshop.git</code></li> <li>Choose the destination folder: <code>/home/jovyan/data-store</code></li> <li>Click \"Clone\"</li> </ol>"},{"location":"setup/#step-4-create-a-virtual-environment","title":"Step 4: Create a Virtual Environment","text":"<p>Creating a virtual environment isolates your project dependencies:</p> <pre><code># Navigate to the repository directory\ncd /home/jovyan/data-store/AI-ML_PipelineWorkshop\n\n# Create a virtual environment named 'workshop_env'\npython -m venv workshop_env\n\n# Activate the virtual environment\nsource workshop_env/bin/activate\n</code></pre> <p>You should see <code>(workshop_env)</code> in your terminal prompt, indicating the environment is active.</p>"},{"location":"setup/#step-5-install-required-packages","title":"Step 5: Install Required Packages","text":"<p>Install all necessary Python packages using pip:</p> <pre><code>pip install --upgrade pip\npip install -r requirements.txt\n</code></pre>"},{"location":"setup/#key-packages-installed","title":"Key Packages Installed","text":"<p>The <code>requirements.txt</code> file includes:</p> <ul> <li>torch &amp; torchvision: PyTorch for deep learning</li> <li>transformers: Hugging Face transformers library</li> <li>sentence-transformers: For semantic similarity</li> <li>google-generativeai: Google Gemini API client</li> <li>PIL (Pillow): Image processing</li> <li>opencv-python: Computer vision tasks</li> <li>langchain: LLM orchestration</li> <li>pandas &amp; numpy: Data manipulation</li> <li>matplotlib &amp; seaborn: Data visualization</li> <li>networkx: Network analysis</li> <li>jupyter: Notebook interface</li> </ul>"},{"location":"setup/#step-6-set-up-google-gemini-api-key","title":"Step 6: Set Up Google Gemini API Key","text":"<p>You'll need a Google Gemini API key to generate synthetic images.</p>"},{"location":"setup/#get-your-api-key","title":"Get Your API Key","text":"<ol> <li>Go to https://makersuite.google.com/app/apikey</li> <li>Sign in with your Google account</li> <li>Click \"Create API Key\"</li> <li>Copy your API key</li> </ol>"},{"location":"setup/#configure-your-api-key","title":"Configure Your API Key","text":"<p>Create a <code>.env</code> file in the repository root:</p> <pre><code># In the terminal, navigate to repository root\ncd /home/jovyan/data-store/AI-ML_PipelineWorkshop\n\n# Create .env file\nnano .env\n</code></pre> <p>Add your API key:</p> <pre><code>GOOGLE_API_KEY=your_api_key_here\n</code></pre> <p>Save and exit (Ctrl+X, then Y, then Enter).</p> <p>Security Note: Never commit your <code>.env</code> file to Git. The repository includes a <code>.gitignore</code> file to prevent this.</p>"},{"location":"setup/#verify-installation","title":"Verify Installation","text":"<p>Test your setup by running this Python code in a new notebook:</p> <pre><code>import os\nfrom dotenv import load_dotenv\nimport google.generativeai as genai\n\n# Load environment variables\nload_dotenv()\n\n# Configure Gemini\ngenai.configure(api_key=os.getenv('GOOGLE_API_KEY'))\n\n# Test connection\nmodel = genai.GenerativeModel('gemini-pro')\nresponse = model.generate_content(\"Hello, this is a test.\")\nprint(response.text)\n</code></pre> <p>If this runs without errors, your setup is complete.</p>"},{"location":"setup/#step-7-verify-gpu-access","title":"Step 7: Verify GPU Access","text":"<p>Verify that PyTorch can access the GPU:</p> <pre><code>import torch\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"CUDA version: {torch.version.cuda}\")\nif torch.cuda.is_available():\n    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n</code></pre> <p>Expected output should show <code>CUDA available: True</code> and list the GPU device.</p>"},{"location":"setup/#directory-structure","title":"Directory Structure","text":"<p>After setup, your directory should look like this:</p> <pre><code>AI-ML_PipelineWorkshop/\n\u251c\u2500\u2500 DataCollection/          # Synthetic data generation notebooks\n\u251c\u2500\u2500 docs/                    # Documentation (this site)\n\u251c\u2500\u2500 workshop_env/            # Virtual environment (not tracked by git)\n\u251c\u2500\u2500 .env                     # API keys (not tracked by git)\n\u251c\u2500\u2500 .gitignore              # Git ignore rules\n\u251c\u2500\u2500 requirements.txt         # Python dependencies\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 AUTHORS.md\n\u2514\u2500\u2500 LICENSE\n</code></pre>"},{"location":"setup/#troubleshooting","title":"Troubleshooting","text":""},{"location":"setup/#virtual-environment-issues","title":"Virtual Environment Issues","text":"<p>If you can't activate the virtual environment:</p> <pre><code># Make sure you're in the right directory\ncd /home/jovyan/data-store/AI-ML_PipelineWorkshop\n\n# Try creating it again\npython3 -m venv workshop_env\nsource workshop_env/bin/activate\n</code></pre>"},{"location":"setup/#package-installation-errors","title":"Package Installation Errors","text":"<p>If pip install fails:</p> <pre><code># Upgrade pip first\npip install --upgrade pip\n\n# Try installing packages individually\npip install torch torchvision\npip install transformers sentence-transformers\npip install google-generativeai\n</code></pre>"},{"location":"setup/#gpu-not-available","title":"GPU Not Available","text":"<p>If CUDA is not available:</p> <ol> <li>Verify you launched the PyTorch GPU version of Jupyter Lab</li> <li>Restart the kernel: Kernel &gt; Restart Kernel</li> <li>Check CyVerse status page for GPU availability</li> <li>Contact workshop support if issues persist</li> </ol>"},{"location":"setup/#api-key-issues","title":"API Key Issues","text":"<p>If Gemini API doesn't work:</p> <ol> <li>Verify your API key is correct in the <code>.env</code> file</li> <li>Check you've enabled the Gemini API in Google Cloud Console</li> <li>Ensure you have API quota remaining</li> <li>Test with a simple curl request:    <pre><code>curl -H 'Content-Type: application/json' \\\n     -d '{\"contents\":[{\"parts\":[{\"text\":\"Hello\"}]}]}' \\\n     \"https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent?key=YOUR_API_KEY\"\n</code></pre></li> </ol>"},{"location":"setup/#additional-resources","title":"Additional Resources","text":"<ul> <li>CyVerse Learning Center</li> <li>PyTorch Documentation</li> <li>Hugging Face Documentation</li> <li>Google Gemini API Documentation</li> </ul>"},{"location":"setup/#getting-help","title":"Getting Help","text":"<ul> <li>During the workshop: Ask instructors or teaching assistants</li> <li>CyVerse support: support@cyverse.org</li> <li>Repository issues: GitHub Issues</li> </ul> <p>Once your setup is complete, you're ready to start with Data Collection.</p>"}]}